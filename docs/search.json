[
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides a variety of built-in algorithms, foundation models, and solution templates to streamline machine learning (ML) workflows. This post explores:\n\nHow to choose built-in algorithms, foundation models, and solution templates from SageMaker JumpStart and Amazon Bedrock.\nMethods for integrating externally developed models into SageMaker.\nUsing SageMaker built-in algorithms and common ML libraries to develop ML models.\n\n\n\n\n\nSageMaker JumpStart provides pre-built ML models, solution templates, and training scripts to help users get started quickly. It includes models for computer vision, NLP, and tabular data.\n\n\nimport boto3\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\nmodels = list_jumpstart_models()\nprint(models[:5])  # Display first five models\n\n\n\n\nAmazon Bedrock allows developers to build and scale applications using foundation models from AWS and third-party providers. It provides access to powerful large language models (LLMs) without needing extensive infrastructure management.\n\n\n\n\n\n\n\n\nFeature\nSageMaker JumpStart\nAmazon Bedrock\n\n\n\n\nModel Type\nPre-trained ML models\nFoundation Models (LLMs)\n\n\nCustomization\nFine-tuning and transfer learning\nAPI-based integration\n\n\nUse Case\nComputer vision, NLP, structured data\nChatbots, content generation, summarization\n\n\n\n\n\nimport boto3\n\nbedrock = boto3.client(\"bedrock-runtime\")\nresponse = bedrock.invoke_model(\n    modelId=\"ai21.j2-ultra\",\n    body=\"{\"prompt\": \"What are the benefits of using AWS SageMaker?\"}\"\n)\nprint(response[\"body\"].read().decode(\"utf-8\"))\n\n\n\n\n\nIf a model is built outside of SageMaker, it can still be deployed using SageMaker endpoints. The most common methods include:\n\n\n\nSave the trained model in an s3:// bucket.\nCreate a SageMaker inference script.\nDeploy using a SageMaker endpoint.\n\n\n\nfrom sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    entry_point=\"inference.py\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n)\n\npredictor = model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)\nprint(\"Model deployed successfully\")\n\n\n\n\n\nSageMaker provides built-in algorithms optimized for scalability and efficiency.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")\n\n\n\n\nAWS SageMaker simplifies the ML development process by offering pre-built models, seamless integration of external models, and a variety of built-in algorithms. Whether you’re using SageMaker JumpStart, Amazon Bedrock, or integrating your custom models, AWS provides a scalable and efficient ML workflow."
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#choosing-built-in-algorithms-foundation-models-and-solution-templates",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#choosing-built-in-algorithms-foundation-models-and-solution-templates",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "SageMaker JumpStart provides pre-built ML models, solution templates, and training scripts to help users get started quickly. It includes models for computer vision, NLP, and tabular data.\n\n\nimport boto3\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\nmodels = list_jumpstart_models()\nprint(models[:5])  # Display first five models\n\n\n\n\nAmazon Bedrock allows developers to build and scale applications using foundation models from AWS and third-party providers. It provides access to powerful large language models (LLMs) without needing extensive infrastructure management.\n\n\n\n\n\n\n\n\nFeature\nSageMaker JumpStart\nAmazon Bedrock\n\n\n\n\nModel Type\nPre-trained ML models\nFoundation Models (LLMs)\n\n\nCustomization\nFine-tuning and transfer learning\nAPI-based integration\n\n\nUse Case\nComputer vision, NLP, structured data\nChatbots, content generation, summarization\n\n\n\n\n\nimport boto3\n\nbedrock = boto3.client(\"bedrock-runtime\")\nresponse = bedrock.invoke_model(\n    modelId=\"ai21.j2-ultra\",\n    body=\"{\"prompt\": \"What are the benefits of using AWS SageMaker?\"}\"\n)\nprint(response[\"body\"].read().decode(\"utf-8\"))"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#integrating-external-models-into-sagemaker",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#integrating-external-models-into-sagemaker",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "If a model is built outside of SageMaker, it can still be deployed using SageMaker endpoints. The most common methods include:\n\n\n\nSave the trained model in an s3:// bucket.\nCreate a SageMaker inference script.\nDeploy using a SageMaker endpoint.\n\n\n\nfrom sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    entry_point=\"inference.py\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n)\n\npredictor = model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)\nprint(\"Model deployed successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#developing-ml-models-with-sagemaker-built-in-algorithms",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#developing-ml-models-with-sagemaker-built-in-algorithms",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "SageMaker provides built-in algorithms optimized for scalability and efficiency.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#conclusion",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#conclusion",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker simplifies the ML development process by offering pre-built models, seamless integration of external models, and a variety of built-in algorithms. Whether you’re using SageMaker JumpStart, Amazon Bedrock, or integrating your custom models, AWS provides a scalable and efficient ML workflow."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "",
    "text": "Amazon SageMaker provides a comprehensive suite of tools for building, training, and deploying machine learning models at scale. This post explores SageMaker’s capabilities, data ingestion options, feature engineering tools, and deployment strategies."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#introduction",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#introduction",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "",
    "text": "Amazon SageMaker provides a comprehensive suite of tools for building, training, and deploying machine learning models at scale. This post explores SageMaker’s capabilities, data ingestion options, feature engineering tools, and deployment strategies."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#sagemaker-capabilities-and-algorithms",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#sagemaker-capabilities-and-algorithms",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "SageMaker Capabilities and Algorithms",
    "text": "SageMaker Capabilities and Algorithms\nAmazon SageMaker supports various built-in machine learning algorithms for supervised and unsupervised learning, including:\n\n\n\n\n\n\n\n\nAlgorithm\nType\nUse Case\n\n\n\n\nXGBoost\nSupervised\nRegression, Classification\n\n\nLinear Learner\nSupervised\nRegression, Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nRandom Cut Forest\nUnsupervised\nAnomaly Detection\n\n\nDeepAR Forecasting\nSupervised\nTime-Series Forecasting\n\n\n\nAdditionally, SageMaker allows users to train custom models using frameworks like TensorFlow, PyTorch, and Scikit-Learn.\n\nExample: Training an XGBoost Model in SageMaker\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.xgboost.estimator import XGBoost\n\n# Initialize SageMaker session\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Define S3 data paths\ntrain_data = 's3://your-bucket/train.csv'\ntest_data = 's3://your-bucket/test.csv'\n\n# Create XGBoost estimator\nxgb = XGBoost(entry_point='script.py',\n              framework_version='1.3-1',\n              role=role,\n              instance_count=1,\n              instance_type='ml.m5.large',\n              hyperparameters={'num_round': 100})\n\n# Train the model\nxgb.fit({'train': TrainingInput(train_data, content_type='csv')})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#ingesting-data-into-sagemaker",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#ingesting-data-into-sagemaker",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Ingesting Data into SageMaker",
    "text": "Ingesting Data into SageMaker\n\nUsing SageMaker Data Wrangler\nSageMaker Data Wrangler allows you to import, clean, and transform data before training models. You can: - Import data from Amazon S3, Redshift, Athena, and other sources. - Perform transformations such as filtering, joining, and feature engineering. - Export processed data directly into SageMaker Feature Store or S3.\n\n\nUsing SageMaker Feature Store\nSageMaker Feature Store helps manage features across ML workflows. It supports: - Online store for real-time inference. - Offline store for batch processing.\n\n\nExample: Storing Features in SageMaker Feature Store\nfrom sagemaker.feature_store.feature_group import FeatureGroup\nimport pandas as pd\n\n# Define feature group\nfeature_group = FeatureGroup(name=\"customer_features\", sagemaker_session=sagemaker_session)\n\n# Sample data\ndata = pd.DataFrame({\n    \"customer_id\": [1, 2, 3],\n    \"purchase_count\": [10, 20, 15],\n    \"avg_spend\": [100.5, 200.0, 150.7]\n})\n\n# Ingest data into feature store\nfeature_group.ingest(data_frame=data, max_workers=3, wait=True)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#data-exploration-and-feature-engineering",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#data-exploration-and-feature-engineering",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Data Exploration and Feature Engineering",
    "text": "Data Exploration and Feature Engineering\nAWS offers multiple tools for data exploration and transformation:\n\n\n\nTool\nPurpose\n\n\n\n\nSageMaker Data Wrangler\nData preprocessing and transformation\n\n\nAWS Glue\nData cataloging and ETL\n\n\nAWS Glue DataBrew\nData cleaning and enrichment\n\n\n\n\nExample: Using AWS Glue for ETL\nimport boto3\n\nglue_client = boto3.client('glue')\nresponse = glue_client.create_job(\n    Name='etl-job',\n    Role='AWSGlueServiceRole',\n    Command={'Name': 'glueetl', 'ScriptLocation': 's3://your-bucket/scripts/etl.py'},\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#deploying-models-in-sagemaker",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#deploying-models-in-sagemaker",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Deploying Models in SageMaker",
    "text": "Deploying Models in SageMaker\nSageMaker provides multiple deployment options:\n\n\n\nDeployment Method\nUse Case\n\n\n\n\nReal-time endpoint\nLow-latency inference\n\n\nBatch transform\nLarge-scale batch inference\n\n\nEdge deployment\nDeploying models to IoT and edge devices\n\n\n\n\nExample: Deploying a Model for Real-Time Inference\n# Deploy trained model as an endpoint\npredictor = xgb.deploy(instance_type='ml.m5.large', initial_instance_count=1)\n\n# Make predictions\nresult = predictor.predict(data=[1.5, 2.3, 3.1])\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#conclusion",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#conclusion",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nAmazon SageMaker provides a powerful suite of tools for data preparation, model training, feature management, and deployment. By leveraging SageMaker Data Wrangler, Feature Store, and AWS Glue, you can create scalable and efficient ML pipelines."
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#conclusion",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html",
    "href": "sagemaker/sagemaker-optimizing-deployments.html",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "",
    "text": "Amazon SageMaker offers flexible deployment options, cost-saving strategies, and security features. This post explores:\n\nRightsizing instance families and sizes using SageMaker Inference Recommender and AWS Compute Optimizer.\nOptimizing costs with different purchasing options.\nManaging access control with IAM roles and policies.\nImplementing security and compliance best practices in SageMaker."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#introduction",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#introduction",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "",
    "text": "Amazon SageMaker offers flexible deployment options, cost-saving strategies, and security features. This post explores:\n\nRightsizing instance families and sizes using SageMaker Inference Recommender and AWS Compute Optimizer.\nOptimizing costs with different purchasing options.\nManaging access control with IAM roles and policies.\nImplementing security and compliance best practices in SageMaker."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#rightsizing-instance-families-and-sizes",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#rightsizing-instance-families-and-sizes",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Rightsizing Instance Families and Sizes",
    "text": "Rightsizing Instance Families and Sizes\nSelecting the appropriate instance type ensures optimal performance and cost-efficiency.\n\n\n\n\n\n\n\nInstance Type\nUse Case\n\n\n\n\nCPU (ml.m5, ml.c5)\nLightweight inference, batch processing\n\n\nGPU (ml.g4dn, ml.p3)\nDeep learning inference, high-performance computing\n\n\nInf (ml.inf1)\nOptimized for deep learning with Inferentia chips\n\n\nGraviton (ml.c7g, ml.m7g)\nCost-efficient CPU workloads\n\n\n\n\nUsing SageMaker Inference Recommender\nSageMaker Inference Recommender suggests the best instance type based on model performance.\nimport boto3\nsagemaker = boto3.client('sagemaker')\n\nresponse = sagemaker.create_inference_recommendations_job(\n    JobName='my-inference-recommender-job',\n    RoleArn='arn:aws:iam::account-id:role/SageMakerRole',\n    ModelName='my-model',\n    EndpointConfigurations=[\n        {\n            'InstanceTypes': ['ml.m5.large', 'ml.g4dn.xlarge'],\n            'InitialInstanceCount': 1\n        }\n    ]\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#optimizing-infrastructure-costs",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#optimizing-infrastructure-costs",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Optimizing Infrastructure Costs",
    "text": "Optimizing Infrastructure Costs\nAWS provides multiple purchasing options to reduce costs:\n\n\n\n\n\n\n\n\nPurchasing Option\nDescription\nSavings Potential\n\n\n\n\nOn-Demand\nPay-as-you-go pricing\nBaseline cost\n\n\nSpot Instances\nUp to 90% discount but can be interrupted\nHigh\n\n\nReserved Instances\nCommit for 1-3 years for lower rates\nUp to 72%\n\n\nSageMaker Savings Plans\nFlexible, commitment-based discount\nUp to 64%\n\n\n\n\nUsing Spot Instances in SageMaker\nfrom sagemaker.estimator import Estimator\n\nestimator = Estimator(\n    image_uri='your-container-image',\n    role='arn:aws:iam::account-id:role/SageMakerRole',\n    instance_count=1,\n    instance_type='ml.m5.large',\n    use_spot_instances=True,\n    max_wait=3600,\n    max_run=1800\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#iam-roles-policies-and-groups-for-access-control",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#iam-roles-policies-and-groups-for-access-control",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "IAM Roles, Policies, and Groups for Access Control",
    "text": "IAM Roles, Policies, and Groups for Access Control\nAWS Identity and Access Management (IAM) controls permissions for SageMaker and related services.\n\nKey IAM Components\n\n\n\nIAM Component\nDescription\n\n\n\n\nIAM Roles\nGrant temporary access to AWS services\n\n\nIAM Policies\nDefine permissions for actions and resources\n\n\nIAM Groups\nOrganize multiple users with the same permissions\n\n\n\n\n\nExample: Creating a SageMaker Execution Role\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"sagemaker.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#sagemaker-security-and-compliance-features",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#sagemaker-security-and-compliance-features",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "SageMaker Security and Compliance Features",
    "text": "SageMaker Security and Compliance Features\nAWS SageMaker includes built-in security measures:\n\n\n\n\n\n\n\nSecurity Feature\nDescription\n\n\n\n\nVPC Isolation\nDeploy endpoints inside a VPC for network control\n\n\nEncryption\nEncrypt data at rest (S3) and in transit (TLS)\n\n\nSageMaker Role Manager\nSimplifies IAM role management\n\n\nLogging & Auditing\nTrack actions with AWS CloudTrail and CloudWatch\n\n\n\n\nEnforcing Encryption in SageMaker\nfrom sagemaker import Model\n\nmodel = Model(\n    image_uri='your-ecr-image',\n    model_data='s3://your-bucket/model.tar.gz',\n    role='arn:aws:iam::account-id:role/SageMakerRole',\n    vpc_config={'SecurityGroupIds': ['sg-xxxx'], 'Subnets': ['subnet-xxxx']},\n    enable_network_isolation=True\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#conclusion",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#conclusion",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Conclusion",
    "text": "Conclusion\nBy rightsizing instances, optimizing cost strategies, enforcing IAM policies, and utilizing SageMaker’s security features, we can build efficient and secure ML deployments. These best practices ensure scalability while keeping costs under control."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "",
    "text": "Amazon SageMaker provides powerful tools for deploying and monitoring machine learning models in a secure and scalable way. This blog post covers:\n\nConfiguring SageMaker endpoints within a VPC network.\nDeploying and hosting models using the SageMaker SDK.\nMonitoring models in production using SageMaker Model Monitor.\nDetecting changes in data distribution with SageMaker Clarify."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#introduction",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#introduction",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "",
    "text": "Amazon SageMaker provides powerful tools for deploying and monitoring machine learning models in a secure and scalable way. This blog post covers:\n\nConfiguring SageMaker endpoints within a VPC network.\nDeploying and hosting models using the SageMaker SDK.\nMonitoring models in production using SageMaker Model Monitor.\nDetecting changes in data distribution with SageMaker Clarify."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#configuring-sagemaker-endpoints-within-a-vpc",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#configuring-sagemaker-endpoints-within-a-vpc",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Configuring SageMaker Endpoints within a VPC",
    "text": "Configuring SageMaker Endpoints within a VPC\nDeploying SageMaker endpoints within an Amazon Virtual Private Cloud (VPC) enhances security by preventing data from being exposed to the internet.\n\nSteps to Configure a VPC-Enabled Endpoint:\n\nCreate a VPC with private and public subnets.\nSet up security groups for inbound and outbound rules.\nDeploy SageMaker endpoints inside the VPC.\n\nimport boto3\nsagemaker = boto3.client('sagemaker')\n\nresponse = sagemaker.create_endpoint_config(\n    EndpointConfigName='my-vpc-endpoint-config',\n    ProductionVariants=[\n        {\n            'VariantName': 'AllTraffic',\n            'ModelName': 'my-sagemaker-model',\n            'InstanceType': 'ml.m5.large',\n            'InitialInstanceCount': 1\n        }\n    ],\n    VpcConfig={\n        'Subnets': ['subnet-xxxxxx'],\n        'SecurityGroupIds': ['sg-xxxxxx']\n    }\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#deploying-and-hosting-models-using-the-sagemaker-sdk",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#deploying-and-hosting-models-using-the-sagemaker-sdk",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Deploying and Hosting Models using the SageMaker SDK",
    "text": "Deploying and Hosting Models using the SageMaker SDK\nUsing the SageMaker SDK, we can deploy models to fully managed endpoints with minimal setup.\n\nExample: Deploying a Model with the SageMaker SDK\nfrom sagemaker import Session, Model\nsession = Session()\n\nmodel = Model(\n    image_uri='your-ecr-image-uri',\n    model_data='s3://your-model-path/model.tar.gz',\n    role='arn:aws:iam::account-id:role/service-role'\n)\n\npredictor = model.deploy(instance_type='ml.m5.large', initial_instance_count=1)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#monitoring-models-in-production-with-sagemaker-model-monitor",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#monitoring-models-in-production-with-sagemaker-model-monitor",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Monitoring Models in Production with SageMaker Model Monitor",
    "text": "Monitoring Models in Production with SageMaker Model Monitor\nModel Monitor enables continuous evaluation of model predictions and data quality.\n\n\n\n\n\n\n\nMonitoring Type\nDescription\n\n\n\n\nData Quality\nDetects missing or unexpected values in input data.\n\n\nModel Bias\nIdentifies bias drift over time.\n\n\nModel Explainability\nEvaluates feature importance.\n\n\nModel Drift\nDetects changes in model predictions.\n\n\n\n\nExample: Setting Up Model Monitor\nfrom sagemaker.model_monitor import DataCaptureConfig\n\ndata_capture_config = DataCaptureConfig(\n    enable_capture=True,\n    sampling_percentage=100,\n    destination_s3_uri='s3://your-monitoring-bucket/'\n)\n\npredictor.update_data_capture_config(data_capture_config)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#detecting-data-distribution-changes-with-sagemaker-clarify",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#detecting-data-distribution-changes-with-sagemaker-clarify",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Detecting Data Distribution Changes with SageMaker Clarify",
    "text": "Detecting Data Distribution Changes with SageMaker Clarify\nSageMaker Clarify helps identify shifts in data distribution that can impact model performance.\n\nExample: Running a Baseline Drift Analysis\nfrom sagemaker.clarify import BiasConfig, ModelMonitor\n\nbias_config = BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name='feature-column',\n    facet_values_or_threshold=[0]\n)\n\nmonitor = ModelMonitor(\n    job_definition_name='clarify-job',\n    role='arn:aws:iam::account-id:role/service-role',\n    bias_config=bias_config\n)\n\nmonitor.create_monitoring_schedule()"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#conclusion",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#conclusion",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Conclusion",
    "text": "Conclusion\nBy configuring SageMaker endpoints within a VPC, deploying with the SageMaker SDK, and monitoring data with Model Monitor and Clarify, we ensure secure and efficient model operations. These best practices help maintain model reliability and performance in production environments."
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#conclusion",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "examples/glue.html",
    "href": "examples/glue.html",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "",
    "text": "This document compares AWS Glue Data Catalog, AWS Glue DataBrew, and AWS Glue Studio, highlighting their use cases, differences, and providing code examples."
  },
  {
    "objectID": "examples/glue.html#aws-glue-data-catalog",
    "href": "examples/glue.html#aws-glue-data-catalog",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue Data Catalog",
    "text": "AWS Glue Data Catalog\nAWS Glue Data Catalog is a metadata repository for organizing and discovering datasets across AWS services.\n\nKey Features:\n\nCentralized metadata management\nSchema discovery and versioning\nIntegration with Athena, Redshift, and EMR\n\n\n\nSample Code: Creating a Glue Database\nimport boto3\n\ndef create_glue_database(database_name):\n    glue_client = boto3.client('glue')\n    response = glue_client.create_database(\n        DatabaseInput={\n            'Name': database_name,\n            'Description': 'Example Glue Database'\n        }\n    )\n    return response\n\n# Example usage\ncreate_glue_database(\"my_glue_db\")"
  },
  {
    "objectID": "examples/glue.html#aws-glue-databrew",
    "href": "examples/glue.html#aws-glue-databrew",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue DataBrew",
    "text": "AWS Glue DataBrew\nAWS Glue DataBrew is a visual data preparation tool for cleaning and transforming data without coding.\n\nKey Features:\n\nNo-code data transformation\nIntegration with S3, Redshift, and RDS\nAutomated data profiling and anomaly detection\n\n\n\nSample Code: Creating a DataBrew Dataset\nimport boto3\n\ndef create_databrew_dataset(name, s3_path, role_arn):\n    databrew_client = boto3.client('databrew')\n    response = databrew_client.create_dataset(\n        Name=name,\n        Input={\n            'S3InputDefinition': {'Bucket': s3_path}\n        },\n        RoleArn=role_arn\n    )\n    return response\n\n# Example usage\ncreate_databrew_dataset(\"my_dataset\", \"my-s3-bucket\", \"arn:aws:iam::123456789012:role/my-role\")"
  },
  {
    "objectID": "examples/glue.html#aws-glue-studio",
    "href": "examples/glue.html#aws-glue-studio",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue Studio",
    "text": "AWS Glue Studio\nAWS Glue Studio provides a graphical interface to visually build, run, and monitor AWS Glue ETL jobs.\n\nKey Features:\n\nDrag-and-drop interface for ETL workflows\nSupports Spark-based transformations\nIntegration with Glue Data Catalog\n\n\n\nSample Code: Creating a Glue Job\nimport boto3\n\ndef create_glue_job(job_name, script_s3_path, role_arn):\n    glue_client = boto3.client('glue')\n    response = glue_client.create_job(\n        Name=job_name,\n        Role=role_arn,\n        Command={\n            'Name': 'glueetl',\n            'ScriptLocation': script_s3_path\n        },\n        GlueVersion='2.0'\n    )\n    return response\n\n# Example usage\ncreate_glue_job(\"my_glue_job\", \"s3://my-bucket/scripts/job.py\", \"arn:aws:iam::123456789012:role/my-role\")"
  },
  {
    "objectID": "examples/storage.html",
    "href": "examples/storage.html",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "",
    "text": "AWS offers multiple storage options, each optimized for different use cases. This document compares Amazon S3, EFS, FSx, and EBS based on features, performance, and use cases."
  },
  {
    "objectID": "examples/storage.html#amazon-s3-simple-storage-service",
    "href": "examples/storage.html#amazon-s3-simple-storage-service",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon S3 (Simple Storage Service)",
    "text": "Amazon S3 (Simple Storage Service)\n\nUse Case: Backup, Data Lakes, Static Websites\nFeatures:\n\nScalable, durable object storage\nMultiple storage classes (Standard, Intelligent-Tiering, Glacier)\nServer-side encryption and lifecycle management\n\n\n\nExample: Upload a File to S3 Using Boto3\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('local_file.txt', 'my-bucket', 'uploaded_file.txt')"
  },
  {
    "objectID": "examples/storage.html#amazon-efs-elastic-file-system",
    "href": "examples/storage.html#amazon-efs-elastic-file-system",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EFS (Elastic File System)",
    "text": "Amazon EFS (Elastic File System)\n\nUse Case: Multi-instance shared storage, Kubernetes, Serverless\nFeatures:\n\nPOSIX-compliant, automatically scales\nSupports NFS protocol\nMountable across multiple EC2 instances\n\n\n\nExample: Mount EFS on an EC2 Instance\nsudo mount -t nfs4 -o nfsvers=4.1 fs-0123456789abcdef0.efs.us-east-1.amazonaws.com:/ efs-mount-point"
  },
  {
    "objectID": "examples/storage.html#amazon-fsx",
    "href": "examples/storage.html#amazon-fsx",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon FSx",
    "text": "Amazon FSx\n\nUse Case: High-performance file storage for Windows/Linux applications\nFeatures:\n\nOptimized for Windows File Server or Lustre (HPC)\nFully managed with built-in backups\n\n\n\nExample: Mount FSx for Windows on EC2\nNew-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\fs-0123456789abcdef0.example.com\\share\""
  },
  {
    "objectID": "examples/storage.html#amazon-ebs-elastic-block-store",
    "href": "examples/storage.html#amazon-ebs-elastic-block-store",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EBS (Elastic Block Store)",
    "text": "Amazon EBS (Elastic Block Store)\n\nUse Case: Low-latency storage for EC2, Databases, Big Data\nFeatures:\n\nPersistent block storage for EC2\nSSD (gp3, io2) and HDD (sc1, st1) options\nSnapshots for backup & recovery\n\n\n\nExample: Attach an EBS Volume to EC2\naws ec2 attach-volume --volume-id vol-0123456789abcdef0 --instance-id i-0123456789abcdef0 --device /dev/xvdf"
  },
  {
    "objectID": "examples/search.html",
    "href": "examples/search.html",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "",
    "text": "AWS provides two primary search services: Amazon Kendra and Amazon OpenSearch Service. While both enable search capabilities, they serve different use cases and have distinct strengths."
  },
  {
    "objectID": "examples/search.html#overview-table",
    "href": "examples/search.html#overview-table",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Overview Table",
    "text": "Overview Table\n\n\n\n\n\n\n\n\nFeature\nAmazon Kendra\nAmazon OpenSearch Service\n\n\n\n\nType\nAI-powered enterprise search\nOpen-source search and analytics engine\n\n\nUse Case\nDocument and knowledge search\nLog analytics, full-text search, observability\n\n\nData Sources\nPre-built connectors for common enterprise sources\nStructured and unstructured data from various sources\n\n\nQuery Type\nNatural Language Processing (NLP)-based\nKeyword-based and structured queries (Elasticsearch DSL)\n\n\nIndexing\nManaged, AI-driven indexing\nFull control over indexing with custom mappings\n\n\nAccess Model\nAPI-based search queries\nREST API, Kibana dashboards, SQL\n\n\nScaling\nFully managed, auto-scaling\nCluster-based, requires management"
  },
  {
    "objectID": "examples/search.html#amazon-kendra",
    "href": "examples/search.html#amazon-kendra",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Amazon Kendra",
    "text": "Amazon Kendra\nDefinition: Amazon Kendra is an AI-powered enterprise search service that enables organizations to search structured and unstructured documents using natural language queries.\nCommon Use Cases: - Enterprise knowledge management - Internal document search - Customer support knowledge bases\n\nExample Code: Querying Kendra with Python (Boto3)\nimport boto3\n\nkendra = boto3.client('kendra')\nresponse = kendra.query(\n    IndexId='index-id',\n    QueryText='How do I reset my password?'\n)\nprint(response)"
  },
  {
    "objectID": "examples/search.html#amazon-opensearch-service",
    "href": "examples/search.html#amazon-opensearch-service",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Amazon OpenSearch Service",
    "text": "Amazon OpenSearch Service\nDefinition: Amazon OpenSearch Service is a managed search and analytics engine based on the open-source Elasticsearch and OpenSearch projects.\nCommon Use Cases: - Log and event data analysis - Real-time application search - Security monitoring and observability\n\nExample Code: Querying OpenSearch with Python\nfrom opensearchpy import OpenSearch\n\nclient = OpenSearch(\n    hosts=[{'host': 'your-opensearch-domain', 'port': 443}],\n    http_auth=('user', 'password'),\n    use_ssl=True\n)\n\nquery = {\n    \"query\": {\n        \"match\": {\"message\": \"error\"}\n    }\n}\nresponse = client.search(index=\"logs\", body=query)\nprint(response)"
  },
  {
    "objectID": "examples/search.html#conclusion",
    "href": "examples/search.html#conclusion",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Conclusion",
    "text": "Conclusion\nIf your use case involves enterprise knowledge retrieval with AI-driven natural language search, Amazon Kendra is the better option. If you need log analytics, real-time search, or structured query capabilities, OpenSearch is the preferred choice."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a collection of notes, comparisons and tools that I’ve found useful while studying for AWS MLE Associate certifications. It’s a work in progress, and I’ll be adding more content over time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AWS Machine Learning Engineer Associate",
    "section": "",
    "text": "AWS MLE Associate Badge\n\n\nWelcome, this is a work in progress. I’m creating this site to help me study for the AWS Machine Learning Engineer Associate certification. I’ll be adding more content over time, so check back for updates."
  },
  {
    "objectID": "aws-mla-exam-resources.html",
    "href": "aws-mla-exam-resources.html",
    "title": "AWS MLA Exam Resources",
    "section": "",
    "text": "Overview of AWS Machine Learning Services\nThe course is extensive, so detailed practice should help you prepare for the exam. There are a few approaches that can be taken.\nHere’s a great list of resources on reddit.\nStephan Maarek and Frank Kane’s courses are the most well known paid course at this time.\nHere’s a link to the Udemy course and practice exams\nThere’s also tutorial dojo practice exams\nAdditionally, there are some free (and paid) official AWS resources:\n\nCertification Page\nExam Guide\nFree SkillBuilder\nOfficial Free Practice Questions\nOfficial Paid Practice Questions"
  },
  {
    "objectID": "aws-definitions.html",
    "href": "aws-definitions.html",
    "title": "AWS Services Definitions",
    "section": "",
    "text": "Click each dropdown to reveal the definition/explanation of each service."
  },
  {
    "objectID": "aws-definitions.html#instructions",
    "href": "aws-definitions.html#instructions",
    "title": "AWS Services Definitions",
    "section": "",
    "text": "Click each dropdown to reveal the definition/explanation of each service."
  },
  {
    "objectID": "aws-definitions.html#data-services",
    "href": "aws-definitions.html#data-services",
    "title": "AWS Services Definitions",
    "section": "Data Services",
    "text": "Data Services\n\n\n\n\n\n\nAmazon Kinesis Data Streams\n\n\n\n\n\nA scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources.\n\n\n\n\n\n\n\n\n\nAmazon Kinesis Data Firehose\n\n\n\n\n\nA fully managed service that scales automatically to match the throughput of your data. It can batch, compress, and encrypt data before loading it into storage and analytics services.\n\n\n\n\n\n\n\n\n\nAmazon MSK (Managed Streaming for Apache Kafka)\n\n\n\n\n\nA fully managed service that makes it easy to build and run applications using Apache Kafka, an open-source distributed event streaming platform."
  },
  {
    "objectID": "aws-definitions.html#storage-solutions",
    "href": "aws-definitions.html#storage-solutions",
    "title": "AWS Services Definitions",
    "section": "Storage Solutions",
    "text": "Storage Solutions\n\n\n\n\n\n\nAmazon S3\n\n\n\n\n\nHighly scalable object storage service offering industry-leading durability, security, and performance. Store and retrieve any amount of data from anywhere.\n\n\n\n\n\n\n\n\n\nAmazon EBS, FSx, and EFS\n\n\n\n\n\n\nAmazon EBS (Elastic Block Store): Block-level storage for EC2 instances.\nAmazon FSx: Fully managed file storage for Windows, Lustre, and other file systems.\nAmazon EFS (Elastic File System): Fully managed, scalable NFS file storage."
  },
  {
    "objectID": "aws-definitions.html#big-data-analytics",
    "href": "aws-definitions.html#big-data-analytics",
    "title": "AWS Services Definitions",
    "section": "Big Data & Analytics",
    "text": "Big Data & Analytics\n\n\n\n\n\n\nApache Hadoop\n\n\n\n\n\nAn open-source framework for distributed processing of large datasets across clusters of computers.\n\n\n\n\n\n\n\n\n\nAWS Glue\n\n\n\n\n\nA fully managed extract, transform, and load (ETL) service that simplifies data preparation for analytics.\n\n\n\n\n\n\n\n\n\nAmazon EMR (Elastic MapReduce)\n\n\n\n\n\nA cloud big data platform for processing vast amounts of data using open-source frameworks like Apache Spark, Hadoop, and Presto.\n\n\n\n\n\n\n\n\n\nAmazon Athena\n\n\n\n\n\nAn interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\n\n\n\n\n\n\n\n\n\nAWS Glue DataBrew\n\n\n\n\n\nA visual data preparation tool that makes it easy for data analysts and data scientists to clean and normalize data.\n\n\n\n\n\n\n\n\n\nAWS Glue Data Quality\n\n\n\n\n\nA service that helps you measure and monitor the quality of your data.\n\n\n\n\n\n\n\n\n\nAmazon Kinesis\n\n\n\n\n\nA platform on AWS to collect, process, and analyze real-time, streaming data.\n\n\n\n\n\n\n\n\n\nAWS Lake Formation\n\n\n\n\n\nA service that makes it easy to set up a secure data lake in days.\n\n\n\n\n\n\n\n\n\nAmazon Managed Service for Apache Flink\n\n\n\n\n\nA fully managed service that enables you to build and run Apache Flink applications.\n\n\n\n\n\n\n\n\n\nAmazon OpenSearch Service\n\n\n\n\n\nA fully managed service that makes it easy to deploy, secure, and operate OpenSearch at scale.\n\n\n\n\n\n\n\n\n\nAmazon QuickSight\n\n\n\n\n\nA scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud.\n\n\n\n\n\n\n\n\n\nAmazon Redshift\n\n\n\n\n\nA fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools."
  },
  {
    "objectID": "aws-definitions.html#aiml-services",
    "href": "aws-definitions.html#aiml-services",
    "title": "AWS Services Definitions",
    "section": "AI/ML Services",
    "text": "AI/ML Services\n\n\n\n\n\n\nAmazon Bedrock\n\n\n\n\n\nA fully managed service that provides access to foundation models from leading AI providers via a single API.\n\n\n\n\n\n\n\n\n\nAmazon Comprehend\n\n\n\n\n\nA natural language processing (NLP) service that uses machine learning to analyze text for insights, such as sentiment and key phrases.\n\n\n\n\n\n\n\n\n\nAmazon Personalize\n\n\n\n\n\nA machine learning service for building real-time personalized recommendations.\n\n\n\n\n\n\n\n\n\nAmazon Lex\n\n\n\n\n\nA fully managed AI service for building conversational interfaces using speech and text.\n\n\n\n\n\n\n\n\n\nAmazon Polly\n\n\n\n\n\nA text-to-speech service that converts written text into lifelike speech.\n\n\n\n\n\n\n\n\n\nAmazon Rekognition\n\n\n\n\n\nA deep learning-based image and video analysis service for object detection, facial recognition, and content moderation.\n\n\n\n\n\n\n\n\n\nAmazon Textract\n\n\n\n\n\nAn AI service that automatically extracts text, handwriting, and data from scanned documents.\n\n\n\n\n\n\n\n\n\nAmazon Transcribe\n\n\n\n\n\nA speech-to-text service that uses machine learning to convert spoken language into written text.\n\n\n\n\n\n\n\n\n\nAmazon Kendra\n\n\n\n\n\nAn intelligent search service that uses machine learning to provide highly accurate search results.\n\n\n\n\n\n\n\n\n\nAmazon SageMaker\n\n\n\n\n\nA fully managed service that provides tools to build, train, and deploy machine learning models at scale.\n\n\n\n\n\n\n\n\n\nAWS Panorama\n\n\n\n\n\nA machine learning appliance and software development kit (SDK) that enables adding computer vision to on-premises cameras.\n\n\n\n\n\n\n\n\n\nAmazon CodeWhisperer\n\n\n\n\n\nAn AI-powered coding assistant that provides real-time code suggestions.\n\n\n\n\n\n\n\n\n\nAmazon HealthLake\n\n\n\n\n\nA HIPAA-eligible service that uses machine learning to store, transform, and analyze health data.\n\n\n\n\n\n\n\n\n\nAmazon Forecast\n\n\n\n\n\nA time-series forecasting service that uses machine learning to predict future trends.\n\n\n\n\n\n\n\n\n\nAmazon Fraud Detector\n\n\n\n\n\nAn AI service that helps identify potentially fraudulent activities in real-time."
  },
  {
    "objectID": "aws-definitions.html#sagemaker-ecosystem",
    "href": "aws-definitions.html#sagemaker-ecosystem",
    "title": "AWS Services Definitions",
    "section": "SageMaker Ecosystem",
    "text": "SageMaker Ecosystem\n\n\n\n\n\n\nAmazon SageMaker Studio\n\n\n\n\n\nA fully integrated development environment (IDE) for machine learning with a single, web-based visual interface. Provides capabilities for data preparation, model building, training, debugging, and deployment.\n\n\n\n\n\n\n\n\n\nBuilt-in SageMaker Algorithms\n\n\n\n\n\nPrebuilt algorithms optimized for performance and scalability: - Linear Learner (Regression, Classification) - XGBoost (Gradient Boosting) - DeepAR (Time Series Forecasting) - BlazingText (Word2Vec, Text Classification) - Object Detection (Image Analysis) - Image Classification (ResNet, VGG, etc.) - Semantic Segmentation - Factorization Machines (Recommender Systems) - Neural Topic Model (Topic Modeling) - K-Means Clustering - PCA (Dimensionality Reduction) - LDA (Latent Dirichlet Allocation) - Sequence-to-Sequence (Translation, Text Generation) - Random Cut Forest (Anomaly Detection) - And more…\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Pipelines\n\n\n\n\n\nA CI/CD service for automating machine learning workflows, including data preprocessing, model training, and deployment. Features include: - Step-based workflow definition - Integration with SageMaker Training, Processing, and Model Registry - Lineage tracking and experiment management - Execution via SDK or AWS Console\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Model Monitor\n\n\n\n\n\nContinuously monitors deployed models for data drift and quality issues. Key features: - Baseline creation from training data - Monitoring statistics and visualizations - Alerts and automated remediation - Integration with SageMaker Pipelines\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Inference & Deployment\n\n\n\n\n\nMultiple options for deploying models: - Real-time inference: Deploy models as endpoints with auto-scaling - Batch Transform: Process large datasets asynchronously - Serverless inference: Deploy without managing infrastructure - Edge deployment: Use SageMaker Edge Manager for on-device inference\n\n\n\n\n\n\n\n\n\nAmazon SageMaker APIs\n\n\n\n\n\nSageMaker provides SDKs and APIs for integrating ML into applications: - Boto3 (Python SDK): Automate SageMaker workflows - SageMaker SDK: Train, deploy, and manage models - SageMaker JumpStart: Access pre-trained models and solutions - Model Registry API: Manage model versions and deployments\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Feature Store\n\n\n\n\n\nA fully managed repository to store, update, and retrieve ML features. Supports: - Online and offline stores for low-latency and batch processing - Feature versioning and governance - Integration with SageMaker Pipelines and Model Monitor\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Autopilot\n\n\n\n\n\nAutomates model selection, hyperparameter tuning, and feature engineering. Provides: - Explainability reports - Candidate model evaluation - Deployable models without manual tuning\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Debugger\n\n\n\n\n\nAutomatically detects training issues, such as overfitting and vanishing gradients. Key features: - Real-time monitoring of training metrics - Integration with TensorBoard - Automated rule-based alerts\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Ground Truth\n\n\n\n\n\nA data labeling service that supports: - Human-in-the-loop labeling - Automated labeling with active learning - Integration with Amazon Mechanical Turk and private workforce\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Data Wrangler\n\n\n\n\n\nSimplifies data preparation and feature engineering with: - Pre-built data transformations - Automated data quality checks - Integration with SageMaker Pipelines\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Clarify\n\n\n\n\n\nProvides explainability and bias detection tools: - Feature importance analysis - Bias detection in datasets and models - Integration with Model Monitor for ongoing fairness assessment"
  },
  {
    "objectID": "aws-definitions.html#security-networking",
    "href": "aws-definitions.html#security-networking",
    "title": "AWS Services Definitions",
    "section": "Security & Networking",
    "text": "Security & Networking\n\n\n\n\n\n\nAmazon VPC Components\n\n\n\n\n\n\nVPC: A private, isolated cloud network.\nSubnets: Segments of VPC IP address ranges.\nInternet Gateway: Enables internet access for VPC.\nNAT Gateway: Allows private subnets to access the internet securely.\nSecurity Groups: Instance-level firewalls for controlling inbound/outbound traffic.\n\n\n\n\n\n\n\n\n\n\nAWS Key Management Service (KMS)\n\n\n\n\n\nA fully managed service that makes it easy to create, control, and use cryptographic keys to encrypt your data."
  },
  {
    "objectID": "aws-definitions.html#devops-cicd",
    "href": "aws-definitions.html#devops-cicd",
    "title": "AWS Services Definitions",
    "section": "DevOps & CI/CD",
    "text": "DevOps & CI/CD\n\n\n\n\n\n\nAWS CodePipeline\n\n\n\n\n\nA fully managed continuous integration and continuous delivery (CI/CD) service that automates software release pipelines.\n\n\n\n\n\n\n\n\n\nContainers & Orchestration\n\n\n\n\n\n\nDocker: A platform for building, shipping, and running applications in containers.\nAmazon ECS: A scalable container orchestration service.\nAmazon ECR: A fully managed container registry for storing Docker images.\nAmazon EKS: A managed Kubernetes service for running containerized applications."
  },
  {
    "objectID": "aws-definitions.html#monitoring-management",
    "href": "aws-definitions.html#monitoring-management",
    "title": "AWS Services Definitions",
    "section": "Monitoring & Management",
    "text": "Monitoring & Management\n\n\n\n\n\n\nAmazon CloudWatch\n\n\n\n\n\nA monitoring and observability service for logs, metrics, alarms, and application performance.\n\n\n\n\n\n\n\n\n\nAWS Glue (ETL)\n\n\n\n\n\nA fully managed ETL (Extract, Transform, Load) service that prepares and integrates data for analytics and machine learning."
  },
  {
    "objectID": "aws-definitions.html#application-integration",
    "href": "aws-definitions.html#application-integration",
    "title": "AWS Services Definitions",
    "section": "Application Integration",
    "text": "Application Integration\n\n\n\n\n\n\nAmazon EventBridge\n\n\n\n\n\nA serverless event bus service that makes it easy to connect applications using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services.\n\n\n\n\n\n\n\n\n\nAmazon Managed Workflows for Apache Airflow (Amazon MWAA)\n\n\n\n\n\nA managed orchestration service for Apache Airflow that makes it easy to set up and operate end-to-end data pipelines in the cloud.\n\n\n\n\n\n\n\n\n\nAmazon Simple Notification Service (Amazon SNS)\n\n\n\n\n\nA fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication.\n\n\n\n\n\n\n\n\n\nAmazon Simple Queue Service (Amazon SQS)\n\n\n\n\n\nA fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\n\n\n\n\n\n\n\n\n\nAWS Step Functions\n\n\n\n\n\nA serverless orchestration service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications."
  },
  {
    "objectID": "aws-definitions.html#cloud-financial-management",
    "href": "aws-definitions.html#cloud-financial-management",
    "title": "AWS Services Definitions",
    "section": "Cloud Financial Management",
    "text": "Cloud Financial Management\n\n\n\n\n\n\nAWS Billing and Cost Management\n\n\n\n\n\nA service that helps you pay your AWS bill, monitor your usage, and budget your costs.\n\n\n\n\n\n\n\n\n\nAWS Budgets\n\n\n\n\n\nA service that lets you set custom cost and usage budgets and receive alerts when you exceed them.\n\n\n\n\n\n\n\n\n\nAWS Cost Explorer\n\n\n\n\n\nA service that enables you to visualize, understand, and manage your AWS costs and usage over time."
  },
  {
    "objectID": "aws-comparison-tables.html",
    "href": "aws-comparison-tables.html",
    "title": "AWS Comparison Tables",
    "section": "",
    "text": "Feature\nKinesis Data Streams\nAWS Firehose\nMSK (Managed Kafka)\n\n\n\n\nLatency\nReal-time (sub-second)\nNear real-time (60s minimum)\nReal-time (sub-second)\n\n\nScaling\nManual provisioning of shards\nAutomatic scaling\nManual provisioning of brokers\n\n\nData Retention\n24h by default, up to 365 days\nNo retention (immediate processing)\nConfigurable, unlimited\n\n\nUse Cases\nReal-time analytics, processing logs\nData loading into S3, Redshift, ES\nEvent streaming, log aggregation\n\n\nThroughput\nMB/s per shard\nAutomatic up to GB/s\nMB/s per broker\n\n\nCost Model\nPer shard hour\nPer GB processed\nPer broker hour"
  },
  {
    "objectID": "aws-comparison-tables.html#streaming-services",
    "href": "aws-comparison-tables.html#streaming-services",
    "title": "AWS Comparison Tables",
    "section": "",
    "text": "Feature\nKinesis Data Streams\nAWS Firehose\nMSK (Managed Kafka)\n\n\n\n\nLatency\nReal-time (sub-second)\nNear real-time (60s minimum)\nReal-time (sub-second)\n\n\nScaling\nManual provisioning of shards\nAutomatic scaling\nManual provisioning of brokers\n\n\nData Retention\n24h by default, up to 365 days\nNo retention (immediate processing)\nConfigurable, unlimited\n\n\nUse Cases\nReal-time analytics, processing logs\nData loading into S3, Redshift, ES\nEvent streaming, log aggregation\n\n\nThroughput\nMB/s per shard\nAutomatic up to GB/s\nMB/s per broker\n\n\nCost Model\nPer shard hour\nPer GB processed\nPer broker hour"
  },
  {
    "objectID": "aws-comparison-tables.html#storage-services",
    "href": "aws-comparison-tables.html#storage-services",
    "title": "AWS Comparison Tables",
    "section": "Storage Services",
    "text": "Storage Services\n\n\n\n\n\n\n\n\n\n\nFeature\nS3\nEBS\nEFS\nFSx\n\n\n\n\nType\nObject Storage\nBlock Storage\nFile System\nFile System\n\n\nLatency\nms (varies)\nSub-ms\nms\nSub-ms to ms\n\n\nScalability\nUnlimited\nUp to 64TB per volume\nAutomatic\nAutomatic\n\n\nAccess Pattern\nWeb/API\nMount as drive\nNFS mount\nSMB/Lustre\n\n\nUse Cases\nStatic files, backups\nOS drives, databases\nShared files, web serving\nWindows/HPC workloads\n\n\nAvailability\n99.99%\n99.99%\n99.99%\n99.99%\n\n\nCost Model\nGB-month + requests\nGB-month provisioned\nGB-month used\nGB-month + throughput"
  },
  {
    "objectID": "aws-comparison-tables.html#aiml-services",
    "href": "aws-comparison-tables.html#aiml-services",
    "title": "AWS Comparison Tables",
    "section": "AI/ML Services",
    "text": "AI/ML Services\n\n\n\n\n\n\n\n\n\n\nFeature\nComprehend\nPersonalize\nBedrock\nSageMaker\n\n\n\n\nType\nManaged NLP\nManaged Recommendations\nFoundation Models\nML Platform\n\n\nTraining Required\nNo\nYes (with your data)\nNo\nYes\n\n\nCustomization\nLimited\nHigh\nModel fine-tuning\nComplete control\n\n\nLatency\n~100ms\n~100ms\nVaries by model\nDepends on deployment\n\n\nUse Cases\nText analysis\nRecommendations\nGen AI applications\nCustom ML models\n\n\nScaling\nAutomatic\nAutomatic\nAutomatic\nManual/Auto\n\n\nCost Model\nPer unit processed\nPer training hour + predictions\nPer token/request\nPer instance hour"
  },
  {
    "objectID": "aws-comparison-tables.html#container-services",
    "href": "aws-comparison-tables.html#container-services",
    "title": "AWS Comparison Tables",
    "section": "Container Services",
    "text": "Container Services\n\n\n\n\n\n\n\n\n\n\nFeature\nECS\nEKS\nApp Runner\nLambda Container\n\n\n\n\nComplexity\nLow\nHigh\nVery Low\nLow\n\n\nControl\nModerate\nHigh\nLow\nLow\n\n\nScaling\nAuto/Manual\nAuto/Manual\nAutomatic\nAutomatic\n\n\nMax Container Size\n30GB\n30GB\n10GB\n10GB\n\n\nCold Start\nNo\nNo\nYes\nYes\n\n\nUse Cases\nMicroservices\nComplex orchestration\nWeb apps\nServerless containers\n\n\nCost Model\nPer task hour\nPer cluster hour + tasks\nPer vCPU/GB\nPer request + duration"
  },
  {
    "objectID": "aws-comparison-tables.html#data-analytics-services",
    "href": "aws-comparison-tables.html#data-analytics-services",
    "title": "AWS Comparison Tables",
    "section": "Data Analytics Services",
    "text": "Data Analytics Services\n\n\n\n\n\n\n\n\n\n\nFeature\nEMR\nGlue\nAthena\nRedshift\n\n\n\n\nType\nManaged Hadoop\nETL Service\nQuery Service\nData Warehouse\n\n\nProcessing\nBatch/Stream\nBatch\nInteractive\nBatch/Interactive\n\n\nLatency\nMinutes\nMinutes\nSeconds\nSub-second to seconds\n\n\nScale\nManual clusters\nAutomatic\nAutomatic\nManual/Auto\n\n\nUse Cases\nBig data processing\nData preparation\nAd-hoc queries\nData warehousing\n\n\nSetup Time\nHours\nMinutes\nMinutes\nHours\n\n\nCost Model\nPer instance hour\nPer DPU hour\nPer TB scanned\nPer node hour"
  },
  {
    "objectID": "aws-comparison-tables.html#monitoring-services",
    "href": "aws-comparison-tables.html#monitoring-services",
    "title": "AWS Comparison Tables",
    "section": "Monitoring Services",
    "text": "Monitoring Services\n\n\n\n\n\n\n\n\n\n\nFeature\nCloudWatch\nX-Ray\nPrometheus\nGrafana\n\n\n\n\nType\nMetrics/Logs\nTracing\nMetrics\nVisualization\n\n\nData Retention\n15 months\n30 days\nConfigurable\nDepends on source\n\n\nGranularity\n1s to 1h\nPer request\nCustom\nCustom\n\n\nUse Cases\nBasic monitoring\nDistributed tracing\nContainer monitoring\nDashboarding\n\n\nIntegration\nAWS native\nAWS services\nKubernetes\nMultiple sources\n\n\nCost Model\nPer metric/GB\nPer trace\nStorage based\nBy features"
  },
  {
    "objectID": "aws-comparison-tables.html#ml-model-deployment-options",
    "href": "aws-comparison-tables.html#ml-model-deployment-options",
    "title": "AWS Comparison Tables",
    "section": "ML Model Deployment Options",
    "text": "ML Model Deployment Options\n\n\n\n\n\n\n\n\n\n\nFeature\nSageMaker Endpoints\nSageMaker Serverless\nLambda\nECS/EKS\n\n\n\n\nCold Start\nNo\nYes\nYes\nNo\n\n\nAuto-scaling\nYes\nYes\nYes\nYes\n\n\nMax Duration\nUnlimited\n15 min\n15 min\nUnlimited\n\n\nCost Model\nPer instance hour\nPer request\nPer request\nPer container\n\n\nUse Cases\nSteady traffic\nVariable traffic\nLightweight inference\nCustom deployment\n\n\nMax Model Size\nInstance dependent\n10GB\n10GB\nNo limit\n\n\nConcurrency\nHigh\nLimited\nLimited\nHigh"
  },
  {
    "objectID": "examples/datastreams.html",
    "href": "examples/datastreams.html",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "",
    "text": "This document compares AWS streaming solutions: Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Streaming for Apache Kafka (MSK), and Amazon Managed Workflows for Apache Airflow (AMAA). We’ll explore their use cases, differences, and provide code examples."
  },
  {
    "objectID": "examples/datastreams.html#kinesis-data-streams",
    "href": "examples/datastreams.html#kinesis-data-streams",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Kinesis Data Streams",
    "text": "Kinesis Data Streams\nAWS Kinesis Data Streams is a real-time streaming service for collecting, processing, and analyzing large volumes of data. It provides low-latency access and supports custom consumer applications.\n\nKey Features:\n\nReal-time data processing\nCustom applications using AWS SDKs\nRetention period up to 7 days\n\n\n\nSample Code: Writing to Kinesis Data Stream\nimport boto3\nimport json\n\ndef put_record_to_kinesis(stream_name, data):\n    kinesis_client = boto3.client('kinesis')\n    response = kinesis_client.put_record(\n        StreamName=stream_name,\n        Data=json.dumps(data),\n        PartitionKey=\"partition-1\"\n    )\n    return response\n\n# Example usage\nrecord = {\"event\": \"click\", \"user\": \"12345\"}\nput_record_to_kinesis(\"my-stream\", record)"
  },
  {
    "objectID": "examples/datastreams.html#kinesis-data-firehose",
    "href": "examples/datastreams.html#kinesis-data-firehose",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Kinesis Data Firehose",
    "text": "Kinesis Data Firehose\nAWS Kinesis Data Firehose is a managed service that loads streaming data into data lakes, warehouses, or analytics services. Unlike Data Streams, Firehose handles buffering and batch writing.\n\nKey Features:\n\nFully managed with automatic scaling\nSupports S3, Redshift, Elasticsearch, and Splunk as destinations\nNo custom consumer applications required\n\n\n\nSample Code: Writing to Firehose\nimport boto3\nimport json\n\ndef put_record_to_firehose(delivery_stream_name, data):\n    firehose_client = boto3.client('firehose')\n    response = firehose_client.put_record(\n        DeliveryStreamName=delivery_stream_name,\n        Record={\"Data\": json.dumps(data) + \"\\n\"}\n    )\n    return response\n\n# Example usage\nrecord = {\"event\": \"purchase\", \"amount\": 100}\nput_record_to_firehose(\"my-firehose-stream\", record)"
  },
  {
    "objectID": "examples/datastreams.html#amazon-managed-streaming-for-apache-kafka-msk",
    "href": "examples/datastreams.html#amazon-managed-streaming-for-apache-kafka-msk",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Amazon Managed Streaming for Apache Kafka (MSK)",
    "text": "Amazon Managed Streaming for Apache Kafka (MSK)\nMSK provides a fully managed Kafka cluster that integrates with AWS services. It’s ideal for applications needing open-source Kafka features.\n\nKey Features:\n\nFully managed Kafka clusters\nSecure with IAM authentication\nSupports standard Kafka clients\n\n\n\nSample Code: Producing Messages to Kafka\nfrom kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers=['b-1.msk-cluster.amazonaws.com:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\nproducer.send(\"my-topic\", {\"event\": \"login\", \"user\": \"user_123\"})\nproducer.flush()"
  },
  {
    "objectID": "examples/datastreams.html#amazon-managed-workflows-for-apache-airflow-amaa",
    "href": "examples/datastreams.html#amazon-managed-workflows-for-apache-airflow-amaa",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Amazon Managed Workflows for Apache Airflow (AMAA)",
    "text": "Amazon Managed Workflows for Apache Airflow (AMAA)\nAMAA is a managed workflow orchestration service based on Apache Airflow. Unlike real-time streaming services, it is used for data pipeline scheduling and orchestration.\n\nKey Features:\n\nFully managed Airflow environment\nDAG-based workflow scheduling\nIntegrates with AWS services\n\n\n\nSample Code: Defining an Airflow DAG in AMAA\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef my_task():\n    print(\"Running my task\")\n\ndefault_args = {\"start_date\": datetime(2024, 2, 1)}\n\ndag = DAG(\"my_dag\", default_args=default_args, schedule_interval=\"@daily\")\n\nstart = DummyOperator(task_id=\"start\", dag=dag)\ntask = PythonOperator(task_id=\"run_task\", python_callable=my_task, dag=dag)\n\nstart &gt;&gt; task"
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html",
    "href": "examples/emr-vs-hadoop-vs-spark.html",
    "title": "EMR vs Hadoop vs Spark",
    "section": "",
    "text": "This document compares Amazon EMR, Apache Hadoop, and Apache Spark, highlighting their use cases, differences, and providing code examples."
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#amazon-emr",
    "href": "examples/emr-vs-hadoop-vs-spark.html#amazon-emr",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Amazon EMR",
    "text": "Amazon EMR\nAWS Elastic MapReduce (EMR) is a cloud-based big data processing service that simplifies running frameworks like Hadoop and Spark.\n\nKey Features:\n\nManaged Hadoop, Spark, Presto, and more\nAuto-scaling and cost-efficient\nTight integration with AWS services\n\n\n\nSample Code: Submitting a Spark Job to EMR\nimport boto3\n\ndef submit_spark_job(cluster_id, script_s3_path):\n    emr_client = boto3.client('emr')\n    response = emr_client.add_job_flow_steps(\n        JobFlowId=cluster_id,\n        Steps=[{\n            'Name': 'Spark Job',\n            'ActionOnFailure': 'CONTINUE',\n            'HadoopJarStep': {\n                'Jar': 'command-runner.jar',\n                'Args': ['spark-submit', script_s3_path]\n            }\n        }]\n    )\n    return response\n\n# Example usage\nsubmit_spark_job(\"j-XYZ123\", \"s3://my-bucket/my-spark-job.py\")"
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#apache-hadoop",
    "href": "examples/emr-vs-hadoop-vs-spark.html#apache-hadoop",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Apache Hadoop",
    "text": "Apache Hadoop\nHadoop is an open-source framework for distributed storage and processing of large datasets using MapReduce.\n\nKey Features:\n\nDistributed computing using HDFS and YARN\nBatch processing of large data volumes\nEcosystem includes Hive, Pig, HBase, and more\n\n\n\nSample Code: Hadoop Word Count Job\nimport java.io.IOException;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.*;\n\npublic class WordCount {\n    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {\n        private final static IntWritable one = new IntWritable(1);\n        private Text word = new Text();\n\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            for (String token : value.toString().split(\" \")) {\n                word.set(token);\n                context.write(word, one);\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#apache-spark",
    "href": "examples/emr-vs-hadoop-vs-spark.html#apache-spark",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Apache Spark",
    "text": "Apache Spark\nSpark is a fast, in-memory data processing framework designed for large-scale data analytics and machine learning.\n\nKey Features:\n\nIn-memory processing for speed\nSupports batch, streaming, and machine learning workloads\nAPIs in Python, Java, Scala, and R\n\n\n\nSample Code: Running a PySpark Job\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Example\").getOrCreate()\ndata = [(\"Alice\", 34), (\"Bob\", 45)]\ndf = spark.createDataFrame(data, [\"Name\", \"Age\"])\ndf.show()"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html",
    "href": "examples/flink-vs-kinesis.html",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Apache Flink and AWS Kinesis are both powerful tools for real-time data processing, but they serve different purposes and have different architectures. Below is a comparison of the two, with definitions, explanations, and code examples.\n\n\n\n\nApache Flink is an open-source stream-processing framework used for distributed processing of data streams. It is designed for stateful computations over unbounded data streams and can handle large-scale, real-time analytics.\n\nKey Features:\n\nReal-time stream processing\nFault tolerance with checkpointing\nSupports batch processing (as a special case of stream processing)\nAdvanced windowing and event time processing\nSupport for complex event processing (CEP)\n\n\n\n\n\nAWS Kinesis is a fully managed service by Amazon Web Services for real-time data streaming and processing. It allows users to collect, process, and analyze real-time data at massive scale.\n\nKey Features:\n\nFully managed, scalable stream processing\nIntegration with AWS ecosystem (e.g., Lambda, Redshift, etc.)\nReal-time analytics\nSupports data ingestion, storage, and analytics on streaming data\nAuto-scaling based on data volume\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nApache Flink\nAWS Kinesis\n\n\n\n\nPurpose\nStream processing and complex event processing\nReal-time data streaming and ingestion\n\n\nManagement\nSelf-managed (on-premise or cloud)\nFully managed service by AWS\n\n\nFault Tolerance\nCheckpointing and state management\nBuilt-in replication and data retention\n\n\nUse Case\nComplex analytics, real-time ETL, CEP\nData ingestion, analytics, and streaming apps\n\n\nScaling\nManual or automated scaling\nAuto-scaling based on data volume\n\n\nIntegrations\nKafka, HDFS, Cassandra, Elasticsearch\nAWS ecosystem (Lambda, Redshift, etc.)\n\n\nLanguage Support\nJava, Scala, Python, SQL\nKinesis Client Library (Java, Python, etc.)\n\n\nLatency\nSub-second latency (low-latency)\nTypically sub-second (low-latency)\n\n\n\n\n\n\n\n\nHere is a basic example of a Flink job that processes a stream of events:\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\npublic class FlinkExample {\n    public static void main(String[] args) throws Exception {\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // Example: Create a stream of integers and map them to a string\n        DataStream&lt;Integer&gt; numbers = env.fromElements(1, 2, 3, 4, 5);\n        \n        DataStream&lt;String&gt; result = numbers.map(new MapFunction&lt;Integer, String&gt;() {\n            @Override\n            public String map(Integer value) throws Exception {\n                return \"Number: \" + value;\n            }\n        });\n\n        result.print(); // Output the result\n        \n        env.execute(\"Flink Streaming Example\");\n    }\n}\nIn this example, Flink reads from a stream of integers, processes them, and outputs a transformed result. AWS Kinesis Example\nBelow is an example of how to produce and consume data from Kinesis streams using the AWS SDK for Python (Boto3).\nKinesis Producer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef send_to_kinesis(stream_name, data):\n    payload = json.dumps(data)\n    kinesis.put_record(\n        StreamName=stream_name,\n        Data=payload,\n        PartitionKey=\"partitionkey\"\n    )\n\n# Example usage\nsend_to_kinesis(\"my-kinesis-stream\", {\"event\": \"start\", \"timestamp\": \"2025-02-03T12:00:00\"})\nKinesis Consumer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef consume_from_kinesis(stream_name):\n    shard_iterator = kinesis.get_shard_iterator(\n        StreamName=stream_name,\n        ShardId='shardId-000000000000',\n        ShardIteratorType='TRIM_HORIZON'\n    )['ShardIterator']\n    \n    while True:\n        record_response = kinesis.get_records(ShardIterator=shard_iterator, Limit=10)\n        for record in record_response['Records']:\n            print(json.loads(record['Data']))\n        \n        shard_iterator = record_response['NextShardIterator']\n\n# Example usage\nconsume_from_kinesis(\"my-kinesis-stream\")\nIn the Kinesis example, we have a producer that sends data to a stream and a consumer that reads data from it. Conclusion\nBoth Apache Flink and AWS Kinesis are valuable tools in the realm of real-time data processing. Flink is a stream-processing framework ideal for complex event processing and advanced analytics, while Kinesis provides a fully managed solution for collecting, processing, and analyzing streaming data.\nThe choice between the two depends on factors such as the scale of your infrastructure, integration needs, and whether you prefer a fully managed service (Kinesis) or a more customizable stream processing framework (Flink)."
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#definitions",
    "href": "examples/flink-vs-kinesis.html#definitions",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Apache Flink is an open-source stream-processing framework used for distributed processing of data streams. It is designed for stateful computations over unbounded data streams and can handle large-scale, real-time analytics.\n\nKey Features:\n\nReal-time stream processing\nFault tolerance with checkpointing\nSupports batch processing (as a special case of stream processing)\nAdvanced windowing and event time processing\nSupport for complex event processing (CEP)\n\n\n\n\n\nAWS Kinesis is a fully managed service by Amazon Web Services for real-time data streaming and processing. It allows users to collect, process, and analyze real-time data at massive scale.\n\nKey Features:\n\nFully managed, scalable stream processing\nIntegration with AWS ecosystem (e.g., Lambda, Redshift, etc.)\nReal-time analytics\nSupports data ingestion, storage, and analytics on streaming data\nAuto-scaling based on data volume"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#key-differences",
    "href": "examples/flink-vs-kinesis.html#key-differences",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Feature\nApache Flink\nAWS Kinesis\n\n\n\n\nPurpose\nStream processing and complex event processing\nReal-time data streaming and ingestion\n\n\nManagement\nSelf-managed (on-premise or cloud)\nFully managed service by AWS\n\n\nFault Tolerance\nCheckpointing and state management\nBuilt-in replication and data retention\n\n\nUse Case\nComplex analytics, real-time ETL, CEP\nData ingestion, analytics, and streaming apps\n\n\nScaling\nManual or automated scaling\nAuto-scaling based on data volume\n\n\nIntegrations\nKafka, HDFS, Cassandra, Elasticsearch\nAWS ecosystem (Lambda, Redshift, etc.)\n\n\nLanguage Support\nJava, Scala, Python, SQL\nKinesis Client Library (Java, Python, etc.)\n\n\nLatency\nSub-second latency (low-latency)\nTypically sub-second (low-latency)"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#comparison-in-action",
    "href": "examples/flink-vs-kinesis.html#comparison-in-action",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Here is a basic example of a Flink job that processes a stream of events:\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\npublic class FlinkExample {\n    public static void main(String[] args) throws Exception {\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // Example: Create a stream of integers and map them to a string\n        DataStream&lt;Integer&gt; numbers = env.fromElements(1, 2, 3, 4, 5);\n        \n        DataStream&lt;String&gt; result = numbers.map(new MapFunction&lt;Integer, String&gt;() {\n            @Override\n            public String map(Integer value) throws Exception {\n                return \"Number: \" + value;\n            }\n        });\n\n        result.print(); // Output the result\n        \n        env.execute(\"Flink Streaming Example\");\n    }\n}\nIn this example, Flink reads from a stream of integers, processes them, and outputs a transformed result. AWS Kinesis Example\nBelow is an example of how to produce and consume data from Kinesis streams using the AWS SDK for Python (Boto3).\nKinesis Producer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef send_to_kinesis(stream_name, data):\n    payload = json.dumps(data)\n    kinesis.put_record(\n        StreamName=stream_name,\n        Data=payload,\n        PartitionKey=\"partitionkey\"\n    )\n\n# Example usage\nsend_to_kinesis(\"my-kinesis-stream\", {\"event\": \"start\", \"timestamp\": \"2025-02-03T12:00:00\"})\nKinesis Consumer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef consume_from_kinesis(stream_name):\n    shard_iterator = kinesis.get_shard_iterator(\n        StreamName=stream_name,\n        ShardId='shardId-000000000000',\n        ShardIteratorType='TRIM_HORIZON'\n    )['ShardIterator']\n    \n    while True:\n        record_response = kinesis.get_records(ShardIterator=shard_iterator, Limit=10)\n        for record in record_response['Records']:\n            print(json.loads(record['Data']))\n        \n        shard_iterator = record_response['NextShardIterator']\n\n# Example usage\nconsume_from_kinesis(\"my-kinesis-stream\")\nIn the Kinesis example, we have a producer that sends data to a stream and a consumer that reads data from it. Conclusion\nBoth Apache Flink and AWS Kinesis are valuable tools in the realm of real-time data processing. Flink is a stream-processing framework ideal for complex event processing and advanced analytics, while Kinesis provides a fully managed solution for collecting, processing, and analyzing streaming data.\nThe choice between the two depends on factors such as the scale of your infrastructure, integration needs, and whether you prefer a fully managed service (Kinesis) or a more customizable stream processing framework (Flink)."
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "Ensuring high-quality data is a crucial step in any machine learning pipeline. AWS offers a range of services to help validate, label, and mitigate bias in datasets. In this post, we will explore:\n\nData validation and labeling using Amazon SageMaker Ground Truth and Amazon Mechanical Turk\nIdentifying and mitigating bias with SageMaker Clarify\nApplying AWS-built algorithms for various ML tasks\n\n\n\n\n\nAmazon SageMaker Ground Truth is a managed service that simplifies data labeling by using machine learning-assisted workflows. It supports human labelers, automated data labeling, and integration with Amazon Mechanical Turk.\n\n\nimport boto3\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_labeling_job(\n    LabelingJobName=\"MyLabelingJob\",\n    HumanTaskConfig={\n        \"WorkteamArn\": \"arn:aws:sagemaker:us-east-1:123456789012:workteam/private-crowd/MyTeam\",\n        \"UiConfig\": {\"UiTemplateS3Uri\": \"s3://my-bucket/ui-template.html\"},\n        \"PreHumanTaskLambdaArn\": \"arn:aws:lambda:us-east-1:432987654321:function:preprocessing\",\n        \"AnnotationConsolidationConfig\": {\n            \"AnnotationConsolidationLambdaArn\": \"arn:aws:lambda:us-east-1:123456789012:function:consolidation\"\n        },\n    },\n    InputConfig={\"DataSource\": {\"S3DataSource\": {\"ManifestS3Uri\": \"s3://my-bucket/input.manifest\"}}},\n    OutputConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    RoleArn=\"arn:aws:iam::123456789012:role/MySageMakerRole\"\n)\nprint(\"Labeling job started:\", response)\n\n\n\n\nAmazon Mechanical Turk (MTurk) allows businesses to leverage a global workforce to complete data annotation tasks. Ground Truth integrates directly with MTurk to provide a scalable solution for labeling large datasets.\n\n\n\n\nSageMaker Clarify helps detect bias in datasets and models by analyzing distributions and fairness metrics.\n\n\n\n\n\n\n\n\n\nBias Type\nDescription\n\n\n\n\nSelection Bias\nThe dataset is not representative of the real-world population.\n\n\nMeasurement Bias\nErrors in data collection or labeling lead to incorrect outcomes.\n\n\nLabeling Bias\nSubjectivity in human labeling skews the data.\n\n\n\n\n\n\nfrom sagemaker import clarify\n\nclarify_processor = clarify.SageMakerClarifyProcessor(role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\")\n\nbias_config = clarify.BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name=\"gender\",\n    dataset_type=\"text/csv\")\n\nclarify_processor.run_pre_training_bias(\n    data_config=clarify.DataConfig(\n        s3_data_input_path=\"s3://my-bucket/data.csv\",\n        s3_output_path=\"s3://my-bucket/output/\",\n        label=\"outcome\"),\n    bias_config=bias_config)\nprint(\"Bias analysis complete\")\n\n\n\n\nSageMaker offers built-in algorithms optimized for scalability and efficiency. Below is a table summarizing key algorithms and their applications.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\nObject Detection\nComputer Vision\nDetecting objects in images\n\n\nSemantic Segmentation\nComputer Vision\nImage segmentation tasks\n\n\nFactorization Machines\nSupervised\nRecommendation Systems\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.algorithm_specifier import AlgorithmSpecification\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region=session.boto_region_name)\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")\n\n\n\n\nAWS provides powerful tools for data validation, bias mitigation, and model training. By using services like SageMaker Ground Truth for data labeling, SageMaker Clarify for bias detection, and SageMaker’s built-in algorithms for training, organizations can build robust and ethical machine learning models efficiently."
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#validating-and-labeling-data-with-aws-services",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#validating-and-labeling-data-with-aws-services",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "Amazon SageMaker Ground Truth is a managed service that simplifies data labeling by using machine learning-assisted workflows. It supports human labelers, automated data labeling, and integration with Amazon Mechanical Turk.\n\n\nimport boto3\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_labeling_job(\n    LabelingJobName=\"MyLabelingJob\",\n    HumanTaskConfig={\n        \"WorkteamArn\": \"arn:aws:sagemaker:us-east-1:123456789012:workteam/private-crowd/MyTeam\",\n        \"UiConfig\": {\"UiTemplateS3Uri\": \"s3://my-bucket/ui-template.html\"},\n        \"PreHumanTaskLambdaArn\": \"arn:aws:lambda:us-east-1:432987654321:function:preprocessing\",\n        \"AnnotationConsolidationConfig\": {\n            \"AnnotationConsolidationLambdaArn\": \"arn:aws:lambda:us-east-1:123456789012:function:consolidation\"\n        },\n    },\n    InputConfig={\"DataSource\": {\"S3DataSource\": {\"ManifestS3Uri\": \"s3://my-bucket/input.manifest\"}}},\n    OutputConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    RoleArn=\"arn:aws:iam::123456789012:role/MySageMakerRole\"\n)\nprint(\"Labeling job started:\", response)\n\n\n\n\nAmazon Mechanical Turk (MTurk) allows businesses to leverage a global workforce to complete data annotation tasks. Ground Truth integrates directly with MTurk to provide a scalable solution for labeling large datasets."
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#identifying-and-mitigating-bias-with-sagemaker-clarify",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#identifying-and-mitigating-bias-with-sagemaker-clarify",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "SageMaker Clarify helps detect bias in datasets and models by analyzing distributions and fairness metrics.\n\n\n\n\n\n\n\n\n\nBias Type\nDescription\n\n\n\n\nSelection Bias\nThe dataset is not representative of the real-world population.\n\n\nMeasurement Bias\nErrors in data collection or labeling lead to incorrect outcomes.\n\n\nLabeling Bias\nSubjectivity in human labeling skews the data.\n\n\n\n\n\n\nfrom sagemaker import clarify\n\nclarify_processor = clarify.SageMakerClarifyProcessor(role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\")\n\nbias_config = clarify.BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name=\"gender\",\n    dataset_type=\"text/csv\")\n\nclarify_processor.run_pre_training_bias(\n    data_config=clarify.DataConfig(\n        s3_data_input_path=\"s3://my-bucket/data.csv\",\n        s3_output_path=\"s3://my-bucket/output/\",\n        label=\"outcome\"),\n    bias_config=bias_config)\nprint(\"Bias analysis complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#sagemaker-built-in-algorithms-and-when-to-use-them",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#sagemaker-built-in-algorithms-and-when-to-use-them",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "SageMaker offers built-in algorithms optimized for scalability and efficiency. Below is a table summarizing key algorithms and their applications.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\nObject Detection\nComputer Vision\nDetecting objects in images\n\n\nSemantic Segmentation\nComputer Vision\nImage segmentation tasks\n\n\nFactorization Machines\nSupervised\nRecommendation Systems\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.algorithm_specifier import AlgorithmSpecification\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region=session.boto_region_name)\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#conclusion",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#conclusion",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "AWS provides powerful tools for data validation, bias mitigation, and model training. By using services like SageMaker Ground Truth for data labeling, SageMaker Clarify for bias detection, and SageMaker’s built-in algorithms for training, organizations can build robust and ethical machine learning models efficiently."
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html",
    "href": "sagemaker/sagemaker-pipelines.html",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "Amazon SageMaker Pipelines provide a way to automate and manage machine learning workflows. They help in orchestrating ML steps, such as data processing, training, and deployment.\n\n\nimport boto3\nimport sagemaker\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep\nfrom sagemaker.processing import ScriptProcessor\nfrom sagemaker.sklearn.estimator import SKLearn\nfrom sagemaker.estimator import Estimator\n\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Step 1: Preprocessing\nsklearn_processor = ScriptProcessor(\n    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", boto3.Session().region_name, version=\"0.23-1\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n)\n\nstep_preprocess = ProcessingStep(\n    name=\"PreprocessData\",\n    processor=sklearn_processor,\n    code=\"preprocessing.py\",\n    outputs=[],\n)\n\n# Step 2: Model Training\nestimator = Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    hyperparameters={\"max_depth\": 5, \"eta\": 0.2, \"objective\": \"reg:squarederror\"},\n)\n\nstep_train = TrainingStep(\n    name=\"TrainModel\",\n    estimator=estimator,\n    inputs={},\n)\n\n# Define Pipeline\npipeline = Pipeline(\n    name=\"BasicPipeline\",\n    steps=[step_preprocess, step_train],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()\n\n\n\nfrom sagemaker.workflow.steps import ModelStep\nfrom sagemaker.model import Model\n\nmodel = Model(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=role,\n)\n\nstep_register = ModelStep(\n    name=\"RegisterModel\",\n    model=model,\n)\n\npipeline = Pipeline(\n    name=\"PipelineWithEvaluation\",\n    steps=[step_preprocess, step_train, step_register],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html#example-1-basic-sagemaker-pipeline-with-preprocessing-and-training",
    "href": "sagemaker/sagemaker-pipelines.html#example-1-basic-sagemaker-pipeline-with-preprocessing-and-training",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "import boto3\nimport sagemaker\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep\nfrom sagemaker.processing import ScriptProcessor\nfrom sagemaker.sklearn.estimator import SKLearn\nfrom sagemaker.estimator import Estimator\n\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Step 1: Preprocessing\nsklearn_processor = ScriptProcessor(\n    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", boto3.Session().region_name, version=\"0.23-1\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n)\n\nstep_preprocess = ProcessingStep(\n    name=\"PreprocessData\",\n    processor=sklearn_processor,\n    code=\"preprocessing.py\",\n    outputs=[],\n)\n\n# Step 2: Model Training\nestimator = Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    hyperparameters={\"max_depth\": 5, \"eta\": 0.2, \"objective\": \"reg:squarederror\"},\n)\n\nstep_train = TrainingStep(\n    name=\"TrainModel\",\n    estimator=estimator,\n    inputs={},\n)\n\n# Define Pipeline\npipeline = Pipeline(\n    name=\"BasicPipeline\",\n    steps=[step_preprocess, step_train],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html#example-2-sagemaker-pipeline-with-model-evaluation",
    "href": "sagemaker/sagemaker-pipelines.html#example-2-sagemaker-pipeline-with-model-evaluation",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "from sagemaker.workflow.steps import ModelStep\nfrom sagemaker.model import Model\n\nmodel = Model(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=role,\n)\n\nstep_register = ModelStep(\n    name=\"RegisterModel\",\n    model=model,\n)\n\npipeline = Pipeline(\n    name=\"PipelineWithEvaluation\",\n    steps=[step_preprocess, step_train, step_register],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html",
    "href": "sagemaker/advanced-ml-with-sagemaker.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#conclusion",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "",
    "text": "AWS offers a range of storage solutions tailored for different use cases. Below is a comparison of four major storage options: Amazon S3, Amazon EFS, Amazon FSx, and Amazon EBS."
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#overview-table",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#overview-table",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Overview Table",
    "text": "Overview Table\n\n\n\n\n\n\n\n\n\n\nFeature\nAmazon S3\nAmazon EFS\nAmazon FSx\nAmazon EBS\n\n\n\n\nType\nObject Storage\nNetwork File System\nManaged File System\nBlock Storage\n\n\nUse Case\nBackup, archival, static website hosting\nShared storage for Linux-based workloads\nWindows file system or high-performance workloads\nBoot volumes, databases, transactional workloads\n\n\nPerformance\nScalable, eventual consistency\nLow latency, shared access\nOptimized for Windows/Linux workloads\nHigh IOPS, low-latency\n\n\nAccess Model\nREST API, SDKs, web-based access\nNFS mount\nSMB (Windows) or Lustre (HPC)\nBlock device (EC2 instances)\n\n\nPersistence\nHighly durable (11 9’s)\nPersistent\nPersistent\nPersistent but tied to EC2"
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-s3-simple-storage-service",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-s3-simple-storage-service",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon S3 (Simple Storage Service)",
    "text": "Amazon S3 (Simple Storage Service)\nDefinition: Amazon S3 is an object storage service designed for scalable storage of any amount of data, accessible over the internet.\nCommon Use Cases: - Backup and disaster recovery - Static website hosting - Data lakes and big data analytics\n\nExample Code: Upload a File to S3 using Python (Boto3)\nimport boto3\n\ns3 = boto3.client('s3')\n\ns3.upload_file(\"localfile.txt\", \"my-bucket\", \"remote-file.txt\")"
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-efs-elastic-file-system",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-efs-elastic-file-system",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EFS (Elastic File System)",
    "text": "Amazon EFS (Elastic File System)\nDefinition: Amazon EFS is a scalable, fully managed network file system (NFS) for Linux-based applications and workloads.\nCommon Use Cases: - Shared storage for EC2 instances - Machine learning and big data processing - Containerized applications (EKS, ECS)\n\nExample Code: Mount an EFS File System on an EC2 Instance\nsudo yum install -y amazon-efs-utils\nsudo mkdir /mnt/efs\nsudo mount -t efs fs-12345678:/ /mnt/efs"
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-fsx",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-fsx",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon FSx",
    "text": "Amazon FSx\nDefinition: Amazon FSx provides fully managed, high-performance file systems for specialized workloads, including Windows-based and high-performance computing (HPC).\nCommon Use Cases: - Windows applications requiring NTFS - High-performance computing with Lustre - Media rendering and big data processing\n\nExample Code: Mount an FSx File System on Windows\nnet use X: \\\\fsx-share.amazonaws.com\\share /persistent:yes"
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-ebs-elastic-block-store",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#amazon-ebs-elastic-block-store",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EBS (Elastic Block Store)",
    "text": "Amazon EBS (Elastic Block Store)\nDefinition: Amazon EBS is block storage that provides persistent storage for EC2 instances, supporting low-latency, high-performance applications.\nCommon Use Cases: - Databases (MySQL, PostgreSQL, etc.) - Virtual machine storage - Transactional applications\n\nExample Code: Attach an EBS Volume to an EC2 Instance\naws ec2 attach-volume --volume-id vol-12345678 --instance-id i-abcdef12 --device /dev/xvdf"
  },
  {
    "objectID": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#conclusion",
    "href": "sagemaker/sagemaker-processing-vs-training-vs-hosting.html#conclusion",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Conclusion",
    "text": "Conclusion\nEach AWS storage service serves a unique purpose, and choosing the right one depends on your workload needs. If you need object storage, go with S3. If you need shared file storage, use EFS. If you need a managed file system, consider FSx. If you require high-performance block storage, EBS is the best choice."
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html",
    "href": "sagemaker/sagemaker-apis.html",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "Amazon SageMaker provides a variety of APIs to interact with its services, including training, inference, and model deployment. This document explores real-world examples of using these APIs.\n\n\nimport boto3\nimport json\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_training_job(\n    TrainingJobName=\"my-xgboost-training-job\",\n    AlgorithmSpecification={\n        \"TrainingImage\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n        \"TrainingInputMode\": \"File\"\n    },\n    RoleArn=\"arn:aws:iam::123456789012:role/SageMakerRole\",\n    InputDataConfig=[\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3://my-bucket/train/\",\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            }\n        }\n    ],\n    OutputDataConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    ResourceConfig={\n        \"InstanceType\": \"ml.m5.large\",\n        \"InstanceCount\": 1,\n        \"VolumeSizeInGB\": 10\n    },\n    StoppingCondition={\"MaxRuntimeInSeconds\": 3600},\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nresponse = sagemaker_client.create_endpoint_config(\n    EndpointConfigName=\"my-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"AllTraffic\",\n            \"ModelName\": \"my-trained-model\",\n            \"InstanceType\": \"ml.m5.large\",\n            \"InitialInstanceCount\": 1\n        }\n    ]\n)\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName=\"my-endpoint\",\n    EndpointConfigName=\"my-endpoint-config\"\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nimport json\nimport boto3\n\nruntime_client = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=\"my-endpoint\",\n    ContentType=\"application/json\",\n    Body=json.dumps({\"data\": [1.0, 2.0, 3.0, 4.0]})\n)\n\nresult = json.loads(response[\"Body\"].read().decode())\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-1-creating-a-sagemaker-training-job",
    "href": "sagemaker/sagemaker-apis.html#example-1-creating-a-sagemaker-training-job",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "import boto3\nimport json\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_training_job(\n    TrainingJobName=\"my-xgboost-training-job\",\n    AlgorithmSpecification={\n        \"TrainingImage\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n        \"TrainingInputMode\": \"File\"\n    },\n    RoleArn=\"arn:aws:iam::123456789012:role/SageMakerRole\",\n    InputDataConfig=[\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3://my-bucket/train/\",\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            }\n        }\n    ],\n    OutputDataConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    ResourceConfig={\n        \"InstanceType\": \"ml.m5.large\",\n        \"InstanceCount\": 1,\n        \"VolumeSizeInGB\": 10\n    },\n    StoppingCondition={\"MaxRuntimeInSeconds\": 3600},\n)\n\nprint(json.dumps(response, indent=4))"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-2-deploying-a-model-to-a-sagemaker-endpoint",
    "href": "sagemaker/sagemaker-apis.html#example-2-deploying-a-model-to-a-sagemaker-endpoint",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "response = sagemaker_client.create_endpoint_config(\n    EndpointConfigName=\"my-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"AllTraffic\",\n            \"ModelName\": \"my-trained-model\",\n            \"InstanceType\": \"ml.m5.large\",\n            \"InitialInstanceCount\": 1\n        }\n    ]\n)\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName=\"my-endpoint\",\n    EndpointConfigName=\"my-endpoint-config\"\n)\n\nprint(json.dumps(response, indent=4))"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-3-invoking-a-sagemaker-endpoint-for-inference",
    "href": "sagemaker/sagemaker-apis.html#example-3-invoking-a-sagemaker-endpoint-for-inference",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "import json\nimport boto3\n\nruntime_client = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=\"my-endpoint\",\n    ContentType=\"application/json\",\n    Body=json.dumps({\"data\": [1.0, 2.0, 3.0, 4.0]})\n)\n\nresult = json.loads(response[\"Body\"].read().decode())\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html",
    "href": "sagemaker/sagemaker-vs-bedrock.html",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "",
    "text": "AWS provides multiple machine learning (ML) services, with Amazon SageMaker and Amazon Bedrock being two primary options. While both facilitate ML model deployment, they cater to different use cases and user expertise levels."
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#overview-table",
    "href": "sagemaker/sagemaker-vs-bedrock.html#overview-table",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Overview Table",
    "text": "Overview Table\n\n\n\n\n\n\n\n\nFeature\nAmazon SageMaker\nAmazon Bedrock\n\n\n\n\nType\nFully managed ML development platform\nFully managed foundation model (FM) service\n\n\nUse Case\nBuilding, training, and deploying custom ML models\nUsing and fine-tuning foundation models for AI applications\n\n\nCustomization\nHigh (bring your own data and model)\nLimited (fine-tune pre-trained models)\n\n\nModel Hosting\nCustom models on managed infrastructure\nAPI-based access to foundation models (FM)\n\n\nData Handling\nRequires dataset preparation and preprocessing\nUses pre-trained models with minimal data processing\n\n\nAccess Model\nSDK, APIs, Jupyter notebooks\nAPI-based inference\n\n\nScaling\nFully managed infrastructure\nFully managed with auto-scaling"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#amazon-sagemaker",
    "href": "sagemaker/sagemaker-vs-bedrock.html#amazon-sagemaker",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Amazon SageMaker",
    "text": "Amazon SageMaker\nDefinition: Amazon SageMaker is a fully managed ML service that provides tools to build, train, and deploy custom machine learning models.\nCommon Use Cases: - Training custom ML models - Deploying inference endpoints - Experiment tracking and model monitoring\n\nExample Code: Train a Model in SageMaker using Python\nimport boto3\n\nsagemaker = boto3.client('sagemaker')\nresponse = sagemaker.create_training_job(\n    TrainingJobName='my-training-job',\n    AlgorithmSpecification={\n        'TrainingImage': 'your-docker-image',\n        'TrainingInputMode': 'File'\n    },\n    RoleArn='your-role-arn',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3Uri': 's3://your-dataset/',\n                    'S3DataType': 'S3Prefix'\n                }\n            }\n        }\n    ],\n    OutputDataConfig={'S3OutputPath': 's3://your-output-bucket/'},\n    ResourceConfig={\n        'InstanceType': 'ml.m5.large',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 50\n    },\n    StoppingCondition={'MaxRuntimeInSeconds': 3600}\n)\nprint(response)"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#amazon-bedrock",
    "href": "sagemaker/sagemaker-vs-bedrock.html#amazon-bedrock",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Amazon Bedrock",
    "text": "Amazon Bedrock\nDefinition: Amazon Bedrock is a managed service that provides access to foundation models from leading AI providers, allowing users to build generative AI applications without needing to train models from scratch.\nCommon Use Cases: - Text generation and summarization - Chatbots and virtual assistants - Image generation and AI-powered applications\n\nExample Code: Call a Foundation Model via Bedrock API\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\nresponse = bedrock.invoke_model(\n    modelId='anthropic.claude-v2',\n    body='{\"prompt\": \"Write a short poem about the ocean.\", \"max_tokens\": 100}'\n)\nprint(response[\"body\"].read().decode('utf-8'))"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#conclusion",
    "href": "sagemaker/sagemaker-vs-bedrock.html#conclusion",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Conclusion",
    "text": "Conclusion\nIf your goal is to build and train custom ML models, SageMaker provides the flexibility and infrastructure to do so. If you need quick access to pre-trained foundation models for AI applications, Bedrock offers a simple API-based approach to integrate generative AI into applications."
  }
]