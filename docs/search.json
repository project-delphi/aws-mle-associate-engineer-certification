[
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides a variety of built-in algorithms, foundation models, and solution templates to streamline machine learning (ML) workflows. This post explores:\n\nHow to choose built-in algorithms, foundation models, and solution templates from SageMaker JumpStart and Amazon Bedrock.\nMethods for integrating externally developed models into SageMaker.\nUsing SageMaker built-in algorithms and common ML libraries to develop ML models.\n\n\n\n\n\nSageMaker JumpStart provides pre-built ML models, solution templates, and training scripts to help users get started quickly. It includes models for computer vision, NLP, and tabular data.\n\n\nimport boto3\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\nmodels = list_jumpstart_models()\nprint(models[:5])  # Display first five models\n\n\n\n\nAmazon Bedrock allows developers to build and scale applications using foundation models from AWS and third-party providers. It provides access to powerful large language models (LLMs) without needing extensive infrastructure management.\n\n\n\n\n\n\n\n\nFeature\nSageMaker JumpStart\nAmazon Bedrock\n\n\n\n\nModel Type\nPre-trained ML models\nFoundation Models (LLMs)\n\n\nCustomization\nFine-tuning and transfer learning\nAPI-based integration\n\n\nUse Case\nComputer vision, NLP, structured data\nChatbots, content generation, summarization\n\n\n\n\n\nimport boto3\n\nbedrock = boto3.client(\"bedrock-runtime\")\nresponse = bedrock.invoke_model(\n    modelId=\"ai21.j2-ultra\",\n    body=\"{\"prompt\": \"What are the benefits of using AWS SageMaker?\"}\"\n)\nprint(response[\"body\"].read().decode(\"utf-8\"))\n\n\n\n\n\nIf a model is built outside of SageMaker, it can still be deployed using SageMaker endpoints. The most common methods include:\n\n\n\nSave the trained model in an s3:// bucket.\nCreate a SageMaker inference script.\nDeploy using a SageMaker endpoint.\n\n\n\nfrom sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    entry_point=\"inference.py\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n)\n\npredictor = model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)\nprint(\"Model deployed successfully\")\n\n\n\n\n\nSageMaker provides built-in algorithms optimized for scalability and efficiency.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")\n\n\n\n\nAWS SageMaker simplifies the ML development process by offering pre-built models, seamless integration of external models, and a variety of built-in algorithms. Whether you’re using SageMaker JumpStart, Amazon Bedrock, or integrating your custom models, AWS provides a scalable and efficient ML workflow."
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#choosing-built-in-algorithms-foundation-models-and-solution-templates",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#choosing-built-in-algorithms-foundation-models-and-solution-templates",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "SageMaker JumpStart provides pre-built ML models, solution templates, and training scripts to help users get started quickly. It includes models for computer vision, NLP, and tabular data.\n\n\nimport boto3\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\nmodels = list_jumpstart_models()\nprint(models[:5])  # Display first five models\n\n\n\n\nAmazon Bedrock allows developers to build and scale applications using foundation models from AWS and third-party providers. It provides access to powerful large language models (LLMs) without needing extensive infrastructure management.\n\n\n\n\n\n\n\n\nFeature\nSageMaker JumpStart\nAmazon Bedrock\n\n\n\n\nModel Type\nPre-trained ML models\nFoundation Models (LLMs)\n\n\nCustomization\nFine-tuning and transfer learning\nAPI-based integration\n\n\nUse Case\nComputer vision, NLP, structured data\nChatbots, content generation, summarization\n\n\n\n\n\nimport boto3\n\nbedrock = boto3.client(\"bedrock-runtime\")\nresponse = bedrock.invoke_model(\n    modelId=\"ai21.j2-ultra\",\n    body=\"{\"prompt\": \"What are the benefits of using AWS SageMaker?\"}\"\n)\nprint(response[\"body\"].read().decode(\"utf-8\"))"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#integrating-external-models-into-sagemaker",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#integrating-external-models-into-sagemaker",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "If a model is built outside of SageMaker, it can still be deployed using SageMaker endpoints. The most common methods include:\n\n\n\nSave the trained model in an s3:// bucket.\nCreate a SageMaker inference script.\nDeploy using a SageMaker endpoint.\n\n\n\nfrom sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    entry_point=\"inference.py\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n)\n\npredictor = model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)\nprint(\"Model deployed successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#developing-ml-models-with-sagemaker-built-in-algorithms",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#developing-ml-models-with-sagemaker-built-in-algorithms",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "SageMaker provides built-in algorithms optimized for scalability and efficiency.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#conclusion",
    "href": "sagemaker/sagemaker-choosing-and-integrating-ml-models.html#conclusion",
    "title": "Choosing and Integrating ML Models with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker simplifies the ML development process by offering pre-built models, seamless integration of external models, and a variety of built-in algorithms. Whether you’re using SageMaker JumpStart, Amazon Bedrock, or integrating your custom models, AWS provides a scalable and efficient ML workflow."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "",
    "text": "Amazon SageMaker provides a comprehensive suite of tools for building, training, and deploying machine learning models at scale. This post explores SageMaker’s capabilities, data ingestion options, feature engineering tools, and deployment strategies."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#introduction",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#introduction",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "",
    "text": "Amazon SageMaker provides a comprehensive suite of tools for building, training, and deploying machine learning models at scale. This post explores SageMaker’s capabilities, data ingestion options, feature engineering tools, and deployment strategies."
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#sagemaker-capabilities-and-algorithms",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#sagemaker-capabilities-and-algorithms",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "SageMaker Capabilities and Algorithms",
    "text": "SageMaker Capabilities and Algorithms\nAmazon SageMaker supports various built-in machine learning algorithms for supervised and unsupervised learning, including:\n\n\n\n\n\n\n\n\nAlgorithm\nType\nUse Case\n\n\n\n\nXGBoost\nSupervised\nRegression, Classification\n\n\nLinear Learner\nSupervised\nRegression, Classification\n\n\nK-Means\nUnsupervised\nClustering\n\n\nRandom Cut Forest\nUnsupervised\nAnomaly Detection\n\n\nDeepAR Forecasting\nSupervised\nTime-Series Forecasting\n\n\n\nAdditionally, SageMaker allows users to train custom models using frameworks like TensorFlow, PyTorch, and Scikit-Learn.\n\nExample: Training an XGBoost Model in SageMaker\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.xgboost.estimator import XGBoost\n\n# Initialize SageMaker session\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Define S3 data paths\ntrain_data = 's3://your-bucket/train.csv'\ntest_data = 's3://your-bucket/test.csv'\n\n# Create XGBoost estimator\nxgb = XGBoost(entry_point='script.py',\n              framework_version='1.3-1',\n              role=role,\n              instance_count=1,\n              instance_type='ml.m5.large',\n              hyperparameters={'num_round': 100})\n\n# Train the model\nxgb.fit({'train': TrainingInput(train_data, content_type='csv')})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#ingesting-data-into-sagemaker",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#ingesting-data-into-sagemaker",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Ingesting Data into SageMaker",
    "text": "Ingesting Data into SageMaker\n\nUsing SageMaker Data Wrangler\nSageMaker Data Wrangler allows you to import, clean, and transform data before training models. You can: - Import data from Amazon S3, Redshift, Athena, and other sources. - Perform transformations such as filtering, joining, and feature engineering. - Export processed data directly into SageMaker Feature Store or S3.\n\n\nUsing SageMaker Feature Store\nSageMaker Feature Store helps manage features across ML workflows. It supports: - Online store for real-time inference. - Offline store for batch processing.\n\n\nExample: Storing Features in SageMaker Feature Store\nfrom sagemaker.feature_store.feature_group import FeatureGroup\nimport pandas as pd\n\n# Define feature group\nfeature_group = FeatureGroup(name=\"customer_features\", sagemaker_session=sagemaker_session)\n\n# Sample data\ndata = pd.DataFrame({\n    \"customer_id\": [1, 2, 3],\n    \"purchase_count\": [10, 20, 15],\n    \"avg_spend\": [100.5, 200.0, 150.7]\n})\n\n# Ingest data into feature store\nfeature_group.ingest(data_frame=data, max_workers=3, wait=True)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#data-exploration-and-feature-engineering",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#data-exploration-and-feature-engineering",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Data Exploration and Feature Engineering",
    "text": "Data Exploration and Feature Engineering\nAWS offers multiple tools for data exploration and transformation:\n\n\n\nTool\nPurpose\n\n\n\n\nSageMaker Data Wrangler\nData preprocessing and transformation\n\n\nAWS Glue\nData cataloging and ETL\n\n\nAWS Glue DataBrew\nData cleaning and enrichment\n\n\n\n\nExample: Using AWS Glue for ETL\nimport boto3\n\nglue_client = boto3.client('glue')\nresponse = glue_client.create_job(\n    Name='etl-job',\n    Role='AWSGlueServiceRole',\n    Command={'Name': 'glueetl', 'ScriptLocation': 's3://your-bucket/scripts/etl.py'},\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#deploying-models-in-sagemaker",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#deploying-models-in-sagemaker",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Deploying Models in SageMaker",
    "text": "Deploying Models in SageMaker\nSageMaker provides multiple deployment options:\n\n\n\nDeployment Method\nUse Case\n\n\n\n\nReal-time endpoint\nLow-latency inference\n\n\nBatch transform\nLarge-scale batch inference\n\n\nEdge deployment\nDeploying models to IoT and edge devices\n\n\n\n\nExample: Deploying a Model for Real-Time Inference\n# Deploy trained model as an endpoint\npredictor = xgb.deploy(instance_type='ml.m5.large', initial_instance_count=1)\n\n# Make predictions\nresult = predictor.predict(data=[1.5, 2.3, 3.1])\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-model-building-and-deployment.html#conclusion",
    "href": "sagemaker/sagemaker-model-building-and-deployment.html#conclusion",
    "title": "Getting Started with Amazon SageMaker for Model Building and Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nAmazon SageMaker provides a powerful suite of tools for data preparation, model training, feature management, and deployment. By leveraging SageMaker Data Wrangler, Feature Store, and AWS Glue, you can create scalable and efficient ML pipelines."
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/sagemaker-model-deployment-and-monitoring.html#conclusion",
    "href": "sagemaker/sagemaker-model-deployment-and-monitoring.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html",
    "href": "sagemaker/sagemaker-optimizing-deployments.html",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "",
    "text": "Amazon SageMaker offers flexible deployment options, cost-saving strategies, and security features. This post explores:\n\nRightsizing instance families and sizes using SageMaker Inference Recommender and AWS Compute Optimizer.\nOptimizing costs with different purchasing options.\nManaging access control with IAM roles and policies.\nImplementing security and compliance best practices in SageMaker."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#introduction",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#introduction",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "",
    "text": "Amazon SageMaker offers flexible deployment options, cost-saving strategies, and security features. This post explores:\n\nRightsizing instance families and sizes using SageMaker Inference Recommender and AWS Compute Optimizer.\nOptimizing costs with different purchasing options.\nManaging access control with IAM roles and policies.\nImplementing security and compliance best practices in SageMaker."
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#rightsizing-instance-families-and-sizes",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#rightsizing-instance-families-and-sizes",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Rightsizing Instance Families and Sizes",
    "text": "Rightsizing Instance Families and Sizes\nSelecting the appropriate instance type ensures optimal performance and cost-efficiency.\n\n\n\n\n\n\n\nInstance Type\nUse Case\n\n\n\n\nCPU (ml.m5, ml.c5)\nLightweight inference, batch processing\n\n\nGPU (ml.g4dn, ml.p3)\nDeep learning inference, high-performance computing\n\n\nInf (ml.inf1)\nOptimized for deep learning with Inferentia chips\n\n\nGraviton (ml.c7g, ml.m7g)\nCost-efficient CPU workloads\n\n\n\n\nUsing SageMaker Inference Recommender\nSageMaker Inference Recommender suggests the best instance type based on model performance.\nimport boto3\nsagemaker = boto3.client('sagemaker')\n\nresponse = sagemaker.create_inference_recommendations_job(\n    JobName='my-inference-recommender-job',\n    RoleArn='arn:aws:iam::account-id:role/SageMakerRole',\n    ModelName='my-model',\n    EndpointConfigurations=[\n        {\n            'InstanceTypes': ['ml.m5.large', 'ml.g4dn.xlarge'],\n            'InitialInstanceCount': 1\n        }\n    ]\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#optimizing-infrastructure-costs",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#optimizing-infrastructure-costs",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Optimizing Infrastructure Costs",
    "text": "Optimizing Infrastructure Costs\nAWS provides multiple purchasing options to reduce costs:\n\n\n\n\n\n\n\n\nPurchasing Option\nDescription\nSavings Potential\n\n\n\n\nOn-Demand\nPay-as-you-go pricing\nBaseline cost\n\n\nSpot Instances\nUp to 90% discount but can be interrupted\nHigh\n\n\nReserved Instances\nCommit for 1-3 years for lower rates\nUp to 72%\n\n\nSageMaker Savings Plans\nFlexible, commitment-based discount\nUp to 64%\n\n\n\n\nUsing Spot Instances in SageMaker\nfrom sagemaker.estimator import Estimator\n\nestimator = Estimator(\n    image_uri='your-container-image',\n    role='arn:aws:iam::account-id:role/SageMakerRole',\n    instance_count=1,\n    instance_type='ml.m5.large',\n    use_spot_instances=True,\n    max_wait=3600,\n    max_run=1800\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#iam-roles-policies-and-groups-for-access-control",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#iam-roles-policies-and-groups-for-access-control",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "IAM Roles, Policies, and Groups for Access Control",
    "text": "IAM Roles, Policies, and Groups for Access Control\nAWS Identity and Access Management (IAM) controls permissions for SageMaker and related services.\n\nKey IAM Components\n\n\n\nIAM Component\nDescription\n\n\n\n\nIAM Roles\nGrant temporary access to AWS services\n\n\nIAM Policies\nDefine permissions for actions and resources\n\n\nIAM Groups\nOrganize multiple users with the same permissions\n\n\n\n\n\nExample: Creating a SageMaker Execution Role\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"sagemaker.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#sagemaker-security-and-compliance-features",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#sagemaker-security-and-compliance-features",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "SageMaker Security and Compliance Features",
    "text": "SageMaker Security and Compliance Features\nAWS SageMaker includes built-in security measures:\n\n\n\n\n\n\n\nSecurity Feature\nDescription\n\n\n\n\nVPC Isolation\nDeploy endpoints inside a VPC for network control\n\n\nEncryption\nEncrypt data at rest (S3) and in transit (TLS)\n\n\nSageMaker Role Manager\nSimplifies IAM role management\n\n\nLogging & Auditing\nTrack actions with AWS CloudTrail and CloudWatch\n\n\n\n\nEnforcing Encryption in SageMaker\nfrom sagemaker import Model\n\nmodel = Model(\n    image_uri='your-ecr-image',\n    model_data='s3://your-bucket/model.tar.gz',\n    role='arn:aws:iam::account-id:role/SageMakerRole',\n    vpc_config={'SecurityGroupIds': ['sg-xxxx'], 'Subnets': ['subnet-xxxx']},\n    enable_network_isolation=True\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-optimizing-deployments.html#conclusion",
    "href": "sagemaker/sagemaker-optimizing-deployments.html#conclusion",
    "title": "Optimizing SageMaker Deployments: Rightsizing, Cost Optimization, and Security",
    "section": "Conclusion",
    "text": "Conclusion\nBy rightsizing instances, optimizing cost strategies, enforcing IAM policies, and utilizing SageMaker’s security features, we can build efficient and secure ML deployments. These best practices ensure scalability while keeping costs under control."
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html",
    "href": "sagemaker/advanced-ml-with-sagemaker.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/advanced-ml-with-sagemaker.html#conclusion",
    "href": "sagemaker/advanced-ml-with-sagemaker.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html",
    "href": "sagemaker/sagemaker-pipelines.html",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "Amazon SageMaker Pipelines provide a way to automate and manage machine learning workflows. They help in orchestrating ML steps, such as data processing, training, and deployment.\n\n\nimport boto3\nimport sagemaker\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep\nfrom sagemaker.processing import ScriptProcessor\nfrom sagemaker.sklearn.estimator import SKLearn\nfrom sagemaker.estimator import Estimator\n\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Step 1: Preprocessing\nsklearn_processor = ScriptProcessor(\n    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", boto3.Session().region_name, version=\"0.23-1\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n)\n\nstep_preprocess = ProcessingStep(\n    name=\"PreprocessData\",\n    processor=sklearn_processor,\n    code=\"preprocessing.py\",\n    outputs=[],\n)\n\n# Step 2: Model Training\nestimator = Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    hyperparameters={\"max_depth\": 5, \"eta\": 0.2, \"objective\": \"reg:squarederror\"},\n)\n\nstep_train = TrainingStep(\n    name=\"TrainModel\",\n    estimator=estimator,\n    inputs={},\n)\n\n# Define Pipeline\npipeline = Pipeline(\n    name=\"BasicPipeline\",\n    steps=[step_preprocess, step_train],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()\n\n\n\nfrom sagemaker.workflow.steps import ModelStep\nfrom sagemaker.model import Model\n\nmodel = Model(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=role,\n)\n\nstep_register = ModelStep(\n    name=\"RegisterModel\",\n    model=model,\n)\n\npipeline = Pipeline(\n    name=\"PipelineWithEvaluation\",\n    steps=[step_preprocess, step_train, step_register],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html#example-1-basic-sagemaker-pipeline-with-preprocessing-and-training",
    "href": "sagemaker/sagemaker-pipelines.html#example-1-basic-sagemaker-pipeline-with-preprocessing-and-training",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "import boto3\nimport sagemaker\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep\nfrom sagemaker.processing import ScriptProcessor\nfrom sagemaker.sklearn.estimator import SKLearn\nfrom sagemaker.estimator import Estimator\n\nsagemaker_session = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n# Step 1: Preprocessing\nsklearn_processor = ScriptProcessor(\n    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", boto3.Session().region_name, version=\"0.23-1\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n)\n\nstep_preprocess = ProcessingStep(\n    name=\"PreprocessData\",\n    processor=sklearn_processor,\n    code=\"preprocessing.py\",\n    outputs=[],\n)\n\n# Step 2: Model Training\nestimator = Estimator(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    hyperparameters={\"max_depth\": 5, \"eta\": 0.2, \"objective\": \"reg:squarederror\"},\n)\n\nstep_train = TrainingStep(\n    name=\"TrainModel\",\n    estimator=estimator,\n    inputs={},\n)\n\n# Define Pipeline\npipeline = Pipeline(\n    name=\"BasicPipeline\",\n    steps=[step_preprocess, step_train],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/sagemaker-pipelines.html#example-2-sagemaker-pipeline-with-model-evaluation",
    "href": "sagemaker/sagemaker-pipelines.html#example-2-sagemaker-pipeline-with-model-evaluation",
    "title": "Amazon SageMaker Pipelines: Real-World Examples",
    "section": "",
    "text": "from sagemaker.workflow.steps import ModelStep\nfrom sagemaker.model import Model\n\nmodel = Model(\n    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n    model_data=\"s3://my-bucket/model.tar.gz\",\n    role=role,\n)\n\nstep_register = ModelStep(\n    name=\"RegisterModel\",\n    model=model,\n)\n\npipeline = Pipeline(\n    name=\"PipelineWithEvaluation\",\n    steps=[step_preprocess, step_train, step_register],\n)\n\npipeline.upsert(role_arn=role)\npipeline.start()"
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "Ensuring high-quality data is a crucial step in any machine learning pipeline. AWS offers a range of services to help validate, label, and mitigate bias in datasets. In this post, we will explore:\n\nData validation and labeling using Amazon SageMaker Ground Truth and Amazon Mechanical Turk\nIdentifying and mitigating bias with SageMaker Clarify\nApplying AWS-built algorithms for various ML tasks\n\n\n\n\n\nAmazon SageMaker Ground Truth is a managed service that simplifies data labeling by using machine learning-assisted workflows. It supports human labelers, automated data labeling, and integration with Amazon Mechanical Turk.\n\n\nimport boto3\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_labeling_job(\n    LabelingJobName=\"MyLabelingJob\",\n    HumanTaskConfig={\n        \"WorkteamArn\": \"arn:aws:sagemaker:us-east-1:123456789012:workteam/private-crowd/MyTeam\",\n        \"UiConfig\": {\"UiTemplateS3Uri\": \"s3://my-bucket/ui-template.html\"},\n        \"PreHumanTaskLambdaArn\": \"arn:aws:lambda:us-east-1:432987654321:function:preprocessing\",\n        \"AnnotationConsolidationConfig\": {\n            \"AnnotationConsolidationLambdaArn\": \"arn:aws:lambda:us-east-1:123456789012:function:consolidation\"\n        },\n    },\n    InputConfig={\"DataSource\": {\"S3DataSource\": {\"ManifestS3Uri\": \"s3://my-bucket/input.manifest\"}}},\n    OutputConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    RoleArn=\"arn:aws:iam::123456789012:role/MySageMakerRole\"\n)\nprint(\"Labeling job started:\", response)\n\n\n\n\nAmazon Mechanical Turk (MTurk) allows businesses to leverage a global workforce to complete data annotation tasks. Ground Truth integrates directly with MTurk to provide a scalable solution for labeling large datasets.\n\n\n\n\nSageMaker Clarify helps detect bias in datasets and models by analyzing distributions and fairness metrics.\n\n\n\n\n\n\n\n\n\nBias Type\nDescription\n\n\n\n\nSelection Bias\nThe dataset is not representative of the real-world population.\n\n\nMeasurement Bias\nErrors in data collection or labeling lead to incorrect outcomes.\n\n\nLabeling Bias\nSubjectivity in human labeling skews the data.\n\n\n\n\n\n\nfrom sagemaker import clarify\n\nclarify_processor = clarify.SageMakerClarifyProcessor(role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\")\n\nbias_config = clarify.BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name=\"gender\",\n    dataset_type=\"text/csv\")\n\nclarify_processor.run_pre_training_bias(\n    data_config=clarify.DataConfig(\n        s3_data_input_path=\"s3://my-bucket/data.csv\",\n        s3_output_path=\"s3://my-bucket/output/\",\n        label=\"outcome\"),\n    bias_config=bias_config)\nprint(\"Bias analysis complete\")\n\n\n\n\nSageMaker offers built-in algorithms optimized for scalability and efficiency. Below is a table summarizing key algorithms and their applications.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\nObject Detection\nComputer Vision\nDetecting objects in images\n\n\nSemantic Segmentation\nComputer Vision\nImage segmentation tasks\n\n\nFactorization Machines\nSupervised\nRecommendation Systems\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.algorithm_specifier import AlgorithmSpecification\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region=session.boto_region_name)\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")\n\n\n\n\nAWS provides powerful tools for data validation, bias mitigation, and model training. By using services like SageMaker Ground Truth for data labeling, SageMaker Clarify for bias detection, and SageMaker’s built-in algorithms for training, organizations can build robust and ethical machine learning models efficiently."
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#validating-and-labeling-data-with-aws-services",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#validating-and-labeling-data-with-aws-services",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "Amazon SageMaker Ground Truth is a managed service that simplifies data labeling by using machine learning-assisted workflows. It supports human labelers, automated data labeling, and integration with Amazon Mechanical Turk.\n\n\nimport boto3\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_labeling_job(\n    LabelingJobName=\"MyLabelingJob\",\n    HumanTaskConfig={\n        \"WorkteamArn\": \"arn:aws:sagemaker:us-east-1:123456789012:workteam/private-crowd/MyTeam\",\n        \"UiConfig\": {\"UiTemplateS3Uri\": \"s3://my-bucket/ui-template.html\"},\n        \"PreHumanTaskLambdaArn\": \"arn:aws:lambda:us-east-1:432987654321:function:preprocessing\",\n        \"AnnotationConsolidationConfig\": {\n            \"AnnotationConsolidationLambdaArn\": \"arn:aws:lambda:us-east-1:123456789012:function:consolidation\"\n        },\n    },\n    InputConfig={\"DataSource\": {\"S3DataSource\": {\"ManifestS3Uri\": \"s3://my-bucket/input.manifest\"}}},\n    OutputConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    RoleArn=\"arn:aws:iam::123456789012:role/MySageMakerRole\"\n)\nprint(\"Labeling job started:\", response)\n\n\n\n\nAmazon Mechanical Turk (MTurk) allows businesses to leverage a global workforce to complete data annotation tasks. Ground Truth integrates directly with MTurk to provide a scalable solution for labeling large datasets."
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#identifying-and-mitigating-bias-with-sagemaker-clarify",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#identifying-and-mitigating-bias-with-sagemaker-clarify",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "SageMaker Clarify helps detect bias in datasets and models by analyzing distributions and fairness metrics.\n\n\n\n\n\n\n\n\n\nBias Type\nDescription\n\n\n\n\nSelection Bias\nThe dataset is not representative of the real-world population.\n\n\nMeasurement Bias\nErrors in data collection or labeling lead to incorrect outcomes.\n\n\nLabeling Bias\nSubjectivity in human labeling skews the data.\n\n\n\n\n\n\nfrom sagemaker import clarify\n\nclarify_processor = clarify.SageMakerClarifyProcessor(role=\"arn:aws:iam::123456789012:role/MySageMakerRole\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\")\n\nbias_config = clarify.BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name=\"gender\",\n    dataset_type=\"text/csv\")\n\nclarify_processor.run_pre_training_bias(\n    data_config=clarify.DataConfig(\n        s3_data_input_path=\"s3://my-bucket/data.csv\",\n        s3_output_path=\"s3://my-bucket/output/\",\n        label=\"outcome\"),\n    bias_config=bias_config)\nprint(\"Bias analysis complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#sagemaker-built-in-algorithms-and-when-to-use-them",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#sagemaker-built-in-algorithms-and-when-to-use-them",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "SageMaker offers built-in algorithms optimized for scalability and efficiency. Below is a table summarizing key algorithms and their applications.\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\n\n\n\n\nXGBoost\nSupervised\nClassification, Regression\n\n\nLinear Learner\nSupervised\nLinear Regression, Binary Classification\n\n\nDeepAR Forecasting\nTime Series\nDemand Forecasting\n\n\nObject Detection\nComputer Vision\nDetecting objects in images\n\n\nSemantic Segmentation\nComputer Vision\nImage segmentation tasks\n\n\nFactorization Machines\nSupervised\nRecommendation Systems\n\n\n\n\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.algorithm_specifier import AlgorithmSpecification\n\nsession = sagemaker.Session()\nrole = get_execution_role()\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region=session.boto_region_name)\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=\"s3://my-bucket/model-output/\",\n    sagemaker_session=session)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    objective=\"binary:logistic\",\n    num_round=100)\n\nxgb.fit({\"train\": \"s3://my-bucket/train.csv\"})\nprint(\"Model training complete\")"
  },
  {
    "objectID": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#conclusion",
    "href": "sagemaker/sagemaker-validating-labelling-mitigating-bias.html#conclusion",
    "title": "Validating, Labeling, and Mitigating Bias in Data Using AWS",
    "section": "",
    "text": "AWS provides powerful tools for data validation, bias mitigation, and model training. By using services like SageMaker Ground Truth for data labeling, SageMaker Clarify for bias detection, and SageMaker’s built-in algorithms for training, organizations can build robust and ethical machine learning models efficiently."
  },
  {
    "objectID": "practice/practice-questions-2.html",
    "href": "practice/practice-questions-2.html",
    "title": "AWS Machine Learning & AI Questions",
    "section": "",
    "text": "AWS Machine Learning & AI Questions\n\n\n\n\n\n\nQuestion 1\n\n\n\nYou need to redact personally identifiable information (PII) from a large set of documents. Which AWS service should you use, while minimizing development and maintenance effort?\n\n\nShow Answer\n\nAmazon Comprehend offers PII identification and redaction capabilities.\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nA retail company wants to monitor its key business metrics, such as daily sales and customer engagement, to quickly identify any unusual changes. Which AWS service should they use to minimize development effort?\n\n\nShow Answer\n\nAmazon Lookout for Metrics is designed to automatically detect anomalies in metrics like sales and customer engagement, allowing businesses to take action quickly.\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nAn e-commerce platform wants to prevent “try before you buy” abuse, where users repeatedly purchase items and return them after use. The company has labeled data of previous fraudulent activities. Which AWS service should they use to address this issue?\n\n\nShow Answer\n\nAmazon Fraud Detector is ideal for detecting and preventing specific types of fraud, such as “try before you buy” abuse, using labeled data.\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nYou are designing a model for sentiment analysis on customer reviews. Which neural network type is best suited for this task and why?\n\n\nShow Answer\n\nRNNs are designed to handle sequential data, making them ideal for text analysis where the order of words matters.\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nYou have a dataset with 10,000 labeled images for training a CNN. To prevent overfitting, which technique should you consider implementing?\n\n\nShow Answer\n\nDropout is a regularization technique that helps prevent overfitting by randomly setting some neurons to zero during training.\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nYou are deploying a deep learning model on an AWS EC2 instance. The model requires extensive GPU resources. Which instance type should you select for optimal performance?\n\n\nShow Answer\n\np3.8xlarge is designed for deep learning tasks with powerful GPUs.\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nYou are training a deep neural network and notice that the training accuracy is high, but the validation accuracy is much lower. What is the most likely issue?\n\n\nShow Answer\n\nOverfitting, where the model memorizes training data but fails to generalize.\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nYou have a model that predicts whether a customer will make a purchase. You notice a high rate of false negatives. Which metric should you focus on improving?\n\n\nShow Answer\n\nRecall is the metric that reflects the ability to identify all actual positives, which is crucial when reducing false negatives.\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nWhile tuning hyperparameters for a model in SageMaker, you want to optimize the model as efficiently as possible. Which strategy should you use?\n\n\nShow Answer\n\nBayesian optimization learns from each trial, making it more efficient.\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nYou’re using transfer learning to fine-tune a pre-trained BERT model for a custom NLP task. What is the most important hyperparameter to adjust during fine-tuning?\n\n\nShow Answer\n\nLearning rate is crucial in fine-tuning because a lower rate ensures that the pre-trained weights are adjusted carefully.\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nYou need to process a massive dataset for training a deep learning model and want to speed up the training process. Which AWS service or feature should you consider using?\n\n\nShow Answer\n\nSageMaker Distributed Training allows you to parallelize the training across multiple nodes, speeding up the process.\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nYour model training job is taking longer than expected on SageMaker. You suspect that the compute instances are not fully utilized. Which SageMaker feature can help you diagnose this issue?\n\n\nShow Answer\n\nSageMaker Debugger provides insights into resource utilization and potential bottlenecks.\n\n\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nWhen training a deep learning model, you notice that your model’s loss is decreasing very slowly. What is a likely reason for this?\n\n\nShow Answer\n\nA low learning rate can cause very slow convergence, leading to slow loss reduction.\n\n\n\n\n\n\n\n\n\nQuestion 14\n\n\n\nWhich SageMaker algorithm would be most appropriate for predicting the next word in a sequence of text?\n\n\nShow Answer\n\nSeq2Seq is specifically designed for sequence-to-sequence tasks, such as predicting the next word in a sequence, making it ideal for tasks like language translation or text generation.\n\n\n\n\n\n\n\n\n\nQuestion 15\n\n\n\nWhich SageMaker input mode is recommended for training when you need high-performance access to data stored in a single Availability Zone?\n\n\nShow Answer\n\nAmazon FSx for Lustre provides high-performance, low-latency access to data in a single Availability Zone, making it ideal for large-scale training jobs.\n\n\n\n\n\n\n\n\n\nQuestion 16\n\n\n\nWhich SageMaker algorithm is best suited for clustering similar items without labeled data?\n\n\nShow Answer\n\nK-Means is an unsupervised algorithm designed for clustering similar items based on their features, without needing labeled data."
  },
  {
    "objectID": "practice/practice-questions-6.html",
    "href": "practice/practice-questions-6.html",
    "title": "AWS SageMaker & Bedrock Quiz",
    "section": "",
    "text": "Which SageMaker feature allows you to compare a new model’s performance against an existing production model without impacting live traffic?\n\n\nShow Answer\n\nShadow Testing runs the new model in parallel to monitor performance without affecting live traffic.\n\n\n\n\nWhich AWS service is used to orchestrate steps in a machine learning pipeline, such as model training and deployment?\n\n\nShow Answer\n\nAmazon SageMaker Pipelines orchestrates multiple steps in a pipeline, including machine learning workflows.\n\n\n\n\nWhat is a primary use case for SageMaker JumpStart?\n\n\nShow Answer\n\nJumpStart simplifies deploying pre-trained models and solutions for common ML tasks.\n\n\n\n\nYou are deploying a SageMaker training job that handles sensitive data. Which option best ensures that the data is protected both at rest and in transit?\n\n\nShow Answer\n\nEncrypting data at rest using AWS KMS and securing data in transit with TLS/SSL ensures protection at both stages.\n\n\n\n\nWhich deployment method in SageMaker is most suitable for handling infrequent or unpredictable traffic with the ability to scale down to zero?\n\n\nShow Answer\n\nSageMaker Serverless Inference is ideal for infrequent or unpredictable workloads, as it scales down to zero.\n\n\n\n\nWhen setting up a new LLM agent in Amazon Bedrock, how does it dynamically decide which tool to use based on the user’s query?\n\n\nShow Answer\n\nAction Groups define the tools available to the LLM and guide it on when to use each based on the user’s query.\n\n\n\n\nWhat is a key advantage of using self-attention in transformer models over traditional RNNs?\n\n\nShow Answer\n\nSelf-attention allows parallel processing, unlike RNNs which process tokens sequentially.\n\n\n\n\nWhich feature in Amazon Bedrock should you use if your AI system needs to write and execute Python code?\n\n\nShow Answer\n\nThe Code Interpreter feature in Action Groups allows the AI to write and execute Python code as part of its responses.\n\n\n\n\nWhat is a key benefit of using Docker containers in Amazon SageMaker?\n\n\nShow Answer\n\nPortability and environment consistency across different deployment stages.\n\n\n\n\nWhat is the most efficient way to frequently update a knowledge base in Amazon Bedrock with new company policies?\n\n\nShow Answer\n\nUpdating the vector database is faster and more efficient than fine-tuning when adding new information."
  },
  {
    "objectID": "practice/practice-questions-6.html#aws-sagemaker-bedrock-quiz",
    "href": "practice/practice-questions-6.html#aws-sagemaker-bedrock-quiz",
    "title": "AWS SageMaker & Bedrock Quiz",
    "section": "",
    "text": "Which SageMaker feature allows you to compare a new model’s performance against an existing production model without impacting live traffic?\n\n\nShow Answer\n\nShadow Testing runs the new model in parallel to monitor performance without affecting live traffic.\n\n\n\n\nWhich AWS service is used to orchestrate steps in a machine learning pipeline, such as model training and deployment?\n\n\nShow Answer\n\nAmazon SageMaker Pipelines orchestrates multiple steps in a pipeline, including machine learning workflows.\n\n\n\n\nWhat is a primary use case for SageMaker JumpStart?\n\n\nShow Answer\n\nJumpStart simplifies deploying pre-trained models and solutions for common ML tasks.\n\n\n\n\nYou are deploying a SageMaker training job that handles sensitive data. Which option best ensures that the data is protected both at rest and in transit?\n\n\nShow Answer\n\nEncrypting data at rest using AWS KMS and securing data in transit with TLS/SSL ensures protection at both stages.\n\n\n\n\nWhich deployment method in SageMaker is most suitable for handling infrequent or unpredictable traffic with the ability to scale down to zero?\n\n\nShow Answer\n\nSageMaker Serverless Inference is ideal for infrequent or unpredictable workloads, as it scales down to zero.\n\n\n\n\nWhen setting up a new LLM agent in Amazon Bedrock, how does it dynamically decide which tool to use based on the user’s query?\n\n\nShow Answer\n\nAction Groups define the tools available to the LLM and guide it on when to use each based on the user’s query.\n\n\n\n\nWhat is a key advantage of using self-attention in transformer models over traditional RNNs?\n\n\nShow Answer\n\nSelf-attention allows parallel processing, unlike RNNs which process tokens sequentially.\n\n\n\n\nWhich feature in Amazon Bedrock should you use if your AI system needs to write and execute Python code?\n\n\nShow Answer\n\nThe Code Interpreter feature in Action Groups allows the AI to write and execute Python code as part of its responses.\n\n\n\n\nWhat is a key benefit of using Docker containers in Amazon SageMaker?\n\n\nShow Answer\n\nPortability and environment consistency across different deployment stages.\n\n\n\n\nWhat is the most efficient way to frequently update a knowledge base in Amazon Bedrock with new company policies?\n\n\nShow Answer\n\nUpdating the vector database is faster and more efficient than fine-tuning when adding new information."
  },
  {
    "objectID": "practice/practice-questions-5.html",
    "href": "practice/practice-questions-5.html",
    "title": "AWS SageMaker and Bedrock Quiz",
    "section": "",
    "text": "This document contains a series of multiple-choice questions with show/hide functionality for answers and detailed explanations.\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You’re working with a sparse dataset and need to predict user-item interactions. Which SageMaker algorithm is most appropriate?\n\n\n\n\nShow Answer\n\nAnswer: Factorization Machines\nExplanation:\n\n\n\n\n\n\n\nAlgorithm\nPurpose\n\n\n\n\nFactorization Machines\nDesigned for sparse data and recommendation systems\n\n\nXGBoost\nHandles structured data but is not optimized for sparse matrices\n\n\nDeepAR\nUsed for time-series forecasting, not recommendation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: In SageMaker’s XGBoost, which hyperparameter helps prevent overfitting by controlling the step size shrinkage?\n\n\n\n\nShow Answer\n\nAnswer: eta\nExplanation:\n\n\n\n\n\n\n\nHyperparameter\nFunction\n\n\n\n\neta\nControls step size shrinkage to prevent overfitting\n\n\nmax_depth\nControls tree depth to prevent overfitting but doesn’t shrink step size\n\n\nlambda\nAdds L2 regularization but doesn’t control learning rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are tasked with training a SageMaker model to predict future values of a time series. Which algorithm should you choose?\n\n\n\n\nShow Answer\n\nAnswer: DeepAR\nExplanation:\n\n\n\nAlgorithm\nPurpose\n\n\n\n\nDeepAR\nSpecifically designed for time series forecasting\n\n\nK-Means\nUsed for clustering, not forecasting\n\n\nObject2Vec\nUsed for embeddings, not time series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Which input format is optimal for training a model using SageMaker’s K-Means algorithm?\n\n\n\n\nShow Answer\n\nAnswer: RecordIO-Protobuf\nExplanation:\n\n\n\nFormat\nSupported?\n\n\n\n\nRecordIO-Protobuf\nYes, optimal for efficiency\n\n\nCSV\nSupported but less efficient\n\n\nJSON\nNot officially supported for K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are using AWS SageMaker JumpStart to deploy a model for text generation. Which model type would you choose for multilingual text generation?\n\n\n\n\nShow Answer\n\nAnswer: Jurassic-2\nExplanation:\n\n\n\nModel\nUse Case\n\n\n\n\nJurassic-2\nStrong multilingual text generation capabilities\n\n\nAmazon Titan\nOptimized for summarization and general text tasks\n\n\nClaude\nDesigned for conversation and automation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You’re working on a Bedrock model that needs to incorporate sensitive customer data during fine-tuning. What steps should you take to ensure the security of this data?\n\n\n\n\nShow Answer\n\nAnswer: Use a VPC and PrivateLink\nExplanation:\n\n\n\n\n\n\n\nSecurity Measure\nPurpose\n\n\n\n\nVPC + PrivateLink\nEnsures secure data transfer\n\n\nS3 Bucket with Encryption\nSecures stored data but does not prevent exposure during transfer\n\n\nIAM Roles\nControls access but does not encrypt data during fine-tuning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are tasked with deploying a Bedrock model that requires high availability and low-latency responses. Which deployment option should you choose?\n\n\n\n\nShow Answer\n\nAnswer: Provisioned Throughput\nExplanation:\n\n\n\n\n\n\n\nDeployment Option\nPurpose\n\n\n\n\nProvisioned Throughput\nEnsures low-latency and high availability\n\n\nServerless Deployment\nScales automatically but may have variable latency\n\n\nOn-Demand\nSuitable for occasional usage but not optimized for high availability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You need to monitor a deployed model for feature attribution drift. Which metric should you monitor using SageMaker Model Monitor?\n\n\n\n\nShow Answer\n\nAnswer: SHAP (SHapley Additive exPlanations)\nExplanation:\n\n\n\nMetric\nUse Case\n\n\n\n\nSHAP Values\nMeasures feature importance and drift\n\n\nLog Loss\nMeasures model performance, not feature drift\n\n\nF1 Score\nMeasures classification performance, not drift\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: What is a key advantage of using SageMaker’s Managed Spot Training?\n\n\n\n\nShow Answer\n\nAnswer: Cost Savings\nExplanation:\n\n\n\n\n\n\n\nTraining Type\nBenefit\n\n\n\n\nSpot Training\nReduces cost by using spare AWS capacity\n\n\nOn-Demand Training\nEnsures availability but is more expensive\n\n\nDedicated Instances\nProvides consistent resources but at a high cost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Which AWS service is used to orchestrate steps in a machine learning pipeline, such as model training and deployment?\n\n\n\n\nShow Answer\n\nAnswer: SageMaker Pipelines\nExplanation:\n\n\n\nService\nPurpose\n\n\n\n\nSageMaker Pipelines\nAutomates ML workflows\n\n\nStep Functions\nGeneral-purpose workflow automation\n\n\nGlue\nUsed for ETL, not ML pipeline orchestration"
  },
  {
    "objectID": "practice/practice-questions-5.html#aws-sagemaker-and-bedrock-quiz",
    "href": "practice/practice-questions-5.html#aws-sagemaker-and-bedrock-quiz",
    "title": "AWS SageMaker and Bedrock Quiz",
    "section": "",
    "text": "This document contains a series of multiple-choice questions with show/hide functionality for answers and detailed explanations.\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You’re working with a sparse dataset and need to predict user-item interactions. Which SageMaker algorithm is most appropriate?\n\n\n\n\nShow Answer\n\nAnswer: Factorization Machines\nExplanation:\n\n\n\n\n\n\n\nAlgorithm\nPurpose\n\n\n\n\nFactorization Machines\nDesigned for sparse data and recommendation systems\n\n\nXGBoost\nHandles structured data but is not optimized for sparse matrices\n\n\nDeepAR\nUsed for time-series forecasting, not recommendation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: In SageMaker’s XGBoost, which hyperparameter helps prevent overfitting by controlling the step size shrinkage?\n\n\n\n\nShow Answer\n\nAnswer: eta\nExplanation:\n\n\n\n\n\n\n\nHyperparameter\nFunction\n\n\n\n\neta\nControls step size shrinkage to prevent overfitting\n\n\nmax_depth\nControls tree depth to prevent overfitting but doesn’t shrink step size\n\n\nlambda\nAdds L2 regularization but doesn’t control learning rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are tasked with training a SageMaker model to predict future values of a time series. Which algorithm should you choose?\n\n\n\n\nShow Answer\n\nAnswer: DeepAR\nExplanation:\n\n\n\nAlgorithm\nPurpose\n\n\n\n\nDeepAR\nSpecifically designed for time series forecasting\n\n\nK-Means\nUsed for clustering, not forecasting\n\n\nObject2Vec\nUsed for embeddings, not time series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Which input format is optimal for training a model using SageMaker’s K-Means algorithm?\n\n\n\n\nShow Answer\n\nAnswer: RecordIO-Protobuf\nExplanation:\n\n\n\nFormat\nSupported?\n\n\n\n\nRecordIO-Protobuf\nYes, optimal for efficiency\n\n\nCSV\nSupported but less efficient\n\n\nJSON\nNot officially supported for K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are using AWS SageMaker JumpStart to deploy a model for text generation. Which model type would you choose for multilingual text generation?\n\n\n\n\nShow Answer\n\nAnswer: Jurassic-2\nExplanation:\n\n\n\nModel\nUse Case\n\n\n\n\nJurassic-2\nStrong multilingual text generation capabilities\n\n\nAmazon Titan\nOptimized for summarization and general text tasks\n\n\nClaude\nDesigned for conversation and automation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You’re working on a Bedrock model that needs to incorporate sensitive customer data during fine-tuning. What steps should you take to ensure the security of this data?\n\n\n\n\nShow Answer\n\nAnswer: Use a VPC and PrivateLink\nExplanation:\n\n\n\n\n\n\n\nSecurity Measure\nPurpose\n\n\n\n\nVPC + PrivateLink\nEnsures secure data transfer\n\n\nS3 Bucket with Encryption\nSecures stored data but does not prevent exposure during transfer\n\n\nIAM Roles\nControls access but does not encrypt data during fine-tuning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You are tasked with deploying a Bedrock model that requires high availability and low-latency responses. Which deployment option should you choose?\n\n\n\n\nShow Answer\n\nAnswer: Provisioned Throughput\nExplanation:\n\n\n\n\n\n\n\nDeployment Option\nPurpose\n\n\n\n\nProvisioned Throughput\nEnsures low-latency and high availability\n\n\nServerless Deployment\nScales automatically but may have variable latency\n\n\nOn-Demand\nSuitable for occasional usage but not optimized for high availability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: You need to monitor a deployed model for feature attribution drift. Which metric should you monitor using SageMaker Model Monitor?\n\n\n\n\nShow Answer\n\nAnswer: SHAP (SHapley Additive exPlanations)\nExplanation:\n\n\n\nMetric\nUse Case\n\n\n\n\nSHAP Values\nMeasures feature importance and drift\n\n\nLog Loss\nMeasures model performance, not feature drift\n\n\nF1 Score\nMeasures classification performance, not drift\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: What is a key advantage of using SageMaker’s Managed Spot Training?\n\n\n\n\nShow Answer\n\nAnswer: Cost Savings\nExplanation:\n\n\n\n\n\n\n\nTraining Type\nBenefit\n\n\n\n\nSpot Training\nReduces cost by using spare AWS capacity\n\n\nOn-Demand Training\nEnsures availability but is more expensive\n\n\nDedicated Instances\nProvides consistent resources but at a high cost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Which AWS service is used to orchestrate steps in a machine learning pipeline, such as model training and deployment?\n\n\n\n\nShow Answer\n\nAnswer: SageMaker Pipelines\nExplanation:\n\n\n\nService\nPurpose\n\n\n\n\nSageMaker Pipelines\nAutomates ML workflows\n\n\nStep Functions\nGeneral-purpose workflow automation\n\n\nGlue\nUsed for ETL, not ML pipeline orchestration"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html",
    "href": "examples/flink-vs-kinesis.html",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Apache Flink and AWS Kinesis are both powerful tools for real-time data processing, but they serve different purposes and have different architectures. Below is a comparison of the two, with definitions, explanations, and code examples.\n\n\n\n\nApache Flink is an open-source stream-processing framework used for distributed processing of data streams. It is designed for stateful computations over unbounded data streams and can handle large-scale, real-time analytics.\n\nKey Features:\n\nReal-time stream processing\nFault tolerance with checkpointing\nSupports batch processing (as a special case of stream processing)\nAdvanced windowing and event time processing\nSupport for complex event processing (CEP)\n\n\n\n\n\nAWS Kinesis is a fully managed service by Amazon Web Services for real-time data streaming and processing. It allows users to collect, process, and analyze real-time data at massive scale.\n\nKey Features:\n\nFully managed, scalable stream processing\nIntegration with AWS ecosystem (e.g., Lambda, Redshift, etc.)\nReal-time analytics\nSupports data ingestion, storage, and analytics on streaming data\nAuto-scaling based on data volume\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nApache Flink\nAWS Kinesis\n\n\n\n\nPurpose\nStream processing and complex event processing\nReal-time data streaming and ingestion\n\n\nManagement\nSelf-managed (on-premise or cloud)\nFully managed service by AWS\n\n\nFault Tolerance\nCheckpointing and state management\nBuilt-in replication and data retention\n\n\nUse Case\nComplex analytics, real-time ETL, CEP\nData ingestion, analytics, and streaming apps\n\n\nScaling\nManual or automated scaling\nAuto-scaling based on data volume\n\n\nIntegrations\nKafka, HDFS, Cassandra, Elasticsearch\nAWS ecosystem (Lambda, Redshift, etc.)\n\n\nLanguage Support\nJava, Scala, Python, SQL\nKinesis Client Library (Java, Python, etc.)\n\n\nLatency\nSub-second latency (low-latency)\nTypically sub-second (low-latency)\n\n\n\n\n\n\n\n\nHere is a basic example of a Flink job that processes a stream of events:\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\npublic class FlinkExample {\n    public static void main(String[] args) throws Exception {\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // Example: Create a stream of integers and map them to a string\n        DataStream&lt;Integer&gt; numbers = env.fromElements(1, 2, 3, 4, 5);\n        \n        DataStream&lt;String&gt; result = numbers.map(new MapFunction&lt;Integer, String&gt;() {\n            @Override\n            public String map(Integer value) throws Exception {\n                return \"Number: \" + value;\n            }\n        });\n\n        result.print(); // Output the result\n        \n        env.execute(\"Flink Streaming Example\");\n    }\n}\nIn this example, Flink reads from a stream of integers, processes them, and outputs a transformed result. AWS Kinesis Example\nBelow is an example of how to produce and consume data from Kinesis streams using the AWS SDK for Python (Boto3).\nKinesis Producer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef send_to_kinesis(stream_name, data):\n    payload = json.dumps(data)\n    kinesis.put_record(\n        StreamName=stream_name,\n        Data=payload,\n        PartitionKey=\"partitionkey\"\n    )\n\n# Example usage\nsend_to_kinesis(\"my-kinesis-stream\", {\"event\": \"start\", \"timestamp\": \"2025-02-03T12:00:00\"})\nKinesis Consumer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef consume_from_kinesis(stream_name):\n    shard_iterator = kinesis.get_shard_iterator(\n        StreamName=stream_name,\n        ShardId='shardId-000000000000',\n        ShardIteratorType='TRIM_HORIZON'\n    )['ShardIterator']\n    \n    while True:\n        record_response = kinesis.get_records(ShardIterator=shard_iterator, Limit=10)\n        for record in record_response['Records']:\n            print(json.loads(record['Data']))\n        \n        shard_iterator = record_response['NextShardIterator']\n\n# Example usage\nconsume_from_kinesis(\"my-kinesis-stream\")\nIn the Kinesis example, we have a producer that sends data to a stream and a consumer that reads data from it. Conclusion\nBoth Apache Flink and AWS Kinesis are valuable tools in the realm of real-time data processing. Flink is a stream-processing framework ideal for complex event processing and advanced analytics, while Kinesis provides a fully managed solution for collecting, processing, and analyzing streaming data.\nThe choice between the two depends on factors such as the scale of your infrastructure, integration needs, and whether you prefer a fully managed service (Kinesis) or a more customizable stream processing framework (Flink).\n\n\n\nFor long running compute tasks, Flink is a better choice as it is designed for stateful computations over unbounded data streams. It can handle large-scale, real-time analytics and complex event processing. On the other hand, Kinesis is more suitable for real-time data streaming and ingestion, making it ideal for data ingestion, analytics, and streaming applications.\nFor GPU compute tasks, Flink can be used with GPU resources for accelerated processing. AWS Kinesis does not provide GPU compute capabilities.\nWhereas Kinesis is suited to real-time data streaming and ingestion, Flink is more versatile and can handle complex analytics, real-time ETL, and complex event processing. For example, Flink can be used for real-time fraud detection, real-time recommendations, and real-time monitoring of IoT devices. The type of compute is small and lightweight, making it ideal for real-time processing of data streams."
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#definitions",
    "href": "examples/flink-vs-kinesis.html#definitions",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Apache Flink is an open-source stream-processing framework used for distributed processing of data streams. It is designed for stateful computations over unbounded data streams and can handle large-scale, real-time analytics.\n\nKey Features:\n\nReal-time stream processing\nFault tolerance with checkpointing\nSupports batch processing (as a special case of stream processing)\nAdvanced windowing and event time processing\nSupport for complex event processing (CEP)\n\n\n\n\n\nAWS Kinesis is a fully managed service by Amazon Web Services for real-time data streaming and processing. It allows users to collect, process, and analyze real-time data at massive scale.\n\nKey Features:\n\nFully managed, scalable stream processing\nIntegration with AWS ecosystem (e.g., Lambda, Redshift, etc.)\nReal-time analytics\nSupports data ingestion, storage, and analytics on streaming data\nAuto-scaling based on data volume"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#key-differences",
    "href": "examples/flink-vs-kinesis.html#key-differences",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Feature\nApache Flink\nAWS Kinesis\n\n\n\n\nPurpose\nStream processing and complex event processing\nReal-time data streaming and ingestion\n\n\nManagement\nSelf-managed (on-premise or cloud)\nFully managed service by AWS\n\n\nFault Tolerance\nCheckpointing and state management\nBuilt-in replication and data retention\n\n\nUse Case\nComplex analytics, real-time ETL, CEP\nData ingestion, analytics, and streaming apps\n\n\nScaling\nManual or automated scaling\nAuto-scaling based on data volume\n\n\nIntegrations\nKafka, HDFS, Cassandra, Elasticsearch\nAWS ecosystem (Lambda, Redshift, etc.)\n\n\nLanguage Support\nJava, Scala, Python, SQL\nKinesis Client Library (Java, Python, etc.)\n\n\nLatency\nSub-second latency (low-latency)\nTypically sub-second (low-latency)"
  },
  {
    "objectID": "examples/flink-vs-kinesis.html#comparison-in-action",
    "href": "examples/flink-vs-kinesis.html#comparison-in-action",
    "title": "Flink vs Kinesis",
    "section": "",
    "text": "Here is a basic example of a Flink job that processes a stream of events:\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n\npublic class FlinkExample {\n    public static void main(String[] args) throws Exception {\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // Example: Create a stream of integers and map them to a string\n        DataStream&lt;Integer&gt; numbers = env.fromElements(1, 2, 3, 4, 5);\n        \n        DataStream&lt;String&gt; result = numbers.map(new MapFunction&lt;Integer, String&gt;() {\n            @Override\n            public String map(Integer value) throws Exception {\n                return \"Number: \" + value;\n            }\n        });\n\n        result.print(); // Output the result\n        \n        env.execute(\"Flink Streaming Example\");\n    }\n}\nIn this example, Flink reads from a stream of integers, processes them, and outputs a transformed result. AWS Kinesis Example\nBelow is an example of how to produce and consume data from Kinesis streams using the AWS SDK for Python (Boto3).\nKinesis Producer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef send_to_kinesis(stream_name, data):\n    payload = json.dumps(data)\n    kinesis.put_record(\n        StreamName=stream_name,\n        Data=payload,\n        PartitionKey=\"partitionkey\"\n    )\n\n# Example usage\nsend_to_kinesis(\"my-kinesis-stream\", {\"event\": \"start\", \"timestamp\": \"2025-02-03T12:00:00\"})\nKinesis Consumer Example\nimport boto3\nimport json\n\n# Initialize Kinesis client\nkinesis = boto3.client('kinesis', region_name='us-east-1')\n\ndef consume_from_kinesis(stream_name):\n    shard_iterator = kinesis.get_shard_iterator(\n        StreamName=stream_name,\n        ShardId='shardId-000000000000',\n        ShardIteratorType='TRIM_HORIZON'\n    )['ShardIterator']\n    \n    while True:\n        record_response = kinesis.get_records(ShardIterator=shard_iterator, Limit=10)\n        for record in record_response['Records']:\n            print(json.loads(record['Data']))\n        \n        shard_iterator = record_response['NextShardIterator']\n\n# Example usage\nconsume_from_kinesis(\"my-kinesis-stream\")\nIn the Kinesis example, we have a producer that sends data to a stream and a consumer that reads data from it. Conclusion\nBoth Apache Flink and AWS Kinesis are valuable tools in the realm of real-time data processing. Flink is a stream-processing framework ideal for complex event processing and advanced analytics, while Kinesis provides a fully managed solution for collecting, processing, and analyzing streaming data.\nThe choice between the two depends on factors such as the scale of your infrastructure, integration needs, and whether you prefer a fully managed service (Kinesis) or a more customizable stream processing framework (Flink).\n\n\n\nFor long running compute tasks, Flink is a better choice as it is designed for stateful computations over unbounded data streams. It can handle large-scale, real-time analytics and complex event processing. On the other hand, Kinesis is more suitable for real-time data streaming and ingestion, making it ideal for data ingestion, analytics, and streaming applications.\nFor GPU compute tasks, Flink can be used with GPU resources for accelerated processing. AWS Kinesis does not provide GPU compute capabilities.\nWhereas Kinesis is suited to real-time data streaming and ingestion, Flink is more versatile and can handle complex analytics, real-time ETL, and complex event processing. For example, Flink can be used for real-time fraud detection, real-time recommendations, and real-time monitoring of IoT devices. The type of compute is small and lightweight, making it ideal for real-time processing of data streams."
  },
  {
    "objectID": "examples/data-formats.html",
    "href": "examples/data-formats.html",
    "title": "File Formats for Data Storage",
    "section": "",
    "text": "When working with data, choosing the right file format for storage is important for performance and cost considerations. Different file formats offer various trade-offs in terms of efficiency, compression, and compatibility with different tools and systems. In this guide, we’ll explore some common file formats used for data storage and processing.\nTo that end there’s many formats to consider, related to organizations’ capabilities and training on AWS. The cost and efficiency difference can be significant, enough to impact the bottom line.\nAWS is a cloud provider that offers a wide range of services for data storage, processing, and analytics. When working with data on AWS, it’s essential to choose the right file format based on your use case, data size, and processing requirements. Here are some common file formats used for data storage on AWS:"
  },
  {
    "objectID": "examples/data-formats.html#parquet",
    "href": "examples/data-formats.html#parquet",
    "title": "File Formats for Data Storage",
    "section": "Parquet",
    "text": "Parquet\nParquet provides efficient columnar storage, high compression, and compatibility with various analytics tools. It’s commonly used for storing structured data in data lakes and data warehouses.\nBusiness Context: A financial services company uses Parquet to store structured transactional data in Amazon S3 and process it efficiently using AWS Glue and Amazon Athena for business intelligence and fraud detection.\nThis is a common use case for Parquet, as it provides columnar storage, efficient compression, and compatibility with various analytics tools.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('data.parquet', 'my-bucket', 'data.parquet')"
  },
  {
    "objectID": "examples/data-formats.html#orc",
    "href": "examples/data-formats.html#orc",
    "title": "File Formats for Data Storage",
    "section": "ORC",
    "text": "ORC\nORC (Optimized Row Columnar) is another columnar storage format that provides efficient compression and query performance for structured data. It’s commonly used with Apache Hive and Presto for processing large-scale datasets.\nBusiness Context: A healthcare analytics firm processes large-scale patient records using Apache Hive on Amazon EMR, benefiting from ORC’s high compression and query performance.\nEMR and ORC a good match, as ORC is optimized for Hive and Presto queries, providing efficient storage and processing of structured data.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('data.orc', 'my-bucket', 'data.orc')"
  },
  {
    "objectID": "examples/data-formats.html#avro",
    "href": "examples/data-formats.html#avro",
    "title": "File Formats for Data Storage",
    "section": "AVRO",
    "text": "AVRO\nAVRO is a row-based data serialization format that provides schema evolution and efficient serialization for data exchange. It’s commonly used in streaming data processing and data pipelines. The benefits of Avro include schema evolution and efficient serialization, making it suitable for streaming data processing and data pipelines.\nBusiness Context:\nA streaming analytics company processes clickstream data using Apache Kafka and stores Avro files in Amazon S3 for downstream analysis with AWS Glue.\nExample Usage in AWS Services:\nimport fastavro\nfrom fastavro.schema import load_schema\n\nschema = load_schema('schema.avsc')\nwith open('data.avro', 'wb') as out:\n    fastavro.writer(out, schema, [{'field1': 'value1'}])"
  },
  {
    "objectID": "examples/data-formats.html#parquet-vs-orc-vs-avro",
    "href": "examples/data-formats.html#parquet-vs-orc-vs-avro",
    "title": "File Formats for Data Storage",
    "section": "Parquet vs ORC vs AVRO",
    "text": "Parquet vs ORC vs AVRO\nParquet, ORC, and Avro are popular file formats for storing and processing data efficiently. They are commonly used in big data processing frameworks like Apache Hadoop, Apache Spark, and AWS services like Amazon EMR and AWS Glue. Here’s a comparison of these file formats. There pros and cons are in this table:\n\n\n\n\n\n\n\n\n\nFeature\nParquet\nORC\nAvro\n\n\n\n\nStorage\nColumnar\nColumnar\nRow-based\n\n\nCompression\nHigh\nHigh\nModerate\n\n\nSchema Evolution\nLimited\nLimited\nFull\n\n\nQuery Performance\nHigh\nHigh\nModerate\n\n\nCompatibility\nVarious tools and systems\nHive, Presto\nStreaming frameworks\n\n\nUse Cases\nData lakes, data warehouses\nLarge-scale datasets\nStreaming data processing"
  },
  {
    "objectID": "examples/data-formats.html#recordio",
    "href": "examples/data-formats.html#recordio",
    "title": "File Formats for Data Storage",
    "section": "RecordIO",
    "text": "RecordIO\nRecordIO is a binary data format used for storing large datasets efficiently, especially for deep learning models. It’s commonly used in Amazon SageMaker for training datasets. It works effiiciently for deep learning models, especially in Amazon SageMaker. The data is stored in a binary format, which is efficient for large datasets.\nBusiness Context: A machine learning startup uses RecordIO to store large training datasets for deep learning models in Amazon SageMaker.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('data.recordio', 'my-bucket', 'data.recordio')"
  },
  {
    "objectID": "examples/data-formats.html#json",
    "href": "examples/data-formats.html#json",
    "title": "File Formats for Data Storage",
    "section": "JSON",
    "text": "JSON\nJSON (JavaScript Object Notation) is a lightweight data interchange format that is easy to read and write. It’s commonly used for storing semi-structured data and exchanging information between systems. JSON is widely used for storing semi-structured data and exchanging information between systems. It is easy to read and write, making it a popular choice for many applications, particularly web services.\nBusiness Context: An e-commerce platform stores product catalog metadata in JSON format and processes it using AWS Lambda and DynamoDB.\nExample Usage in AWS Services:\nimport json\nimport boto3\n\ndata = {'product_id': 123, 'name': 'Laptop'}\njson_data = json.dumps(data)\n\ns3 = boto3.client('s3')\ns3.put_object(Bucket='my-bucket', Key='product.json', Body=json_data)"
  },
  {
    "objectID": "examples/data-formats.html#json-lines",
    "href": "examples/data-formats.html#json-lines",
    "title": "File Formats for Data Storage",
    "section": "JSON Lines",
    "text": "JSON Lines\nJSON Lines (JSONL) is a format for storing multiple JSON objects in a single file, with each object on a separate line. It’s commonly used for log files and streaming data processing. JSON Lines is a format for storing multiple JSON objects in a single file, with each object on a separate line. It is commonly used for log files and streaming data processing. That way each line is a separate JSON object, making it easy to read and process.\nBusiness Context: A social media analytics company stores user activity logs in JSON Lines format for real-time processing with Amazon Kinesis.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\n\nwith open('logs.jsonl', 'rb') as f:\n    s3.upload_fileobj(f, 'my-bucket', 'logs.jsonl')"
  },
  {
    "objectID": "examples/data-formats.html#csv",
    "href": "examples/data-formats.html#csv",
    "title": "File Formats for Data Storage",
    "section": "CSV",
    "text": "CSV\nCSV is a simple tabular data format that is widely used for storing and exchanging structured data. It’s commonly used in data processing pipelines and analytics workflows. CSV is a simple tabular data format that is widely used for storing and exchanging structured data. It is commonly used in data processing pipelines and analytics workflows. It is easy to read and write, making it a popular choice for many applications, and a legacy format for many systems.\nBusiness Context: A retail chain stores daily sales transactions in CSV format and loads them into Amazon Redshift for reporting and analytics.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('sales.csv', 'my-bucket', 'sales.csv')"
  },
  {
    "objectID": "examples/data-formats.html#bzip",
    "href": "examples/data-formats.html#bzip",
    "title": "File Formats for Data Storage",
    "section": "BZip",
    "text": "BZip\nBZip is a compression format that provides high compression ratios for text data. It’s commonly used for compressing large text files before storage or transfer. BZip is a compression format that provides high compression ratios for text data. It is commonly used for compressing large text files before storage or transfer.\nBusiness Context: A genomics research institute compresses large DNA sequencing datasets using BZip and stores them in Amazon S3 for high-throughput computing.\nExample Usage in AWS Services:\nimport bz2\n\nwith bz2.BZ2File('data.bz2', 'wb') as f:\n    f.write(b'sample data')"
  },
  {
    "objectID": "examples/data-formats.html#pickle",
    "href": "examples/data-formats.html#pickle",
    "title": "File Formats for Data Storage",
    "section": "Pickle",
    "text": "Pickle\nPickle is a serialization format in Python that allows objects to be serialized and deserialized. It’s commonly used for saving and loading machine learning models and other Python objects. Pickle is a serialization format in Python that allows objects to be serialized and deserialized. It is commonly used for saving and loading machine learning models and other Python objects.\nBusiness Context: A data science consultancy saves model objects in pickle format for rapid loading into Amazon SageMaker endpoints.\nExample Usage in AWS Services:\nimport pickle\nimport boto3\n\ndata = {'key': 'value'}\nwith open('data.pkl', 'wb') as f:\n    pickle.dump(data, f)\n\ns3 = boto3.client('s3')\ns3.upload_file('data.pkl', 'my-bucket', 'data.pkl')"
  },
  {
    "objectID": "examples/data-formats.html#apache-arrow",
    "href": "examples/data-formats.html#apache-arrow",
    "title": "File Formats for Data Storage",
    "section": "Apache Arrow",
    "text": "Apache Arrow\nArrow format is a columnar in-memory data format that provides efficient data interchange between different systems. It’s commonly used for high-performance analytics and data processing. Arrow format is a columnar in-memory data format that provides efficient data interchange between different systems. It is commonly used for high-performance analytics and data processing.\nBusiness Context: A high-frequency trading firm shares structured data across multiple processing systems using Apache Arrow for in-memory analytics.\nExample Usage in AWS Services:\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\narray = pa.array([1, 2, 3])\ntable = pa.Table.from_arrays([array], names=['column'])\npq.write_table(table, 'data.arrow')"
  },
  {
    "objectID": "examples/data-formats.html#avro-recordio",
    "href": "examples/data-formats.html#avro-recordio",
    "title": "File Formats for Data Storage",
    "section": "AVRO RecordIO",
    "text": "AVRO RecordIO\nAVRO Record IO is a binary data format used for storing training datasets efficiently, especially for deep learning model training. It’s commonly used in cloud-based AI platforms like Amazon SageMaker.\nBusiness Context: A cloud-based AI company stores training datasets in AVRO RecordIO format for efficient deep learning model training on Amazon SageMaker.\nExample Usage in AWS Services:\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('data.avro.recordio', 'my-bucket', 'data.avro.recordio')"
  },
  {
    "objectID": "examples/data-formats.html#thoughts",
    "href": "examples/data-formats.html#thoughts",
    "title": "File Formats for Data Storage",
    "section": "Thoughts",
    "text": "Thoughts\nSo which should you use? It depends on the use case. For example, Parquet and ORC are great for data lakes and warehouses, while JSON and CSV are more common for data exchange. The choice of file format can impact performance, storage costs, and compatibility with different tools and systems. It’s important to consider these factors when choosing a file format for data storage and processing on AWS."
  },
  {
    "objectID": "examples/data-architecture.html",
    "href": "examples/data-architecture.html",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "",
    "text": "Modern data architectures have evolved significantly, leading to various approaches for storing, processing, and analyzing data. In this blog post, we’ll explore four key paradigms: Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake. Each approach has its strengths and trade-offs, and choosing the right one depends on business needs, scalability, and analytical requirements."
  },
  {
    "objectID": "examples/data-architecture.html#key-features",
    "href": "examples/data-architecture.html#key-features",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Key Features:",
    "text": "Key Features:\n\nStructured Data: Data is organized into predefined schemas (e.g., star or snowflake schema).\nOptimized for Analytics: Uses columnar storage and indexing for fast queries.\nETL Process: Data is extracted, transformed, and loaded (ETL) into the warehouse.\nBatch Processing: Updates happen at scheduled intervals."
  },
  {
    "objectID": "examples/data-architecture.html#example-code-using-amazon-redshift",
    "href": "examples/data-architecture.html#example-code-using-amazon-redshift",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Example Code (Using Amazon Redshift)",
    "text": "Example Code (Using Amazon Redshift)\nCREATE TABLE sales (\n    sale_id INT PRIMARY KEY,\n    product VARCHAR(255),\n    amount DECIMAL(10,2),\n    sale_date DATE\n);\n\nCOPY sales\nFROM 's3://mybucket/sales_data.csv'\nIAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftRole'\nCSV;"
  },
  {
    "objectID": "examples/data-architecture.html#when-to-use",
    "href": "examples/data-architecture.html#when-to-use",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Business intelligence and reporting ✅ Structured data with predefined schemas ✅ High-speed analytical queries"
  },
  {
    "objectID": "examples/data-architecture.html#limitations",
    "href": "examples/data-architecture.html#limitations",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Poor handling of unstructured or semi-structured data ❌ Expensive to scale"
  },
  {
    "objectID": "examples/data-architecture.html#key-principles",
    "href": "examples/data-architecture.html#key-principles",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Key Principles:",
    "text": "Key Principles:\n\nDomain-Oriented Ownership: Each team owns and manages its data.\nSelf-Service Data Infrastructure: Tools and platforms enable teams to build and share data products.\nFederated Governance: Policies and security controls are standardized across domains.\nData as a Product: Teams treat data like an API-driven product."
  },
  {
    "objectID": "examples/data-architecture.html#example-using-aws-data-catalog-for-a-data-mesh",
    "href": "examples/data-architecture.html#example-using-aws-data-catalog-for-a-data-mesh",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Example (Using AWS Data Catalog for a Data Mesh)",
    "text": "Example (Using AWS Data Catalog for a Data Mesh)\nimport boto3\nclient = boto3.client('glue')\n\n# Create a new database for a domain-specific data product\nclient.create_database(\n    DatabaseInput={'Name': 'customer_data', 'Description': 'Customer analytics data'}\n)"
  },
  {
    "objectID": "examples/data-architecture.html#when-to-use-1",
    "href": "examples/data-architecture.html#when-to-use-1",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Large organizations with multiple teams handling data ✅ Need for scalable and flexible data sharing ✅ Avoiding centralized bottlenecks"
  },
  {
    "objectID": "examples/data-architecture.html#limitations-1",
    "href": "examples/data-architecture.html#limitations-1",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Requires cultural shift and strong governance ❌ Complexity in managing decentralized teams"
  },
  {
    "objectID": "examples/data-architecture.html#key-features-1",
    "href": "examples/data-architecture.html#key-features-1",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Key Features:",
    "text": "Key Features:\n\nSchema-on-Read: Data is ingested as-is and transformed when queried.\nSupports All Data Types: Text, images, videos, and logs.\nLow Cost & Scalability: Typically stored in cloud-based object storage like Amazon S3.\nBatch & Streaming Processing: Can handle real-time and batch workloads."
  },
  {
    "objectID": "examples/data-architecture.html#example-using-aws-s3-for-a-data-lake",
    "href": "examples/data-architecture.html#example-using-aws-s3-for-a-data-lake",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Example (Using AWS S3 for a Data Lake)",
    "text": "Example (Using AWS S3 for a Data Lake)\nimport boto3\ns3 = boto3.client('s3')\n\n# Upload raw data to S3\ns3.upload_file('local_file.json', 'my-data-lake-bucket', 'raw/data.json')"
  },
  {
    "objectID": "examples/data-architecture.html#when-to-use-2",
    "href": "examples/data-architecture.html#when-to-use-2",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Storing vast amounts of raw data ✅ Machine learning, big data, and exploratory analysis ✅ Cost-effective storage"
  },
  {
    "objectID": "examples/data-architecture.html#limitations-2",
    "href": "examples/data-architecture.html#limitations-2",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Query performance can be slow without indexing ❌ Risk of becoming a “data swamp” if not properly managed"
  },
  {
    "objectID": "examples/data-architecture.html#key-features-2",
    "href": "examples/data-architecture.html#key-features-2",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Key Features:",
    "text": "Key Features:\n\nSchema Enforcement & Governance: Supports ACID transactions and structured querying.\nUnified Storage: Uses a data lake but adds indexing and metadata layers.\nSupports BI & ML Workloads: Optimized for both analytics and AI applications.\nOpen Table Formats: Technologies like Apache Iceberg and Delta Lake."
  },
  {
    "objectID": "examples/data-architecture.html#example-using-delta-lake-with-pyspark",
    "href": "examples/data-architecture.html#example-using-delta-lake-with-pyspark",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Example (Using Delta Lake with PySpark)",
    "text": "Example (Using Delta Lake with PySpark)\nfrom delta import *\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Lakehouse\")\\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n    .getOrCreate()\n\n# Create a Delta Lake table\nspark.sql(\"CREATE TABLE sales_delta (id INT, amount FLOAT) USING delta\")"
  },
  {
    "objectID": "examples/data-architecture.html#when-to-use-3",
    "href": "examples/data-architecture.html#when-to-use-3",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Need structured querying on a data lake ✅ Unified analytics and machine learning ✅ Avoiding data duplication across systems"
  },
  {
    "objectID": "examples/data-architecture.html#limitations-3",
    "href": "examples/data-architecture.html#limitations-3",
    "title": "Understanding Data Warehousing, Data Mesh, Data Lakehouse, and Data Lake",
    "section": "Limitations:",
    "text": "Limitations:\n❌ More complex than a pure data warehouse or lake ❌ Newer technology with evolving best practices"
  },
  {
    "objectID": "examples/glue-athena.html",
    "href": "examples/glue-athena.html",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "",
    "text": "AWS offers a suite of data processing and analytics tools to help organizations manage large-scale data efficiently. In this blog post, we will explore AWS Athena, AWS Glue DataBrew, AWS Glue Data Catalog, AWS Glue ETL, AWS Glue Crawlers, AWS Glue Studio, and AWS Glue Data Quality. Each of these services plays a unique role in data processing and analytics."
  },
  {
    "objectID": "examples/glue-athena.html#key-features",
    "href": "examples/glue-athena.html#key-features",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nServerless: No infrastructure management required.\nSQL-Based Queries: Uses standard SQL via Presto and Trino engines.\nPay-Per-Query: Charges based on the amount of data scanned.\nIntegration with AWS Glue: Uses Glue Data Catalog for schema management."
  },
  {
    "objectID": "examples/glue-athena.html#example-query-querying-an-s3-dataset",
    "href": "examples/glue-athena.html#example-query-querying-an-s3-dataset",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example Query (Querying an S3 Dataset):",
    "text": "Example Query (Querying an S3 Dataset):\nSELECT * FROM sales_data\nWHERE transaction_date &gt;= '2024-01-01';"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use",
    "href": "examples/glue-athena.html#when-to-use",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Ad-hoc analysis on S3 data\n✅ Business intelligence and dashboards\n✅ Cost-effective querying"
  },
  {
    "objectID": "examples/glue-athena.html#limitations",
    "href": "examples/glue-athena.html#limitations",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Performance depends on partitioning and file format optimization\n❌ No built-in transaction support"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-1",
    "href": "examples/glue-athena.html#key-features-1",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nNo-Code Data Cleaning: GUI-based transformations.\nPrebuilt Transformations: 250+ transformations like normalization, deduplication, and missing value handling.\nIntegration with S3, Redshift, and Athena: Works with popular AWS data services."
  },
  {
    "objectID": "examples/glue-athena.html#example-applying-a-transformation-using-python-api",
    "href": "examples/glue-athena.html#example-applying-a-transformation-using-python-api",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example (Applying a Transformation Using Python API):",
    "text": "Example (Applying a Transformation Using Python API):\nimport boto3\nclient = boto3.client('databrew')\nresponse = client.create_recipe(\n    Name='clean_sales_data',\n    Steps=[\n        {'Action': 'REMOVE_DUPLICATES', 'Column': 'order_id'},\n        {'Action': 'FILL_MISSING_VALUES', 'Column': 'price', 'Value': '0'}\n    ]\n)"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-1",
    "href": "examples/glue-athena.html#when-to-use-1",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Data cleaning and transformation without coding\n✅ Exploratory data analysis\n✅ Preparing data for machine learning"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-1",
    "href": "examples/glue-athena.html#limitations-1",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Not ideal for large-scale ETL pipelines\n❌ Limited advanced transformation capabilities"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-2",
    "href": "examples/glue-athena.html#key-features-2",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nCentralized Metadata Store: Stores schema and table definitions.\nSupports Multiple Services: Used by Athena, Redshift, and EMR.\nAutomated Schema Detection: Works with Glue Crawlers."
  },
  {
    "objectID": "examples/glue-athena.html#example-creating-a-table-in-glue-data-catalog",
    "href": "examples/glue-athena.html#example-creating-a-table-in-glue-data-catalog",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example (Creating a Table in Glue Data Catalog):",
    "text": "Example (Creating a Table in Glue Data Catalog):\nimport boto3\nclient = boto3.client('glue')\nresponse = client.create_table(\n    DatabaseName='sales_db',\n    TableInput={\n        'Name': 'sales_data',\n        'StorageDescriptor': {\n            'Columns': [\n                {'Name': 'order_id', 'Type': 'string'},\n                {'Name': 'amount', 'Type': 'double'}\n            ],\n            'Location': 's3://my-bucket/sales/'\n        }\n    }\n)"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-2",
    "href": "examples/glue-athena.html#when-to-use-2",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Managing table metadata for Athena and Redshift\n✅ Schema discovery and governance"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-2",
    "href": "examples/glue-athena.html#limitations-2",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Only supports AWS data sources"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-3",
    "href": "examples/glue-athena.html#key-features-3",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nServerless Apache Spark-Based Processing\nJob Scheduling & Dependency Management\nAutomatic Schema Inference\nSupports Python and Scala"
  },
  {
    "objectID": "examples/glue-athena.html#example-simple-etl-script-in-pyspark",
    "href": "examples/glue-athena.html#example-simple-etl-script-in-pyspark",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example (Simple ETL Script in PySpark):",
    "text": "Example (Simple ETL Script in PySpark):\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\nglueContext = GlueContext(SparkContext.getOrCreate())\ndf = glueContext.create_dynamic_frame.from_catalog(database=\"sales_db\", table_name=\"sales_data\")\ndf.show()"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-3",
    "href": "examples/glue-athena.html#when-to-use-3",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Large-scale data transformations\n✅ Data migration across AWS services"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-3",
    "href": "examples/glue-athena.html#limitations-3",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Learning curve for PySpark-based development"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-4",
    "href": "examples/glue-athena.html#key-features-4",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nSchema Detection & Updating\nSupports JSON, CSV, Parquet, and more\nRuns on a schedule or on-demand"
  },
  {
    "objectID": "examples/glue-athena.html#example-creating-a-glue-crawler",
    "href": "examples/glue-athena.html#example-creating-a-glue-crawler",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example (Creating a Glue Crawler):",
    "text": "Example (Creating a Glue Crawler):\nimport boto3\nclient = boto3.client('glue')\nresponse = client.create_crawler(\n    Name='sales_crawler',\n    Role='AWSGlueServiceRole',\n    DatabaseName='sales_db',\n    Targets={'S3Targets': [{'Path': 's3://my-bucket/sales/'}]}\n)"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-4",
    "href": "examples/glue-athena.html#when-to-use-4",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Keeping schema definitions updated\n✅ Automating data discovery"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-4",
    "href": "examples/glue-athena.html#limitations-4",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Not optimized for highly dynamic schemas"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-5",
    "href": "examples/glue-athena.html#key-features-5",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nDrag-and-Drop UI for ETL Pipelines\nCode Generation for Python & Spark\nIntegrated Job Monitoring"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-5",
    "href": "examples/glue-athena.html#when-to-use-5",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Simplifying ETL development\n✅ Visualizing data pipelines"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-5",
    "href": "examples/glue-athena.html#limitations-5",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Limited advanced transformations"
  },
  {
    "objectID": "examples/glue-athena.html#key-features-6",
    "href": "examples/glue-athena.html#key-features-6",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Key Features:",
    "text": "Key Features:\n\nAutomated Data Quality Rules\nAnomaly Detection and Validation\nIntegration with Glue ETL and Athena"
  },
  {
    "objectID": "examples/glue-athena.html#example-running-a-data-quality-check-in-glue",
    "href": "examples/glue-athena.html#example-running-a-data-quality-check-in-glue",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Example (Running a Data Quality Check in Glue):",
    "text": "Example (Running a Data Quality Check in Glue):\nimport boto3\nclient = boto3.client('glue')\nresponse = client.create_data_quality_ruleset(\n    Name='sales_data_quality',\n    RulesetExpression='amount &gt; 0'\n)"
  },
  {
    "objectID": "examples/glue-athena.html#when-to-use-6",
    "href": "examples/glue-athena.html#when-to-use-6",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Ensuring clean and valid data\n✅ Identifying anomalies"
  },
  {
    "objectID": "examples/glue-athena.html#limitations-6",
    "href": "examples/glue-athena.html#limitations-6",
    "title": "Understanding AWS Athena, Glue, and Data Processing Services",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Limited rule customization"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a collection of notes, comparisons and tools that I’ve found useful while studying for AWS MLE Associate certifications. It’s a work in progress, and I’ll be adding more content over time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AWS Machine Learning Engineer Associate",
    "section": "",
    "text": "AWS MLE Associate Badge\n\n\nWelcome, this is a work in progress. I’m creating this site to help me study for the AWS Machine Learning Engineer Associate certification. I’ll be adding more content over time, so check back for updates."
  },
  {
    "objectID": "aws-mla-exam-resources.html",
    "href": "aws-mla-exam-resources.html",
    "title": "AWS MLA Exam Resources",
    "section": "",
    "text": "Overview of AWS Machine Learning Services\nThe course is extensive, so detailed practice should help you prepare for the exam. There are a few approaches that can be taken.\nHere’s a great list of resources on reddit.\nStephan Maarek and Frank Kane’s courses are the most well known paid course at this time.\nHere’s a link to the Udemy course and practice exams\nThere’s also tutorial dojo practice exams\nAdditionally, there are some free (and paid) official AWS resources:\n\nCertification Page\nExam Guide\nFree SkillBuilder\nOfficial Free Practice Questions\nOfficial Paid Practice Questions"
  },
  {
    "objectID": "aws-definitions.html",
    "href": "aws-definitions.html",
    "title": "AWS Services Definitions",
    "section": "",
    "text": "Click each dropdown to reveal the definition/explanation of each service."
  },
  {
    "objectID": "aws-definitions.html#instructions",
    "href": "aws-definitions.html#instructions",
    "title": "AWS Services Definitions",
    "section": "",
    "text": "Click each dropdown to reveal the definition/explanation of each service."
  },
  {
    "objectID": "aws-definitions.html#data-services",
    "href": "aws-definitions.html#data-services",
    "title": "AWS Services Definitions",
    "section": "Data Services",
    "text": "Data Services\n\n\n\n\n\n\nAmazon Kinesis Data Streams\n\n\n\n\n\nA scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources.\n\n\n\n\n\n\n\n\n\nAmazon Kinesis Data Firehose\n\n\n\n\n\nA fully managed service that scales automatically to match the throughput of your data. It can batch, compress, and encrypt data before loading it into storage and analytics services.\n\n\n\n\n\n\n\n\n\nAmazon MSK (Managed Streaming for Apache Kafka)\n\n\n\n\n\nA fully managed service that makes it easy to build and run applications using Apache Kafka, an open-source distributed event streaming platform."
  },
  {
    "objectID": "aws-definitions.html#storage-solutions",
    "href": "aws-definitions.html#storage-solutions",
    "title": "AWS Services Definitions",
    "section": "Storage Solutions",
    "text": "Storage Solutions\n\n\n\n\n\n\nAmazon S3\n\n\n\n\n\nHighly scalable object storage service offering industry-leading durability, security, and performance. Store and retrieve any amount of data from anywhere.\n\n\n\n\n\n\n\n\n\nAmazon EBS, FSx, and EFS\n\n\n\n\n\n\nAmazon EBS (Elastic Block Store): Block-level storage for EC2 instances.\nAmazon FSx: Fully managed file storage for Windows, Lustre, and other file systems.\nAmazon EFS (Elastic File System): Fully managed, scalable NFS file storage."
  },
  {
    "objectID": "aws-definitions.html#big-data-analytics",
    "href": "aws-definitions.html#big-data-analytics",
    "title": "AWS Services Definitions",
    "section": "Big Data & Analytics",
    "text": "Big Data & Analytics\n\n\n\n\n\n\nApache Hadoop\n\n\n\n\n\nAn open-source framework for distributed processing of large datasets across clusters of computers.\n\n\n\n\n\n\n\n\n\nAWS Glue\n\n\n\n\n\nA fully managed extract, transform, and load (ETL) service that simplifies data preparation for analytics.\n\n\n\n\n\n\n\n\n\nAmazon EMR (Elastic MapReduce)\n\n\n\n\n\nA cloud big data platform for processing vast amounts of data using open-source frameworks like Apache Spark, Hadoop, and Presto.\n\n\n\n\n\n\n\n\n\nAmazon Athena\n\n\n\n\n\nAn interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\n\n\n\n\n\n\n\n\n\nAWS Glue DataBrew\n\n\n\n\n\nA visual data preparation tool that makes it easy for data analysts and data scientists to clean and normalize data.\n\n\n\n\n\n\n\n\n\nAWS Glue Data Quality\n\n\n\n\n\nA service that helps you measure and monitor the quality of your data.\n\n\n\n\n\n\n\n\n\nAmazon Kinesis\n\n\n\n\n\nA platform on AWS to collect, process, and analyze real-time, streaming data.\n\n\n\n\n\n\n\n\n\nAWS Lake Formation\n\n\n\n\n\nA service that makes it easy to set up a secure data lake in days.\n\n\n\n\n\n\n\n\n\nAmazon Managed Service for Apache Flink\n\n\n\n\n\nA fully managed service that enables you to build and run Apache Flink applications.\n\n\n\n\n\n\n\n\n\nAmazon OpenSearch Service\n\n\n\n\n\nA fully managed service that makes it easy to deploy, secure, and operate OpenSearch at scale.\n\n\n\n\n\n\n\n\n\nAmazon QuickSight\n\n\n\n\n\nA scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud.\n\n\n\n\n\n\n\n\n\nAmazon Redshift\n\n\n\n\n\nA fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools."
  },
  {
    "objectID": "aws-definitions.html#aiml-services",
    "href": "aws-definitions.html#aiml-services",
    "title": "AWS Services Definitions",
    "section": "AI/ML Services",
    "text": "AI/ML Services\n\n\n\n\n\n\nAmazon Bedrock\n\n\n\n\n\nA fully managed service that provides access to foundation models from leading AI providers via a single API.\n\n\n\n\n\n\n\n\n\nAmazon Comprehend\n\n\n\n\n\nA natural language processing (NLP) service that uses machine learning to analyze text for insights, such as sentiment and key phrases.\n\n\n\n\n\n\n\n\n\nAmazon Personalize\n\n\n\n\n\nA machine learning service for building real-time personalized recommendations.\n\n\n\n\n\n\n\n\n\nAmazon Lex\n\n\n\n\n\nA fully managed AI service for building conversational interfaces using speech and text.\n\n\n\n\n\n\n\n\n\nAmazon Polly\n\n\n\n\n\nA text-to-speech service that converts written text into lifelike speech.\n\n\n\n\n\n\n\n\n\nAmazon Rekognition\n\n\n\n\n\nA deep learning-based image and video analysis service for object detection, facial recognition, and content moderation.\n\n\n\n\n\n\n\n\n\nAmazon Textract\n\n\n\n\n\nAn AI service that automatically extracts text, handwriting, and data from scanned documents.\n\n\n\n\n\n\n\n\n\nAmazon Transcribe\n\n\n\n\n\nA speech-to-text service that uses machine learning to convert spoken language into written text.\n\n\n\n\n\n\n\n\n\nAmazon Kendra\n\n\n\n\n\nAn intelligent search service that uses machine learning to provide highly accurate search results.\n\n\n\n\n\n\n\n\n\nAmazon SageMaker\n\n\n\n\n\nA fully managed service that provides tools to build, train, and deploy machine learning models at scale.\n\n\n\n\n\n\n\n\n\nAWS Panorama\n\n\n\n\n\nA machine learning appliance and software development kit (SDK) that enables adding computer vision to on-premises cameras.\n\n\n\n\n\n\n\n\n\nAmazon CodeWhisperer\n\n\n\n\n\nAn AI-powered coding assistant that provides real-time code suggestions.\n\n\n\n\n\n\n\n\n\nAmazon HealthLake\n\n\n\n\n\nA HIPAA-eligible service that uses machine learning to store, transform, and analyze health data.\n\n\n\n\n\n\n\n\n\nAmazon Forecast\n\n\n\n\n\nA time-series forecasting service that uses machine learning to predict future trends.\n\n\n\n\n\n\n\n\n\nAmazon Fraud Detector\n\n\n\n\n\nAn AI service that helps identify potentially fraudulent activities in real-time."
  },
  {
    "objectID": "aws-definitions.html#sagemaker-ecosystem",
    "href": "aws-definitions.html#sagemaker-ecosystem",
    "title": "AWS Services Definitions",
    "section": "SageMaker Ecosystem",
    "text": "SageMaker Ecosystem\n\n\n\n\n\n\nAmazon SageMaker Studio\n\n\n\n\n\nA fully integrated development environment (IDE) for machine learning with a single, web-based visual interface. Provides capabilities for data preparation, model building, training, debugging, and deployment.\n\n\n\n\n\n\n\n\n\nBuilt-in SageMaker Algorithms\n\n\n\n\n\nPrebuilt algorithms optimized for performance and scalability: - Linear Learner (Regression, Classification) - XGBoost (Gradient Boosting) - DeepAR (Time Series Forecasting) - BlazingText (Word2Vec, Text Classification) - Object Detection (Image Analysis) - Image Classification (ResNet, VGG, etc.) - Semantic Segmentation - Factorization Machines (Recommender Systems) - Neural Topic Model (Topic Modeling) - K-Means Clustering - PCA (Dimensionality Reduction) - LDA (Latent Dirichlet Allocation) - Sequence-to-Sequence (Translation, Text Generation) - Random Cut Forest (Anomaly Detection) - And more…\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Pipelines\n\n\n\n\n\nA CI/CD service for automating machine learning workflows, including data preprocessing, model training, and deployment. Features include: - Step-based workflow definition - Integration with SageMaker Training, Processing, and Model Registry - Lineage tracking and experiment management - Execution via SDK or AWS Console\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Model Monitor\n\n\n\n\n\nContinuously monitors deployed models for data drift and quality issues. Key features: - Baseline creation from training data - Monitoring statistics and visualizations - Alerts and automated remediation - Integration with SageMaker Pipelines\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Inference & Deployment\n\n\n\n\n\nMultiple options for deploying models: - Real-time inference: Deploy models as endpoints with auto-scaling - Batch Transform: Process large datasets asynchronously - Serverless inference: Deploy without managing infrastructure - Edge deployment: Use SageMaker Edge Manager for on-device inference\n\n\n\n\n\n\n\n\n\nAmazon SageMaker APIs\n\n\n\n\n\nSageMaker provides SDKs and APIs for integrating ML into applications: - Boto3 (Python SDK): Automate SageMaker workflows - SageMaker SDK: Train, deploy, and manage models - SageMaker JumpStart: Access pre-trained models and solutions - Model Registry API: Manage model versions and deployments\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Feature Store\n\n\n\n\n\nA fully managed repository to store, update, and retrieve ML features. Supports: - Online and offline stores for low-latency and batch processing - Feature versioning and governance - Integration with SageMaker Pipelines and Model Monitor\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Autopilot\n\n\n\n\n\nAutomates model selection, hyperparameter tuning, and feature engineering. Provides: - Explainability reports - Candidate model evaluation - Deployable models without manual tuning\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Debugger\n\n\n\n\n\nAutomatically detects training issues, such as overfitting and vanishing gradients. Key features: - Real-time monitoring of training metrics - Integration with TensorBoard - Automated rule-based alerts\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Ground Truth\n\n\n\n\n\nA data labeling service that supports: - Human-in-the-loop labeling - Automated labeling with active learning - Integration with Amazon Mechanical Turk and private workforce\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Data Wrangler\n\n\n\n\n\nSimplifies data preparation and feature engineering with: - Pre-built data transformations - Automated data quality checks - Integration with SageMaker Pipelines\n\n\n\n\n\n\n\n\n\nAmazon SageMaker Clarify\n\n\n\n\n\nProvides explainability and bias detection tools: - Feature importance analysis - Bias detection in datasets and models - Integration with Model Monitor for ongoing fairness assessment"
  },
  {
    "objectID": "aws-definitions.html#security-networking",
    "href": "aws-definitions.html#security-networking",
    "title": "AWS Services Definitions",
    "section": "Security & Networking",
    "text": "Security & Networking\n\n\n\n\n\n\nAmazon VPC Components\n\n\n\n\n\n\nVPC: A private, isolated cloud network.\nSubnets: Segments of VPC IP address ranges.\nInternet Gateway: Enables internet access for VPC.\nNAT Gateway: Allows private subnets to access the internet securely.\nSecurity Groups: Instance-level firewalls for controlling inbound/outbound traffic.\n\n\n\n\n\n\n\n\n\n\nAWS Key Management Service (KMS)\n\n\n\n\n\nA fully managed service that makes it easy to create, control, and use cryptographic keys to encrypt your data."
  },
  {
    "objectID": "aws-definitions.html#devops-cicd",
    "href": "aws-definitions.html#devops-cicd",
    "title": "AWS Services Definitions",
    "section": "DevOps & CI/CD",
    "text": "DevOps & CI/CD\n\n\n\n\n\n\nAWS CodePipeline\n\n\n\n\n\nA fully managed continuous integration and continuous delivery (CI/CD) service that automates software release pipelines.\n\n\n\n\n\n\n\n\n\nContainers & Orchestration\n\n\n\n\n\n\nDocker: A platform for building, shipping, and running applications in containers.\nAmazon ECS: A scalable container orchestration service.\nAmazon ECR: A fully managed container registry for storing Docker images.\nAmazon EKS: A managed Kubernetes service for running containerized applications."
  },
  {
    "objectID": "aws-definitions.html#monitoring-management",
    "href": "aws-definitions.html#monitoring-management",
    "title": "AWS Services Definitions",
    "section": "Monitoring & Management",
    "text": "Monitoring & Management\n\n\n\n\n\n\nAmazon CloudWatch\n\n\n\n\n\nA monitoring and observability service for logs, metrics, alarms, and application performance.\n\n\n\n\n\n\n\n\n\nAWS Glue (ETL)\n\n\n\n\n\nA fully managed ETL (Extract, Transform, Load) service that prepares and integrates data for analytics and machine learning."
  },
  {
    "objectID": "aws-definitions.html#application-integration",
    "href": "aws-definitions.html#application-integration",
    "title": "AWS Services Definitions",
    "section": "Application Integration",
    "text": "Application Integration\n\n\n\n\n\n\nAmazon EventBridge\n\n\n\n\n\nA serverless event bus service that makes it easy to connect applications using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services.\n\n\n\n\n\n\n\n\n\nAmazon Managed Workflows for Apache Airflow (Amazon MWAA)\n\n\n\n\n\nA managed orchestration service for Apache Airflow that makes it easy to set up and operate end-to-end data pipelines in the cloud.\n\n\n\n\n\n\n\n\n\nAmazon Simple Notification Service (Amazon SNS)\n\n\n\n\n\nA fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication.\n\n\n\n\n\n\n\n\n\nAmazon Simple Queue Service (Amazon SQS)\n\n\n\n\n\nA fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\n\n\n\n\n\n\n\n\n\nAWS Step Functions\n\n\n\n\n\nA serverless orchestration service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications."
  },
  {
    "objectID": "aws-definitions.html#cloud-financial-management",
    "href": "aws-definitions.html#cloud-financial-management",
    "title": "AWS Services Definitions",
    "section": "Cloud Financial Management",
    "text": "Cloud Financial Management\n\n\n\n\n\n\nAWS Billing and Cost Management\n\n\n\n\n\nA service that helps you pay your AWS bill, monitor your usage, and budget your costs.\n\n\n\n\n\n\n\n\n\nAWS Budgets\n\n\n\n\n\nA service that lets you set custom cost and usage budgets and receive alerts when you exceed them.\n\n\n\n\n\n\n\n\n\nAWS Cost Explorer\n\n\n\n\n\nA service that enables you to visualize, understand, and manage your AWS costs and usage over time."
  },
  {
    "objectID": "aws-comparison-tables.html",
    "href": "aws-comparison-tables.html",
    "title": "AWS Comparison Tables",
    "section": "",
    "text": "Feature\nKinesis Data Streams\nAWS Firehose\nMSK (Managed Kafka)\n\n\n\n\nLatency\nReal-time (sub-second)\nNear real-time (60s minimum)\nReal-time (sub-second)\n\n\nScaling\nManual provisioning of shards\nAutomatic scaling\nManual provisioning of brokers\n\n\nData Retention\n24h by default, up to 365 days\nNo retention (immediate processing)\nConfigurable, unlimited\n\n\nUse Cases\nReal-time analytics, processing logs\nData loading into S3, Redshift, ES\nEvent streaming, log aggregation\n\n\nThroughput\nMB/s per shard\nAutomatic up to GB/s\nMB/s per broker\n\n\nCost Model\nPer shard hour\nPer GB processed\nPer broker hour"
  },
  {
    "objectID": "aws-comparison-tables.html#streaming-services",
    "href": "aws-comparison-tables.html#streaming-services",
    "title": "AWS Comparison Tables",
    "section": "",
    "text": "Feature\nKinesis Data Streams\nAWS Firehose\nMSK (Managed Kafka)\n\n\n\n\nLatency\nReal-time (sub-second)\nNear real-time (60s minimum)\nReal-time (sub-second)\n\n\nScaling\nManual provisioning of shards\nAutomatic scaling\nManual provisioning of brokers\n\n\nData Retention\n24h by default, up to 365 days\nNo retention (immediate processing)\nConfigurable, unlimited\n\n\nUse Cases\nReal-time analytics, processing logs\nData loading into S3, Redshift, ES\nEvent streaming, log aggregation\n\n\nThroughput\nMB/s per shard\nAutomatic up to GB/s\nMB/s per broker\n\n\nCost Model\nPer shard hour\nPer GB processed\nPer broker hour"
  },
  {
    "objectID": "aws-comparison-tables.html#storage-services",
    "href": "aws-comparison-tables.html#storage-services",
    "title": "AWS Comparison Tables",
    "section": "Storage Services",
    "text": "Storage Services\n\n\n\n\n\n\n\n\n\n\nFeature\nS3\nEBS\nEFS\nFSx\n\n\n\n\nType\nObject Storage\nBlock Storage\nFile System\nFile System\n\n\nLatency\nms (varies)\nSub-ms\nms\nSub-ms to ms\n\n\nScalability\nUnlimited\nUp to 64TB per volume\nAutomatic\nAutomatic\n\n\nAccess Pattern\nWeb/API\nMount as drive\nNFS mount\nSMB/Lustre\n\n\nUse Cases\nStatic files, backups\nOS drives, databases\nShared files, web serving\nWindows/HPC workloads\n\n\nAvailability\n99.99%\n99.99%\n99.99%\n99.99%\n\n\nCost Model\nGB-month + requests\nGB-month provisioned\nGB-month used\nGB-month + throughput"
  },
  {
    "objectID": "aws-comparison-tables.html#aiml-services",
    "href": "aws-comparison-tables.html#aiml-services",
    "title": "AWS Comparison Tables",
    "section": "AI/ML Services",
    "text": "AI/ML Services\n\n\n\n\n\n\n\n\n\n\nFeature\nComprehend\nPersonalize\nBedrock\nSageMaker\n\n\n\n\nType\nManaged NLP\nManaged Recommendations\nFoundation Models\nML Platform\n\n\nTraining Required\nNo\nYes (with your data)\nNo\nYes\n\n\nCustomization\nLimited\nHigh\nModel fine-tuning\nComplete control\n\n\nLatency\n~100ms\n~100ms\nVaries by model\nDepends on deployment\n\n\nUse Cases\nText analysis\nRecommendations\nGen AI applications\nCustom ML models\n\n\nScaling\nAutomatic\nAutomatic\nAutomatic\nManual/Auto\n\n\nCost Model\nPer unit processed\nPer training hour + predictions\nPer token/request\nPer instance hour"
  },
  {
    "objectID": "aws-comparison-tables.html#container-services",
    "href": "aws-comparison-tables.html#container-services",
    "title": "AWS Comparison Tables",
    "section": "Container Services",
    "text": "Container Services\n\n\n\n\n\n\n\n\n\n\nFeature\nECS\nEKS\nApp Runner\nLambda Container\n\n\n\n\nComplexity\nLow\nHigh\nVery Low\nLow\n\n\nControl\nModerate\nHigh\nLow\nLow\n\n\nScaling\nAuto/Manual\nAuto/Manual\nAutomatic\nAutomatic\n\n\nMax Container Size\n30GB\n30GB\n10GB\n10GB\n\n\nCold Start\nNo\nNo\nYes\nYes\n\n\nUse Cases\nMicroservices\nComplex orchestration\nWeb apps\nServerless containers\n\n\nCost Model\nPer task hour\nPer cluster hour + tasks\nPer vCPU/GB\nPer request + duration"
  },
  {
    "objectID": "aws-comparison-tables.html#data-analytics-services",
    "href": "aws-comparison-tables.html#data-analytics-services",
    "title": "AWS Comparison Tables",
    "section": "Data Analytics Services",
    "text": "Data Analytics Services\n\n\n\n\n\n\n\n\n\n\nFeature\nEMR\nGlue\nAthena\nRedshift\n\n\n\n\nType\nManaged Hadoop\nETL Service\nQuery Service\nData Warehouse\n\n\nProcessing\nBatch/Stream\nBatch\nInteractive\nBatch/Interactive\n\n\nLatency\nMinutes\nMinutes\nSeconds\nSub-second to seconds\n\n\nScale\nManual clusters\nAutomatic\nAutomatic\nManual/Auto\n\n\nUse Cases\nBig data processing\nData preparation\nAd-hoc queries\nData warehousing\n\n\nSetup Time\nHours\nMinutes\nMinutes\nHours\n\n\nCost Model\nPer instance hour\nPer DPU hour\nPer TB scanned\nPer node hour"
  },
  {
    "objectID": "aws-comparison-tables.html#monitoring-services",
    "href": "aws-comparison-tables.html#monitoring-services",
    "title": "AWS Comparison Tables",
    "section": "Monitoring Services",
    "text": "Monitoring Services\n\n\n\n\n\n\n\n\n\n\nFeature\nCloudWatch\nX-Ray\nPrometheus\nGrafana\n\n\n\n\nType\nMetrics/Logs\nTracing\nMetrics\nVisualization\n\n\nData Retention\n15 months\n30 days\nConfigurable\nDepends on source\n\n\nGranularity\n1s to 1h\nPer request\nCustom\nCustom\n\n\nUse Cases\nBasic monitoring\nDistributed tracing\nContainer monitoring\nDashboarding\n\n\nIntegration\nAWS native\nAWS services\nKubernetes\nMultiple sources\n\n\nCost Model\nPer metric/GB\nPer trace\nStorage based\nBy features"
  },
  {
    "objectID": "aws-comparison-tables.html#ml-model-deployment-options",
    "href": "aws-comparison-tables.html#ml-model-deployment-options",
    "title": "AWS Comparison Tables",
    "section": "ML Model Deployment Options",
    "text": "ML Model Deployment Options\n\n\n\n\n\n\n\n\n\n\nFeature\nSageMaker Endpoints\nSageMaker Serverless\nLambda\nECS/EKS\n\n\n\n\nCold Start\nNo\nYes\nYes\nNo\n\n\nAuto-scaling\nYes\nYes\nYes\nYes\n\n\nMax Duration\nUnlimited\n15 min\n15 min\nUnlimited\n\n\nCost Model\nPer instance hour\nPer request\nPer request\nPer container\n\n\nUse Cases\nSteady traffic\nVariable traffic\nLightweight inference\nCustom deployment\n\n\nMax Model Size\nInstance dependent\n10GB\n10GB\nNo limit\n\n\nConcurrency\nHigh\nLimited\nLimited\nHigh"
  },
  {
    "objectID": "examples/datastreams.html",
    "href": "examples/datastreams.html",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "",
    "text": "This document compares AWS streaming solutions: Kinesis Data Streams, Kinesis Data Firehose, Amazon Managed Streaming for Apache Kafka (MSK), and Amazon Managed Workflows for Apache Airflow (AMAA). We’ll explore their use cases, differences, and provide code examples."
  },
  {
    "objectID": "examples/datastreams.html#kinesis-data-streams",
    "href": "examples/datastreams.html#kinesis-data-streams",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Kinesis Data Streams",
    "text": "Kinesis Data Streams\nAWS Kinesis Data Streams is a real-time streaming service for collecting, processing, and analyzing large volumes of data. It provides low-latency access and supports custom consumer applications.\n\nKey Features:\n\nReal-time data processing\nCustom applications using AWS SDKs\nRetention period up to 7 days\n\n\n\nSample Code: Writing to Kinesis Data Stream\nimport boto3\nimport json\n\ndef put_record_to_kinesis(stream_name, data):\n    kinesis_client = boto3.client('kinesis')\n    response = kinesis_client.put_record(\n        StreamName=stream_name,\n        Data=json.dumps(data),\n        PartitionKey=\"partition-1\"\n    )\n    return response\n\n# Example usage\nrecord = {\"event\": \"click\", \"user\": \"12345\"}\nput_record_to_kinesis(\"my-stream\", record)"
  },
  {
    "objectID": "examples/datastreams.html#kinesis-data-firehose",
    "href": "examples/datastreams.html#kinesis-data-firehose",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Kinesis Data Firehose",
    "text": "Kinesis Data Firehose\nAWS Kinesis Data Firehose is a managed service that loads streaming data into data lakes, warehouses, or analytics services. Unlike Data Streams, Firehose handles buffering and batch writing.\n\nKey Features:\n\nFully managed with automatic scaling\nSupports S3, Redshift, Elasticsearch, and Splunk as destinations\nNo custom consumer applications required\n\n\n\nSample Code: Writing to Firehose\nimport boto3\nimport json\n\ndef put_record_to_firehose(delivery_stream_name, data):\n    firehose_client = boto3.client('firehose')\n    response = firehose_client.put_record(\n        DeliveryStreamName=delivery_stream_name,\n        Record={\"Data\": json.dumps(data) + \"\\n\"}\n    )\n    return response\n\n# Example usage\nrecord = {\"event\": \"purchase\", \"amount\": 100}\nput_record_to_firehose(\"my-firehose-stream\", record)"
  },
  {
    "objectID": "examples/datastreams.html#amazon-managed-streaming-for-apache-kafka-msk",
    "href": "examples/datastreams.html#amazon-managed-streaming-for-apache-kafka-msk",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Amazon Managed Streaming for Apache Kafka (MSK)",
    "text": "Amazon Managed Streaming for Apache Kafka (MSK)\nMSK provides a fully managed Kafka cluster that integrates with AWS services. It’s ideal for applications needing open-source Kafka features.\n\nKey Features:\n\nFully managed Kafka clusters\nSecure with IAM authentication\nSupports standard Kafka clients\n\n\n\nSample Code: Producing Messages to Kafka\nfrom kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers=['b-1.msk-cluster.amazonaws.com:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\nproducer.send(\"my-topic\", {\"event\": \"login\", \"user\": \"user_123\"})\nproducer.flush()"
  },
  {
    "objectID": "examples/datastreams.html#amazon-managed-workflows-for-apache-airflow-amaa",
    "href": "examples/datastreams.html#amazon-managed-workflows-for-apache-airflow-amaa",
    "title": "Streaming: Kinesis Data Streams vs Data Firehose vs Managed Service Kafka vs AMAA",
    "section": "Amazon Managed Workflows for Apache Airflow (AMAA)",
    "text": "Amazon Managed Workflows for Apache Airflow (AMAA)\nAMAA is a managed workflow orchestration service based on Apache Airflow. Unlike real-time streaming services, it is used for data pipeline scheduling and orchestration.\n\nKey Features:\n\nFully managed Airflow environment\nDAG-based workflow scheduling\nIntegrates with AWS services\n\n\n\nSample Code: Defining an Airflow DAG in AMAA\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef my_task():\n    print(\"Running my task\")\n\ndefault_args = {\"start_date\": datetime(2024, 2, 1)}\n\ndag = DAG(\"my_dag\", default_args=default_args, schedule_interval=\"@daily\")\n\nstart = DummyOperator(task_id=\"start\", dag=dag)\ntask = PythonOperator(task_id=\"run_task\", python_callable=my_task, dag=dag)\n\nstart &gt;&gt; task"
  },
  {
    "objectID": "examples/search.html",
    "href": "examples/search.html",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "",
    "text": "AWS provides two primary search services: Amazon Kendra and Amazon OpenSearch Service. While both enable search capabilities, they serve different use cases and have distinct strengths."
  },
  {
    "objectID": "examples/search.html#overview-table",
    "href": "examples/search.html#overview-table",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Overview Table",
    "text": "Overview Table\n\n\n\n\n\n\n\n\nFeature\nAmazon Kendra\nAmazon OpenSearch Service\n\n\n\n\nType\nAI-powered enterprise search\nOpen-source search and analytics engine\n\n\nUse Case\nDocument and knowledge search\nLog analytics, full-text search, observability\n\n\nData Sources\nPre-built connectors for common enterprise sources\nStructured and unstructured data from various sources\n\n\nQuery Type\nNatural Language Processing (NLP)-based\nKeyword-based and structured queries (Elasticsearch DSL)\n\n\nIndexing\nManaged, AI-driven indexing\nFull control over indexing with custom mappings\n\n\nAccess Model\nAPI-based search queries\nREST API, Kibana dashboards, SQL\n\n\nScaling\nFully managed, auto-scaling\nCluster-based, requires management"
  },
  {
    "objectID": "examples/search.html#amazon-kendra",
    "href": "examples/search.html#amazon-kendra",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Amazon Kendra",
    "text": "Amazon Kendra\nDefinition: Amazon Kendra is an AI-powered enterprise search service that enables organizations to search structured and unstructured documents using natural language queries.\nCommon Use Cases: - Enterprise knowledge management - Internal document search - Customer support knowledge bases\n\nExample Code: Querying Kendra with Python (Boto3)\nimport boto3\n\nkendra = boto3.client('kendra')\nresponse = kendra.query(\n    IndexId='index-id',\n    QueryText='How do I reset my password?'\n)\nprint(response)"
  },
  {
    "objectID": "examples/search.html#amazon-opensearch-service",
    "href": "examples/search.html#amazon-opensearch-service",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Amazon OpenSearch Service",
    "text": "Amazon OpenSearch Service\nDefinition: Amazon OpenSearch Service is a managed search and analytics engine based on the open-source Elasticsearch and OpenSearch projects.\nCommon Use Cases: - Log and event data analysis - Real-time application search - Security monitoring and observability\n\nExample Code: Querying OpenSearch with Python\nfrom opensearchpy import OpenSearch\n\nclient = OpenSearch(\n    hosts=[{'host': 'your-opensearch-domain', 'port': 443}],\n    http_auth=('user', 'password'),\n    use_ssl=True\n)\n\nquery = {\n    \"query\": {\n        \"match\": {\"message\": \"error\"}\n    }\n}\nresponse = client.search(index=\"logs\", body=query)\nprint(response)"
  },
  {
    "objectID": "examples/search.html#conclusion",
    "href": "examples/search.html#conclusion",
    "title": "Search Services: Kendra vs OpenSearch",
    "section": "Conclusion",
    "text": "Conclusion\nIf your use case involves enterprise knowledge retrieval with AI-driven natural language search, Amazon Kendra is the better option. If you need log analytics, real-time search, or structured query capabilities, OpenSearch is the preferred choice."
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html",
    "href": "examples/emr-vs-hadoop-vs-spark.html",
    "title": "EMR vs Hadoop vs Spark",
    "section": "",
    "text": "This document compares Amazon EMR, Apache Hadoop, and Apache Spark, highlighting their use cases, differences, and providing code examples."
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#amazon-emr",
    "href": "examples/emr-vs-hadoop-vs-spark.html#amazon-emr",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Amazon EMR",
    "text": "Amazon EMR\nAWS Elastic MapReduce (EMR) is a cloud-based big data processing service that simplifies running frameworks like Hadoop and Spark.\n\nKey Features:\n\nManaged Hadoop, Spark, Presto, and more\nAuto-scaling and cost-efficient\nTight integration with AWS services\n\n\n\nSample Code: Submitting a Spark Job to EMR\nimport boto3\n\ndef submit_spark_job(cluster_id, script_s3_path):\n    emr_client = boto3.client('emr')\n    response = emr_client.add_job_flow_steps(\n        JobFlowId=cluster_id,\n        Steps=[{\n            'Name': 'Spark Job',\n            'ActionOnFailure': 'CONTINUE',\n            'HadoopJarStep': {\n                'Jar': 'command-runner.jar',\n                'Args': ['spark-submit', script_s3_path]\n            }\n        }]\n    )\n    return response\n\n# Example usage\nsubmit_spark_job(\"j-XYZ123\", \"s3://my-bucket/my-spark-job.py\")\nOpen source alternatives like Apache Hadoop and Apache Spark offer similar capabilities but require more manual setup and management. What is required is a cluster setup, configuration, and monitoring."
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#apache-hadoop",
    "href": "examples/emr-vs-hadoop-vs-spark.html#apache-hadoop",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Apache Hadoop",
    "text": "Apache Hadoop\nHadoop is an open-source framework for distributed storage and processing of large datasets using MapReduce.\n\nKey Features:\n\nDistributed computing using HDFS and YARN\nBatch processing of large data volumes\nEcosystem includes Hive, Pig, HBase, and more\n\n\n\nSample Code: Hadoop Word Count Job\nimport java.io.IOException;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.*;\n\npublic class WordCount {\n    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {\n        private final static IntWritable one = new IntWritable(1);\n        private Text word = new Text();\n\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            for (String token : value.toString().split(\" \")) {\n                word.set(token);\n                context.write(word, one);\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "examples/emr-vs-hadoop-vs-spark.html#apache-spark",
    "href": "examples/emr-vs-hadoop-vs-spark.html#apache-spark",
    "title": "EMR vs Hadoop vs Spark",
    "section": "Apache Spark",
    "text": "Apache Spark\nSpark is a fast, in-memory data processing framework designed for large-scale data analytics and machine learning.\n\nKey Features\n\nIn-memory processing for speed\nSupports batch, streaming, and machine learning workloads\nAPIs in Python, Java, Scala, and R\n\n\n\nSample Code: Running a PySpark Job\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Example\").getOrCreate()\ndata = [(\"Alice\", 34), (\"Bob\", 45)]\ndf = spark.createDataFrame(data, [\"Name\", \"Age\"])\ndf.show()\nNote, AWS Spark is not an equivalent of Amazon EMR. Apache Spark is an open-source, distributed computing framework for big data processing, while Amazon EMR (Elastic MapReduce) is a fully managed cloud service that simplifies running big data frameworks like Spark, Hadoop, and others on AWS.\nIn other words, Spark is one of the software frameworks that can be run on Amazon EMR, but EMR itself offers a lot more than just Spark. It manages the cluster infrastructure, scaling, and integration with AWS services, while Apache Spark is just the processing engine for big data tasks. The open source tool to manage cluster infrastructure is Apache Hadoop, which includes HDFS and YARN."
  },
  {
    "objectID": "examples/storage.html",
    "href": "examples/storage.html",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "",
    "text": "AWS offers multiple storage options, each optimized for different use cases. This document compares Amazon S3, EFS, FSx, and EBS based on features, performance, and use cases."
  },
  {
    "objectID": "examples/storage.html#amazon-s3-simple-storage-service",
    "href": "examples/storage.html#amazon-s3-simple-storage-service",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon S3 (Simple Storage Service)",
    "text": "Amazon S3 (Simple Storage Service)\n\nUse Case: Backup, Data Lakes, Static Websites\nFeatures:\n\nScalable, durable object storage\nMultiple storage classes (Standard, Intelligent-Tiering, Glacier)\nServer-side encryption and lifecycle management\n\n\n\nExample: Upload a File to S3 Using Boto3\nimport boto3\ns3 = boto3.client('s3')\ns3.upload_file('local_file.txt', 'my-bucket', 'uploaded_file.txt')"
  },
  {
    "objectID": "examples/storage.html#amazon-efs-elastic-file-system",
    "href": "examples/storage.html#amazon-efs-elastic-file-system",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EFS (Elastic File System)",
    "text": "Amazon EFS (Elastic File System)\n\nUse Case: Multi-instance shared storage, Kubernetes, Serverless\nFeatures:\n\nPOSIX-compliant, automatically scales\nSupports NFS protocol\nMountable across multiple EC2 instances\n\n\n\nExample: Mount EFS on an EC2 Instance\nsudo mount -t nfs4 -o nfsvers=4.1 fs-0123456789abcdef0.efs.us-east-1.amazonaws.com:/ efs-mount-point"
  },
  {
    "objectID": "examples/storage.html#amazon-fsx",
    "href": "examples/storage.html#amazon-fsx",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon FSx",
    "text": "Amazon FSx\n\nUse Case: High-performance file storage for Windows/Linux applications\nFeatures:\n\nOptimized for Windows File Server or Lustre (HPC)\nFully managed with built-in backups\n\n\n\nExample: Mount FSx for Windows on EC2\nNew-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\fs-0123456789abcdef0.example.com\\share\""
  },
  {
    "objectID": "examples/storage.html#amazon-ebs-elastic-block-store",
    "href": "examples/storage.html#amazon-ebs-elastic-block-store",
    "title": "Storage: S3 vs EFS vs FSx vs EBS",
    "section": "Amazon EBS (Elastic Block Store)",
    "text": "Amazon EBS (Elastic Block Store)\n\nUse Case: Low-latency storage for EC2, Databases, Big Data\nFeatures:\n\nPersistent block storage for EC2\nSSD (gp3, io2) and HDD (sc1, st1) options\nSnapshots for backup & recovery\n\n\n\nExample: Attach an EBS Volume to EC2\naws ec2 attach-volume --volume-id vol-0123456789abcdef0 --instance-id i-0123456789abcdef0 --device /dev/xvdf"
  },
  {
    "objectID": "examples/glue.html",
    "href": "examples/glue.html",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "",
    "text": "This document compares AWS Glue Data Catalog, AWS Glue DataBrew, and AWS Glue Studio, highlighting their use cases, differences, and providing code examples."
  },
  {
    "objectID": "examples/glue.html#aws-glue-data-catalog",
    "href": "examples/glue.html#aws-glue-data-catalog",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue Data Catalog",
    "text": "AWS Glue Data Catalog\nAWS Glue Data Catalog is a metadata repository for organizing and discovering datasets across AWS services.\n\nKey Features:\n\nCentralized metadata management\nSchema discovery and versioning\nIntegration with Athena, Redshift, and EMR\n\n\n\nSample Code: Creating a Glue Database\nimport boto3\n\ndef create_glue_database(database_name):\n    glue_client = boto3.client('glue')\n    response = glue_client.create_database(\n        DatabaseInput={\n            'Name': database_name,\n            'Description': 'Example Glue Database'\n        }\n    )\n    return response\n\n# Example usage\ncreate_glue_database(\"my_glue_db\")"
  },
  {
    "objectID": "examples/glue.html#aws-glue-databrew",
    "href": "examples/glue.html#aws-glue-databrew",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue DataBrew",
    "text": "AWS Glue DataBrew\nAWS Glue DataBrew is a visual data preparation tool for cleaning and transforming data without coding.\n\nKey Features:\n\nNo-code data transformation\nIntegration with S3, Redshift, and RDS\nAutomated data profiling and anomaly detection\n\n\n\nSample Code: Creating a DataBrew Dataset\nimport boto3\n\ndef create_databrew_dataset(name, s3_path, role_arn):\n    databrew_client = boto3.client('databrew')\n    response = databrew_client.create_dataset(\n        Name=name,\n        Input={\n            'S3InputDefinition': {'Bucket': s3_path}\n        },\n        RoleArn=role_arn\n    )\n    return response\n\n# Example usage\ncreate_databrew_dataset(\"my_dataset\", \"my-s3-bucket\", \"arn:aws:iam::123456789012:role/my-role\")"
  },
  {
    "objectID": "examples/glue.html#aws-glue-studio",
    "href": "examples/glue.html#aws-glue-studio",
    "title": "Glue Data Catalog vs Glue DataBrew vs Glue Studio",
    "section": "AWS Glue Studio",
    "text": "AWS Glue Studio\nAWS Glue Studio provides a graphical interface to visually build, run, and monitor AWS Glue ETL jobs.\n\nKey Features:\n\nDrag-and-drop interface for ETL workflows\nSupports Spark-based transformations\nIntegration with Glue Data Catalog\n\n\n\nSample Code: Creating a Glue Job\nimport boto3\n\ndef create_glue_job(job_name, script_s3_path, role_arn):\n    glue_client = boto3.client('glue')\n    response = glue_client.create_job(\n        Name=job_name,\n        Role=role_arn,\n        Command={\n            'Name': 'glueetl',\n            'ScriptLocation': script_s3_path\n        },\n        GlueVersion='2.0'\n    )\n    return response\n\n# Example usage\ncreate_glue_job(\"my_glue_job\", \"s3://my-bucket/scripts/job.py\", \"arn:aws:iam::123456789012:role/my-role\")"
  },
  {
    "objectID": "practice/practice-questions-4.html",
    "href": "practice/practice-questions-4.html",
    "title": "AWS ML Quiz",
    "section": "",
    "text": "This document contains multiple-choice questions related to AWS Machine Learning services. Click the Show Answer button to reveal the correct answer along with an explanation.\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Fine-tuning the model\nFine-tuning is resource-intensive and not ideal for frequent updates.\n\n\nB) Updating the vector database\n✅ Correct. Updating the vector database is faster and more efficient than fine-tuning for adding new information.\n\n\nC) Retraining the entire model\nRetraining is costly and time-consuming.\n\n\n\n\n\nShow Answer\n\nB) Updating the vector database\nUpdating the vector database allows new information to be added quickly without retraining the model.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Embeddings\nEmbeddings help with similarity search but don’t guide tool selection.\n\n\nB) Action Groups\n✅ Correct. Action Groups define available tools and guide when to use each.\n\n\nC) Model Fine-Tuning\nFine-tuning customizes model behavior but doesn’t handle tool selection dynamically.\n\n\n\n\n\nShow Answer\n\nB) Action Groups\nAction Groups in Bedrock allow the model to decide which tool to use based on the user query.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) High Availability\nServerless automatically scales to demand.\n\n\nB) No Infrastructure Management\n✅ Correct. Serverless eliminates the need to manage infrastructure.\n\n\nC) Lower Cost for Large-Scale Workloads\nServerless can be cost-efficient but may not always be the cheapest for high usage.\n\n\n\n\n\nShow Answer\n\nB) No Infrastructure Management\nServerless environments scale automatically and require no manual infrastructure management.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Model Accuracy\nAccuracy measures overall performance, not feature drift.\n\n\nB) Feature Attribution Scores\n✅ Correct. Feature attribution drift is detected by monitoring these scores.\n\n\nC) Training Loss\nTraining loss relates to the training process, not drift.\n\n\n\n\n\nShow Answer\n\nB) Feature Attribution Scores\nSageMaker Model Monitor detects changes in feature attributions over time.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) AWS Step Functions\nStep Functions can orchestrate workflows but aren’t specialized for ML.\n\n\nB) SageMaker Pipelines\n✅ Correct. SageMaker Pipelines orchestrate ML workflows end-to-end.\n\n\nC) Lambda Functions\nLambda executes code but isn’t designed for full ML pipelines.\n\n\n\n\n\nShow Answer\n\nB) SageMaker Pipelines\nSageMaker Pipelines help automate ML workflows, including training and deployment."
  },
  {
    "objectID": "practice/practice-questions-4.html#aws-machine-learning-quiz",
    "href": "practice/practice-questions-4.html#aws-machine-learning-quiz",
    "title": "AWS ML Quiz",
    "section": "",
    "text": "This document contains multiple-choice questions related to AWS Machine Learning services. Click the Show Answer button to reveal the correct answer along with an explanation.\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Fine-tuning the model\nFine-tuning is resource-intensive and not ideal for frequent updates.\n\n\nB) Updating the vector database\n✅ Correct. Updating the vector database is faster and more efficient than fine-tuning for adding new information.\n\n\nC) Retraining the entire model\nRetraining is costly and time-consuming.\n\n\n\n\n\nShow Answer\n\nB) Updating the vector database\nUpdating the vector database allows new information to be added quickly without retraining the model.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Embeddings\nEmbeddings help with similarity search but don’t guide tool selection.\n\n\nB) Action Groups\n✅ Correct. Action Groups define available tools and guide when to use each.\n\n\nC) Model Fine-Tuning\nFine-tuning customizes model behavior but doesn’t handle tool selection dynamically.\n\n\n\n\n\nShow Answer\n\nB) Action Groups\nAction Groups in Bedrock allow the model to decide which tool to use based on the user query.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) High Availability\nServerless automatically scales to demand.\n\n\nB) No Infrastructure Management\n✅ Correct. Serverless eliminates the need to manage infrastructure.\n\n\nC) Lower Cost for Large-Scale Workloads\nServerless can be cost-efficient but may not always be the cheapest for high usage.\n\n\n\n\n\nShow Answer\n\nB) No Infrastructure Management\nServerless environments scale automatically and require no manual infrastructure management.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) Model Accuracy\nAccuracy measures overall performance, not feature drift.\n\n\nB) Feature Attribution Scores\n✅ Correct. Feature attribution drift is detected by monitoring these scores.\n\n\nC) Training Loss\nTraining loss relates to the training process, not drift.\n\n\n\n\n\nShow Answer\n\nB) Feature Attribution Scores\nSageMaker Model Monitor detects changes in feature attributions over time.\n\n\n\n\n\n\n\n\n\n\n\nOption\nExplanation\n\n\n\n\nA) AWS Step Functions\nStep Functions can orchestrate workflows but aren’t specialized for ML.\n\n\nB) SageMaker Pipelines\n✅ Correct. SageMaker Pipelines orchestrate ML workflows end-to-end.\n\n\nC) Lambda Functions\nLambda executes code but isn’t designed for full ML pipelines.\n\n\n\n\n\nShow Answer\n\nB) SageMaker Pipelines\nSageMaker Pipelines help automate ML workflows, including training and deployment."
  },
  {
    "objectID": "practice/practice-questions-3.html",
    "href": "practice/practice-questions-3.html",
    "title": "AWS Machine Learning Quiz",
    "section": "",
    "text": "You’re working with a sparse dataset and need to predict user-item interactions. Which SageMaker algorithm is most appropriate?\n\n\n\nIn SageMaker’s XGBoost, which hyperparameter helps prevent overfitting by controlling the step size shrinkage?\nAnswer: The eta hyperparameter in XGBoost controls the step size shrinkage, which helps prevent overfitting by reducing the impact of individual trees.\n\n\n\nYou are tasked with training a SageMaker model to predict future values of a time series. Which algorithm should you choose?\nAnswer: DeepAR is designed for forecasting future values in time series data.\n\n\n\nWhich input format is optimal for training a model using SageMaker’s K-Means algorithm?\nAnswer: SageMaker’s K-Means algorithm typically expects input data in the RecordIO-Protobuf format, though CSV is also supported.\n\n\n\nWhen using SageMaker’s Object2Vec algorithm, what is the primary output?\nAnswer: Object2Vec produces low-dimensional dense embeddings of high-dimensional objects, useful for tasks like similarity searches and recommendation systems.\n\n\n\nWhen using a Transformer model for text classification, which component is responsible for capturing the relationships between different tokens in a sentence?\nAnswer: Self-Attention captures the relationships between tokens by considering the entire sentence context.\n\n\n\nYou are fine-tuning a pre-trained GPT model for a customer service chatbot. Which training technique is most appropriate if you want to adapt the model with minimal training data?\nAnswer: Freezing the initial layers and training the last few layers is a common fine-tuning approach that requires less data.\n\n\n\nWhich AWS service would you use to quickly deploy a pre-trained model like GPT-J for text generation tasks?\nAnswer: Amazon SageMaker JumpStart allows easy deployment of pre-trained models like GPT-J.\n\n\n\nIn a sequence-to-sequence transformer model used for translation, what is the role of the decoder?\nAnswer: The decoder generates the target sequence based on the encoded information from the source sequence.\n\n\n\nWhen using masked self-attention in GPT models, what is the purpose of the mask?\nAnswer: The mask ensures that each token can only attend to past tokens, not future ones.\n\n\n\nYou are using AWS SageMaker JumpStart to deploy a model for text generation. Which model type would you choose for multilingual text generation?\nAnswer: Jurassic-2 is noted for its multilingual capabilities.\n\n\n\nDuring the training of a transformer model, which aspect allows the model to consider the position of each token within the sequence?\nAnswer: Positional encoding adds information about token positions.\n\n\n\nYou are tasked with summarizing large volumes of text data. Which model would you select from AWS’s offerings?\nAnswer: Amazon Titan is optimized for tasks like text summarization.\n\n\n\nIn the context of self-attention, which matrices are used to compute the attention scores for each token?\nAnswer: Attention scores are computed by the dot product of Query and Key matrices.\n\n\n\nWhat is a key advantage of using self-attention in transformer models over traditional RNNs?\nAnswer: Self-attention allows parallel processing, unlike RNNs.\n\n\n\nYou are developing a customer support chatbot using Amazon Bedrock. The chatbot needs to respond accurately to specific customer inquiries using recent company data. Which approach would be most effective for this scenario?\nAnswer: RAG (Retrieval-Augmented Generation) is ideal for incorporating recent or proprietary information that the foundational model wasn’t originally trained on, making it well-suited for a chatbot needing up-to-date responses.\n\n\n\nYou are using Amazon Bedrock to build an AI system for generating marketing content. The system must ensure the content generated adheres to brand guidelines and avoids inappropriate language. Which feature of Amazon Bedrock would best serve this purpose?\nAnswer: By using separate action groups, the LLM can be guided on when to use the knowledge base and when to call the API, making the system more flexible and maintainable."
  },
  {
    "objectID": "practice/practice-questions-3.html#aws-ml-quiz-questions",
    "href": "practice/practice-questions-3.html#aws-ml-quiz-questions",
    "title": "AWS Machine Learning Quiz",
    "section": "",
    "text": "You’re working with a sparse dataset and need to predict user-item interactions. Which SageMaker algorithm is most appropriate?\n\n\n\nIn SageMaker’s XGBoost, which hyperparameter helps prevent overfitting by controlling the step size shrinkage?\nAnswer: The eta hyperparameter in XGBoost controls the step size shrinkage, which helps prevent overfitting by reducing the impact of individual trees.\n\n\n\nYou are tasked with training a SageMaker model to predict future values of a time series. Which algorithm should you choose?\nAnswer: DeepAR is designed for forecasting future values in time series data.\n\n\n\nWhich input format is optimal for training a model using SageMaker’s K-Means algorithm?\nAnswer: SageMaker’s K-Means algorithm typically expects input data in the RecordIO-Protobuf format, though CSV is also supported.\n\n\n\nWhen using SageMaker’s Object2Vec algorithm, what is the primary output?\nAnswer: Object2Vec produces low-dimensional dense embeddings of high-dimensional objects, useful for tasks like similarity searches and recommendation systems.\n\n\n\nWhen using a Transformer model for text classification, which component is responsible for capturing the relationships between different tokens in a sentence?\nAnswer: Self-Attention captures the relationships between tokens by considering the entire sentence context.\n\n\n\nYou are fine-tuning a pre-trained GPT model for a customer service chatbot. Which training technique is most appropriate if you want to adapt the model with minimal training data?\nAnswer: Freezing the initial layers and training the last few layers is a common fine-tuning approach that requires less data.\n\n\n\nWhich AWS service would you use to quickly deploy a pre-trained model like GPT-J for text generation tasks?\nAnswer: Amazon SageMaker JumpStart allows easy deployment of pre-trained models like GPT-J.\n\n\n\nIn a sequence-to-sequence transformer model used for translation, what is the role of the decoder?\nAnswer: The decoder generates the target sequence based on the encoded information from the source sequence.\n\n\n\nWhen using masked self-attention in GPT models, what is the purpose of the mask?\nAnswer: The mask ensures that each token can only attend to past tokens, not future ones.\n\n\n\nYou are using AWS SageMaker JumpStart to deploy a model for text generation. Which model type would you choose for multilingual text generation?\nAnswer: Jurassic-2 is noted for its multilingual capabilities.\n\n\n\nDuring the training of a transformer model, which aspect allows the model to consider the position of each token within the sequence?\nAnswer: Positional encoding adds information about token positions.\n\n\n\nYou are tasked with summarizing large volumes of text data. Which model would you select from AWS’s offerings?\nAnswer: Amazon Titan is optimized for tasks like text summarization.\n\n\n\nIn the context of self-attention, which matrices are used to compute the attention scores for each token?\nAnswer: Attention scores are computed by the dot product of Query and Key matrices.\n\n\n\nWhat is a key advantage of using self-attention in transformer models over traditional RNNs?\nAnswer: Self-attention allows parallel processing, unlike RNNs.\n\n\n\nYou are developing a customer support chatbot using Amazon Bedrock. The chatbot needs to respond accurately to specific customer inquiries using recent company data. Which approach would be most effective for this scenario?\nAnswer: RAG (Retrieval-Augmented Generation) is ideal for incorporating recent or proprietary information that the foundational model wasn’t originally trained on, making it well-suited for a chatbot needing up-to-date responses.\n\n\n\nYou are using Amazon Bedrock to build an AI system for generating marketing content. The system must ensure the content generated adheres to brand guidelines and avoids inappropriate language. Which feature of Amazon Bedrock would best serve this purpose?\nAnswer: By using separate action groups, the LLM can be guided on when to use the knowledge base and when to call the API, making the system more flexible and maintainable."
  },
  {
    "objectID": "practice/practice-questions-1.html",
    "href": "practice/practice-questions-1.html",
    "title": "AWS Data & Machine Learning Solutions",
    "section": "",
    "text": "Service\nDescription\n\n\n\n\nAmazon S3\nCost-effective, durable storage for semi-structured data, supporting multiple retrieval options.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage Class\nDescription\n\n\n\n\nS3 Intelligent-Tiering\nAutomatically moves data between access tiers to optimize costs while ensuring low-latency access.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Kinesis\nEnables real-time data streaming and analytics for IoT and big data applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nDescription\n\n\n\n\nAWS Lake Formation\nProvides governance, security, and access control for data lakes on AWS.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon EMR\nManaged Hadoop and Spark cluster for scalable data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Fraud Detector\nUses machine learning to identify potentially fraudulent activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nSageMaker Clarify\nIdentifies and mitigates bias in datasets and models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nSageMaker Edge Manager\nOptimizes, deploys, and manages ML models on edge devices.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAWS Glue\nServerless ETL service that schedules and manages data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\nAmazon EMR\nOptimized for large-scale distributed computing with Hadoop and Spark.\n\n\nAWS Glue\nManaged ETL service with data cataloging and job scheduling.\n\n\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nMedian Imputation\nRobust to outliers compared to mean imputation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nSMOTE (Synthetic Minority Over-sampling Technique)\nGenerates synthetic samples to improve class balance.\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Transcribe\nAutomatic speech-to-text transcription service.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Comprehend\nNLP service for sentiment analysis and entity recognition.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Fraud Detector\nMachine learning-based fraud detection service.\n\n\n\n\n\n\n\n\n\n\n\n\n\nServices\nDescription\n\n\n\n\nAmazon Transcribe + Amazon Translate\nTranscribe speech and translate into multiple languages.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Rekognition\nComputer vision service for real-time object detection."
  },
  {
    "objectID": "practice/practice-questions-1.html#questions-and-answers",
    "href": "practice/practice-questions-1.html#questions-and-answers",
    "title": "AWS Data & Machine Learning Solutions",
    "section": "",
    "text": "Service\nDescription\n\n\n\n\nAmazon S3\nCost-effective, durable storage for semi-structured data, supporting multiple retrieval options.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage Class\nDescription\n\n\n\n\nS3 Intelligent-Tiering\nAutomatically moves data between access tiers to optimize costs while ensuring low-latency access.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Kinesis\nEnables real-time data streaming and analytics for IoT and big data applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nDescription\n\n\n\n\nAWS Lake Formation\nProvides governance, security, and access control for data lakes on AWS.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon EMR\nManaged Hadoop and Spark cluster for scalable data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Fraud Detector\nUses machine learning to identify potentially fraudulent activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nSageMaker Clarify\nIdentifies and mitigates bias in datasets and models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nSageMaker Edge Manager\nOptimizes, deploys, and manages ML models on edge devices.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAWS Glue\nServerless ETL service that schedules and manages data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\nAmazon EMR\nOptimized for large-scale distributed computing with Hadoop and Spark.\n\n\nAWS Glue\nManaged ETL service with data cataloging and job scheduling.\n\n\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nMedian Imputation\nRobust to outliers compared to mean imputation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nSMOTE (Synthetic Minority Over-sampling Technique)\nGenerates synthetic samples to improve class balance.\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Transcribe\nAutomatic speech-to-text transcription service.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Comprehend\nNLP service for sentiment analysis and entity recognition.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Fraud Detector\nMachine learning-based fraud detection service.\n\n\n\n\n\n\n\n\n\n\n\n\n\nServices\nDescription\n\n\n\n\nAmazon Transcribe + Amazon Translate\nTranscribe speech and translate into multiple languages.\n\n\n\n\n\n\n\n\n\n\n\n\n\nService\nDescription\n\n\n\n\nAmazon Rekognition\nComputer vision service for real-time object detection."
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides powerful tools for training, fine-tuning, and managing machine learning models. In this post, we explore:\n\nUsing SageMaker script mode with TensorFlow and PyTorch.\nFine-tuning pre-trained models with custom datasets using Amazon Bedrock and SageMaker JumpStart.\nPerforming hyperparameter tuning with SageMaker Automatic Model Tuning (AMT).\nManaging model versions with the SageMaker Model Registry.\nGaining insights with SageMaker Clarify.\n\n\n\nSageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nPre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})\n\n\n\n\nSageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})\n\n\n\n\nThe SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")\n\n\n\n\nSageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")\n\n\n\n\nAWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-script-mode-for-training",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-script-mode-for-training",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker script mode enables training models using popular frameworks while maintaining flexibility and scalability.\n\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nrole = \"arn:aws:iam::123456789012:role/MySageMakerRole\"\nsession = sagemaker.Session()\n\npytorch_estimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"src\",\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.8\",\n    py_version=\"py3\",\n    sagemaker_session=session\n)\n\npytorch_estimator.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#fine-tuning-pre-trained-models-with-custom-datasets",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "Pre-trained models can be fine-tuned using Amazon Bedrock or SageMaker JumpStart for domain-specific applications.\n\n\nfrom sagemaker.jumpstart.estimator import JumpStartEstimator\n\nmodel_id = \"huggingface-text-classification\"\n\ntuner = JumpStartEstimator(\n    model_id=model_id,\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session,\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/custom-dataset/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#performing-hyperparameter-tuning-with-sagemaker-amt",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Automatic Model Tuning (AMT) optimizes hyperparameters for improved model performance.\n\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\nfrom sagemaker.xgboost import XGBoost\n\nxgb = XGBoost(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    framework_version=\"1.3-1\",\n    output_path=\"s3://my-bucket/xgboost-model/\",\n    sagemaker_session=session,\n)\n\ntuner = HyperparameterTuner(\n    xgb,\n    objective_metric_name=\"validation:rmse\",\n    hyperparameter_ranges={\n        \"max_depth\": IntegerParameter(3, 10),\n        \"eta\": ContinuousParameter(0.1, 0.5)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntuner.fit({\"train\": \"s3://my-bucket/train-data/\"})"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#managing-model-versions-with-sagemaker-model-registry",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#managing-model-versions-with-sagemaker-model-registry",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "The SageMaker Model Registry tracks and manages different versions of ML models for auditability and reproducibility.\n\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\")\n\nresponse = sm_client.create_model_package(\n    ModelPackageGroupName=\"MyModelPackageGroup\",\n    ModelPackageDescription=\"Version 1 of my model\",\n    InferenceSpecification={\n        \"Containers\": [\n            {\n                \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-model:latest\",\n                \"Mode\": \"SingleModel\"\n            }\n        ],\n        \"SupportedContentTypes\": [\"text/csv\"],\n        \"SupportedResponseMIMETypes\": [\"application/json\"]\n    },\n)\nprint(\"Model registered successfully\")"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-clarify-for-model-insights",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#using-sagemaker-clarify-for-model-insights",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "SageMaker Clarify provides metrics to analyze ML training data and models.\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nData Bias\nMeasures imbalances in dataset representation.\n\n\nModel Bias\nEvaluates if the model favors specific groups.\n\n\nFeature Importance\nDetermines which features contribute most to predictions.\n\n\n\n\n\nfrom sagemaker.clarify import ClarifyBiasConfig, ClarifyProcessor\n\nclarify_processor = ClarifyProcessor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    sagemaker_session=session\n)\n\nbias_config = ClarifyBiasConfig(\n    label=\"target\",\n    facet=\"gender\",\n    dataset_format={\"headers\": True, \"dataset_type\": \"csv\"}\n)\n\nclarify_processor.run_pre_training_bias(\n    data_config=bias_config,\n    data_file=\"s3://my-bucket/dataset.csv\"\n)\nprint(\"Bias analysis completed\")"
  },
  {
    "objectID": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#conclusion",
    "href": "sagemaker/sagemaker-multi-model-multi-container-deployments.html#conclusion",
    "title": "Advanced Machine Learning with AWS SageMaker",
    "section": "",
    "text": "AWS SageMaker provides comprehensive tools for training, fine-tuning, hyperparameter tuning, model versioning, and bias analysis. These features streamline ML workflows, ensuring robust and auditable models."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "",
    "text": "Amazon SageMaker provides powerful tools for deploying and monitoring machine learning models in a secure and scalable way. This blog post covers:\n\nConfiguring SageMaker endpoints within a VPC network.\nDeploying and hosting models using the SageMaker SDK.\nMonitoring models in production using SageMaker Model Monitor.\nDetecting changes in data distribution with SageMaker Clarify."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#introduction",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#introduction",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "",
    "text": "Amazon SageMaker provides powerful tools for deploying and monitoring machine learning models in a secure and scalable way. This blog post covers:\n\nConfiguring SageMaker endpoints within a VPC network.\nDeploying and hosting models using the SageMaker SDK.\nMonitoring models in production using SageMaker Model Monitor.\nDetecting changes in data distribution with SageMaker Clarify."
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#configuring-sagemaker-endpoints-within-a-vpc",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#configuring-sagemaker-endpoints-within-a-vpc",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Configuring SageMaker Endpoints within a VPC",
    "text": "Configuring SageMaker Endpoints within a VPC\nDeploying SageMaker endpoints within an Amazon Virtual Private Cloud (VPC) enhances security by preventing data from being exposed to the internet.\n\nSteps to Configure a VPC-Enabled Endpoint:\n\nCreate a VPC with private and public subnets.\nSet up security groups for inbound and outbound rules.\nDeploy SageMaker endpoints inside the VPC.\n\nimport boto3\nsagemaker = boto3.client('sagemaker')\n\nresponse = sagemaker.create_endpoint_config(\n    EndpointConfigName='my-vpc-endpoint-config',\n    ProductionVariants=[\n        {\n            'VariantName': 'AllTraffic',\n            'ModelName': 'my-sagemaker-model',\n            'InstanceType': 'ml.m5.large',\n            'InitialInstanceCount': 1\n        }\n    ],\n    VpcConfig={\n        'Subnets': ['subnet-xxxxxx'],\n        'SecurityGroupIds': ['sg-xxxxxx']\n    }\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#deploying-and-hosting-models-using-the-sagemaker-sdk",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#deploying-and-hosting-models-using-the-sagemaker-sdk",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Deploying and Hosting Models using the SageMaker SDK",
    "text": "Deploying and Hosting Models using the SageMaker SDK\nUsing the SageMaker SDK, we can deploy models to fully managed endpoints with minimal setup.\n\nExample: Deploying a Model with the SageMaker SDK\nfrom sagemaker import Session, Model\nsession = Session()\n\nmodel = Model(\n    image_uri='your-ecr-image-uri',\n    model_data='s3://your-model-path/model.tar.gz',\n    role='arn:aws:iam::account-id:role/service-role'\n)\n\npredictor = model.deploy(instance_type='ml.m5.large', initial_instance_count=1)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#monitoring-models-in-production-with-sagemaker-model-monitor",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#monitoring-models-in-production-with-sagemaker-model-monitor",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Monitoring Models in Production with SageMaker Model Monitor",
    "text": "Monitoring Models in Production with SageMaker Model Monitor\nModel Monitor enables continuous evaluation of model predictions and data quality.\n\n\n\n\n\n\n\nMonitoring Type\nDescription\n\n\n\n\nData Quality\nDetects missing or unexpected values in input data.\n\n\nModel Bias\nIdentifies bias drift over time.\n\n\nModel Explainability\nEvaluates feature importance.\n\n\nModel Drift\nDetects changes in model predictions.\n\n\n\n\nExample: Setting Up Model Monitor\nfrom sagemaker.model_monitor import DataCaptureConfig\n\ndata_capture_config = DataCaptureConfig(\n    enable_capture=True,\n    sampling_percentage=100,\n    destination_s3_uri='s3://your-monitoring-bucket/'\n)\n\npredictor.update_data_capture_config(data_capture_config)"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#detecting-data-distribution-changes-with-sagemaker-clarify",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#detecting-data-distribution-changes-with-sagemaker-clarify",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Detecting Data Distribution Changes with SageMaker Clarify",
    "text": "Detecting Data Distribution Changes with SageMaker Clarify\nSageMaker Clarify helps identify shifts in data distribution that can impact model performance.\n\nExample: Running a Baseline Drift Analysis\nfrom sagemaker.clarify import BiasConfig, ModelMonitor\n\nbias_config = BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name='feature-column',\n    facet_values_or_threshold=[0]\n)\n\nmonitor = ModelMonitor(\n    job_definition_name='clarify-job',\n    role='arn:aws:iam::account-id:role/service-role',\n    bias_config=bias_config\n)\n\nmonitor.create_monitoring_schedule()"
  },
  {
    "objectID": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#conclusion",
    "href": "sagemaker/sagemaker-deploying-and-monitoring-models-in-vpc.html#conclusion",
    "title": "Deploying and Monitoring SageMaker Models in a VPC",
    "section": "Conclusion",
    "text": "Conclusion\nBy configuring SageMaker endpoints within a VPC, deploying with the SageMaker SDK, and monitoring data with Model Monitor and Clarify, we ensure secure and efficient model operations. These best practices help maintain model reliability and performance in production environments."
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "",
    "text": "AWS SageMaker provides a suite of tools to manage machine learning (ML) workflows efficiently. This blog post will focus on three essential components: SageMaker Feature Store, SageMaker Model Monitor, and SageMaker Clarify. These services help in feature management, model monitoring, and ensuring fairness and explainability in ML models."
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Key Features:",
    "text": "Key Features:\n\nOnline & Offline Stores: Supports real-time and batch feature storage.\nFeature Discovery & Reusability: Enables teams to use standardized features across projects.\nIntegration with SageMaker Pipelines & Model Training: Seamlessly connects to ML workflows."
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#example-creating-a-feature-group-in-python",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#example-creating-a-feature-group-in-python",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Example (Creating a Feature Group in Python):",
    "text": "Example (Creating a Feature Group in Python):\nimport boto3\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n\nboto_session = boto3.Session()\nfeature_store_client = boto_session.client(service_name=\"sagemaker-featurestore-runtime\")\n\nfeature_group = FeatureGroup(name=\"customer_churn_features\",\n                             sagemaker_session=boto_session)\n\nfeature_group.create(\n    record_identifier_name=\"customer_id\",\n    event_time_feature_name=\"event_time\",\n    feature_definitions=[\n        {\"FeatureName\": \"age\", \"FeatureType\": \"Integral\"},\n        {\"FeatureName\": \"monthly_spend\", \"FeatureType\": \"Fractional\"}\n    ]\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Centralized feature management across ML models\n✅ Real-time feature retrieval for online inference\n✅ Historical feature storage for batch processing"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Increased cost for maintaining both online and offline stores\n❌ Requires additional governance to avoid feature drift"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features-1",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features-1",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Key Features:",
    "text": "Key Features:\n\nAutomatic Model Drift Detection\nPrebuilt Metrics for Data Quality, Bias, and Feature Drift\nIntegration with Amazon CloudWatch for Alerts\nSupports Custom Metrics and Logs"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#example-setting-up-model-monitor",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#example-setting-up-model-monitor",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Example (Setting Up Model Monitor):",
    "text": "Example (Setting Up Model Monitor):\nfrom sagemaker.model_monitor import ModelMonitor\n\nmonitor = ModelMonitor(\n    role_arn=\"arn:aws:iam::123456789012:role/SageMakerExecutionRole\",\n    instance_count=1,\n    instance_type=\"ml.m5.large\"\n)\nmonitor.create_monitoring_schedule(\n    endpoint_name=\"churn-prediction-endpoint\",\n    schedule_expression=\"cron(0 * * * ? *)\"\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use-1",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use-1",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Ensuring ML models maintain performance over time\n✅ Detecting bias and feature drift in production models\n✅ Setting up alerts for data quality issues"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations-1",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations-1",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Requires scheduled jobs for batch monitoring\n❌ Limited real-time feedback unless integrated with other AWS services"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features-2",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#key-features-2",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Key Features:",
    "text": "Key Features:\n\nBias Detection in Data & Models\nFeature Importance & SHAP-based Interpretability\nIntegration with SageMaker Model Training & Monitoring\nComprehensive Reports for Regulatory Compliance"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#example-running-bias-detection-with-clarify",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#example-running-bias-detection-with-clarify",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Example (Running Bias Detection with Clarify):",
    "text": "Example (Running Bias Detection with Clarify):\nfrom sagemaker.clarify import BiasConfig, SageMakerClarifyProcessor\n\nbias_config = BiasConfig(\n    label_values_or_threshold=[1],\n    facet_name=\"gender\",\n    group_name=\"male\"\n)\nclarify_processor = SageMakerClarifyProcessor(role=\"arn:aws:iam::123456789012:role/SageMakerExecutionRole\",\n                                              instance_count=1,\n                                              instance_type=\"ml.m5.large\")\nclarify_processor.run_bias(\n    data_config=data_config,\n    bias_config=bias_config,\n    model_config=model_config\n)"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use-2",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#when-to-use-2",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "When to Use:",
    "text": "When to Use:\n✅ Ensuring fairness in ML models\n✅ Understanding feature importance for better transparency\n✅ Generating explainability reports for compliance"
  },
  {
    "objectID": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations-2",
    "href": "sagemaker/sagemaker-feature-monitor-clarify.html#limitations-2",
    "title": "Understanding AWS SageMaker Feature Store, Model Monitor, and Clarify",
    "section": "Limitations:",
    "text": "Limitations:\n❌ Bias detection does not solve bias, only detects it\n❌ Computational overhead for large models"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html",
    "href": "sagemaker/sagemaker-apis.html",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "Amazon SageMaker provides a variety of APIs to interact with its services, including training, inference, and model deployment. This document explores real-world examples of using these APIs.\n\n\nimport boto3\nimport json\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_training_job(\n    TrainingJobName=\"my-xgboost-training-job\",\n    AlgorithmSpecification={\n        \"TrainingImage\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n        \"TrainingInputMode\": \"File\"\n    },\n    RoleArn=\"arn:aws:iam::123456789012:role/SageMakerRole\",\n    InputDataConfig=[\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3://my-bucket/train/\",\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            }\n        }\n    ],\n    OutputDataConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    ResourceConfig={\n        \"InstanceType\": \"ml.m5.large\",\n        \"InstanceCount\": 1,\n        \"VolumeSizeInGB\": 10\n    },\n    StoppingCondition={\"MaxRuntimeInSeconds\": 3600},\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nresponse = sagemaker_client.create_endpoint_config(\n    EndpointConfigName=\"my-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"AllTraffic\",\n            \"ModelName\": \"my-trained-model\",\n            \"InstanceType\": \"ml.m5.large\",\n            \"InitialInstanceCount\": 1\n        }\n    ]\n)\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName=\"my-endpoint\",\n    EndpointConfigName=\"my-endpoint-config\"\n)\n\nprint(json.dumps(response, indent=4))\n\n\n\nimport json\nimport boto3\n\nruntime_client = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=\"my-endpoint\",\n    ContentType=\"application/json\",\n    Body=json.dumps({\"data\": [1.0, 2.0, 3.0, 4.0]})\n)\n\nresult = json.loads(response[\"Body\"].read().decode())\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-1-creating-a-sagemaker-training-job",
    "href": "sagemaker/sagemaker-apis.html#example-1-creating-a-sagemaker-training-job",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "import boto3\nimport json\n\nsagemaker_client = boto3.client(\"sagemaker\")\n\nresponse = sagemaker_client.create_training_job(\n    TrainingJobName=\"my-xgboost-training-job\",\n    AlgorithmSpecification={\n        \"TrainingImage\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n        \"TrainingInputMode\": \"File\"\n    },\n    RoleArn=\"arn:aws:iam::123456789012:role/SageMakerRole\",\n    InputDataConfig=[\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3://my-bucket/train/\",\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            }\n        }\n    ],\n    OutputDataConfig={\"S3OutputPath\": \"s3://my-bucket/output/\"},\n    ResourceConfig={\n        \"InstanceType\": \"ml.m5.large\",\n        \"InstanceCount\": 1,\n        \"VolumeSizeInGB\": 10\n    },\n    StoppingCondition={\"MaxRuntimeInSeconds\": 3600},\n)\n\nprint(json.dumps(response, indent=4))"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-2-deploying-a-model-to-a-sagemaker-endpoint",
    "href": "sagemaker/sagemaker-apis.html#example-2-deploying-a-model-to-a-sagemaker-endpoint",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "response = sagemaker_client.create_endpoint_config(\n    EndpointConfigName=\"my-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"AllTraffic\",\n            \"ModelName\": \"my-trained-model\",\n            \"InstanceType\": \"ml.m5.large\",\n            \"InitialInstanceCount\": 1\n        }\n    ]\n)\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName=\"my-endpoint\",\n    EndpointConfigName=\"my-endpoint-config\"\n)\n\nprint(json.dumps(response, indent=4))"
  },
  {
    "objectID": "sagemaker/sagemaker-apis.html#example-3-invoking-a-sagemaker-endpoint-for-inference",
    "href": "sagemaker/sagemaker-apis.html#example-3-invoking-a-sagemaker-endpoint-for-inference",
    "title": "Amazon SageMaker APIs: Real-World Examples",
    "section": "",
    "text": "import json\nimport boto3\n\nruntime_client = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=\"my-endpoint\",\n    ContentType=\"application/json\",\n    Body=json.dumps({\"data\": [1.0, 2.0, 3.0, 4.0]})\n)\n\nresult = json.loads(response[\"Body\"].read().decode())\nprint(result)"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html",
    "href": "sagemaker/sagemaker-vs-bedrock.html",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "",
    "text": "AWS provides multiple machine learning (ML) services, with Amazon SageMaker and Amazon Bedrock being two primary options. While both facilitate ML model deployment, they cater to different use cases and user expertise levels."
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#overview-table",
    "href": "sagemaker/sagemaker-vs-bedrock.html#overview-table",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Overview Table",
    "text": "Overview Table\n\n\n\n\n\n\n\n\nFeature\nAmazon SageMaker\nAmazon Bedrock\n\n\n\n\nType\nFully managed ML development platform\nFully managed foundation model (FM) service\n\n\nUse Case\nBuilding, training, and deploying custom ML models\nUsing and fine-tuning foundation models for AI applications\n\n\nCustomization\nHigh (bring your own data and model)\nLimited (fine-tune pre-trained models)\n\n\nModel Hosting\nCustom models on managed infrastructure\nAPI-based access to foundation models (FM)\n\n\nData Handling\nRequires dataset preparation and preprocessing\nUses pre-trained models with minimal data processing\n\n\nAccess Model\nSDK, APIs, Jupyter notebooks\nAPI-based inference\n\n\nScaling\nFully managed infrastructure\nFully managed with auto-scaling"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#amazon-sagemaker",
    "href": "sagemaker/sagemaker-vs-bedrock.html#amazon-sagemaker",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Amazon SageMaker",
    "text": "Amazon SageMaker\nDefinition: Amazon SageMaker is a fully managed ML service that provides tools to build, train, and deploy custom machine learning models.\nCommon Use Cases: - Training custom ML models - Deploying inference endpoints - Experiment tracking and model monitoring\n\nExample Code: Train a Model in SageMaker using Python\nimport boto3\n\nsagemaker = boto3.client('sagemaker')\nresponse = sagemaker.create_training_job(\n    TrainingJobName='my-training-job',\n    AlgorithmSpecification={\n        'TrainingImage': 'your-docker-image',\n        'TrainingInputMode': 'File'\n    },\n    RoleArn='your-role-arn',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3Uri': 's3://your-dataset/',\n                    'S3DataType': 'S3Prefix'\n                }\n            }\n        }\n    ],\n    OutputDataConfig={'S3OutputPath': 's3://your-output-bucket/'},\n    ResourceConfig={\n        'InstanceType': 'ml.m5.large',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 50\n    },\n    StoppingCondition={'MaxRuntimeInSeconds': 3600}\n)\nprint(response)"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#amazon-bedrock",
    "href": "sagemaker/sagemaker-vs-bedrock.html#amazon-bedrock",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Amazon Bedrock",
    "text": "Amazon Bedrock\nDefinition: Amazon Bedrock is a managed service that provides access to foundation models from leading AI providers, allowing users to build generative AI applications without needing to train models from scratch.\nCommon Use Cases: - Text generation and summarization - Chatbots and virtual assistants - Image generation and AI-powered applications\n\nExample Code: Call a Foundation Model via Bedrock API\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\nresponse = bedrock.invoke_model(\n    modelId='anthropic.claude-v2',\n    body='{\"prompt\": \"Write a short poem about the ocean.\", \"max_tokens\": 100}'\n)\nprint(response[\"body\"].read().decode('utf-8'))"
  },
  {
    "objectID": "sagemaker/sagemaker-vs-bedrock.html#conclusion",
    "href": "sagemaker/sagemaker-vs-bedrock.html#conclusion",
    "title": "ML Services: SageMaker vs Bedrock",
    "section": "Conclusion",
    "text": "Conclusion\nIf your goal is to build and train custom ML models, SageMaker provides the flexibility and infrastructure to do so. If you need quick access to pre-trained foundation models for AI applications, Bedrock offers a simple API-based approach to integrate generative AI into applications."
  }
]